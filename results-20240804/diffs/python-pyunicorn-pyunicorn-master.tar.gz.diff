Source package : python-pyunicorn
File name       : pyunicorn-master.tar.gz
Lookaside URL   : https://src.fedoraproject.org/repo/pkgs/python-pyunicorn/pyunicorn-master.tar.gz/sha512/b5813a3505894384fb3bee840954bd30fc67e7b359fd4efc512bc1ffe8c89bcd1107a9c199f44ec478fc1d8a402e4d0dc4e56ca45014f6b059b0b6e3a90b1083/pyunicorn-master.tar.gz
Source URL      : https://github.com/pik-copan/pyunicorn/archive/master/pyunicorn-master.tar.gz

source file type: /tmp/fedora-lookaside-verification--620412-UnEb0Nzwp0B1/source-pyunicorn-master.tar.gz: gzip compressed data, from Unix, original size modulo 2^32 4382720
lookaside file type: /tmp/fedora-lookaside-verification--620412-UnEb0Nzwp0B1/lookaside-pyunicorn-master.tar.gz: gzip compressed data, from Unix, original size modulo 2^32 32593920


Only in source-pyunicorn-master.tar.gz-extracted/pyunicorn-master: CHANGELOG.rst
Only in source-pyunicorn-master.tar.gz-extracted/pyunicorn-master: CITATION.cff
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/CONTRIBUTIONS.rst lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/CONTRIBUTIONS.rst
2,3c2,3
< Contributions
< =============
---
> **Copyright**
>     |copy| 2008-2023 Jonathan F. Donges and pyunicorn authors.
5,83c5,6
< Copyright
< ---------
< \ |copy| 2008-2024 Jonathan F. Donges and pyunicorn authors.
< 
< License
< -------
< BSD (3-clause)
< 
< URL
< ---
< https://www.pik-potsdam.de/members/donges/software-2/software
< 
< Mail
< ----
< Jonathan Donges, Potsdam Institute for Climate Impact Research,
< P.O. Box 60 12 03, D-14412 Potsdam, Germany
< 
< Related publications
< --------------------
< See `Publications <docs/source/publications.rst>`_.
< 
< Authors
< -------
< Written as part of a diploma/PhD thesis in physics by `Jonathan F. Donges
< <donges@pik-potsdam.de>`_ at Humboldt University Berlin and the Potsdam
< Institute for Climate Impact Research (PIK) and completed at the University of
< Potsdam, Germany. Substantially extended by `Jobst Heitzig
< <heitzig@pik-potsdam.de>`_.
< 
< Contributors
< ------------
< - Jakob Runge (extended ``core`` and ``climate``)
< - Alexander Radebach
< - Hanna Schultz
< - Marc Wiedermann (extended ``core`` and ``climate``)
< - `Alraune Zech <alrauni@web.de>`_
<   (extended ``timeseries`` during an internship at PIK)
< - `Jan Feldhoff <feldhoff@pik-potsdam.de>`_ (extended ``timeseries``)
< - Aljoscha Rheinwalt
< - Hannes Kutza
< - `Boyan Beronov <beronov@pik-potsdam.de>`_ (restructured, updated and linted
<   codebase and documentation, consolidated original packaging, prepared
<   Cythonization and migration to Python 3, managed open-sourcing, introduced CI,
<   migrated to PEP 517/518 package format, overhauled the Python/Cython interface,
<   made Cython/C extensions compatible with MSVC, edited tutorials,
<   overhauled the caching system, maintained test suite and CI)
< - `Paul Schultz <pschultz@pik-potsdam.de>`_, `Stefan Schinkel
<   <mail@dreeg.org>`_ (added ``resistive_network`` and corresponding
<   tests)
< - `Wolfram Barfuss <barfuss@pik-potsdam.de>`_
<   (contributed to Cythonization, extended and maintained package)
< - Malte Ziehbarth (contributed to Python 3 support in
<   `#106 <https://github.com/pik-copan/pyunicorn/pull/106>`_)
< - Nils Harmening (added event coincidence analysis, contributed to Cythonization,
<   extended test suite, migrated from Python 2.7 to 3.6)
< - Jonathan Kroenke (fixed numerous bugs and style issues in
<   `#119 <https://github.com/pik-copan/pyunicorn/pull/119>`_,
<   generalized spatial and interacting network analysis in
<   `#131 <https://github.com/pik-copan/pyunicorn/pull/131>`_,
<   extended test suite, maintained package and managed release)
< - Johannes Kassel (added ``eventseries`` and
<   ``climate.eventseries_climatenetwork`` in
<   `#156 <https://github.com/pik-copan/pyunicorn/pull/156>`_)
< - `Frederik Wolf <Frederik.Wolf@bmz.bund.de>`_ (contributed to ``eventseries``)
< - Lena Schmidt (added tutorials, maintained package)
< - `Max Bechthold <maxbecht@pik-potsdam.de>`_
<   (reenabled CI, migrated plotting to ``Cartopy``, added tutorials,
<   maintained package)
< - `Ronja Hotz <ronja.hotz@yahoo.de>`_ (added a tutorial
<   in `#190 <https://github.com/pik-copan/pyunicorn/pull/190>`_)
< - `Fritz KÃ¼hlein <fritzku@pik-potsdam.de>`_
<   (fixed numerous bugs and style issues, improved test coverage and CI,
<   integrated tutorial notebooks into documentation,
<   maintained package and managed releases)
< 
< Acknowledgements
< ----------------
< - Travis-CI (https://www.travis-ci.com/) for providing free builds for this open
<   source project.
---
> **License**
>     BSD (3-clause)
85c8,72
< .. |copy|   unicode:: U+000A9 .. COPYRIGHT SIGN
---
> **URL**
>     http://www.pik-potsdam.de/members/donges/software
> 
> **Mail**
>     Jonathan Donges, Potsdam Institute for Climate Impact Research,
>     P.O. Box 60 12 03, D-14412 Potsdam, Germany
> 
> **Authors**
>     Written as part of a diploma/PhD thesis in physics by `Jonathan F. Donges
>     <donges@pik-potsdam.de>`_ at Humboldt University Berlin and the Potsdam
>     Institute for Climate Impact Research (PIK) and completed at the University
>     of Potsdam, Germany. Substantially extended by `Jobst Heitzig
>     <heitzig@pik-potsdam.de>`_.
> 
> **Contributors**
>     - Jakob Runge (extended ``core`` and ``climate``)
>     - Alexander Radebach
>     - Hanna Schultz
>     - Marc Wiedermann (extended ``core`` and ``climate``)
>     - `Alraune Zech <alrauni@web.de>`_
>       (extended ``timeseries`` during an internship at PIK)
>     - `Jan Feldhoff <feldhoff@pik-potsdam.de>`_ (extended ``timeseries``)
>     - Aljoscha Rheinwalt
>     - Hannes Kutza
>     - `Boyan Beronov <beronov@pik-potsdam.de>`_ (restructured and updated
>       codebase and documentation, consolidated original packaging and CI,
>       prepared Cythonization and migration to Python 3, managed open-sourcing,
>       migrated to PEP 517/518 package format, introduced semantic Cython type
>       aliases, added MSVC compatibility for Cython/C extensions, maintained
>       compilation and testing configuration)
>     - `Paul Schultz <pschultz@pik-potsdam.de>`_, `Stefan Schinkel
>       <mail@dreeg.org>`_ (supplied ``resistive_network`` and corresponding
>       tests)
>     - `Wolfram Barfuss <barfuss@pik-potsdam.de>`_ (package extensions and maintenance)
>     - Nils Harmening (cythonized ``weave.inline`` functions, extended testing
>       framework, migrated from Python 2.7 to 3.6)
>     - Jonathan Kroenke (extending test framework, package extensions and maintenance)
>     - Lena Schmidt (writing ipynb tutorials, maintenance)
>     - `Max Bechthold <max.bechthold@stud.uni-heidelberg.de>`_
>       (reenabling CI, update to cartopy plotting, writing ipynb tutorials,
>       style improvements, maintenance)
> 
> **Related publications**
>     See `Publications <docs/source/publications.rst>`_.
> 
> **Essential runtime dependencies**
>     - `Numpy <http://www.numpy.org/>`_
>     - `Scipy <http://www.scipy.org/>`_
>     - `python-igraph <http://igraph.org/>`_
> 
> **Essential compile time dependencies**
>     - `Numpy <http://www.numpy.org/>`_
>     - `Cython <http://cython.org/>`_
> 
> **Included**
>     - `progressbar <http://pypi.python.org/pypi/progressbar/>`_
> 
> **To Do**
>   - A lot - See current product backlog.
> 
> **Known Bugs**:
>   - See GitHub issues list.
>   
> **Acknowledgements**:
>   - Travis-CI (https://www.travis-ci.com/) for providing free builds for this open source project.
Only in source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/docs/source/api/climate: eventseries_climatenetwork.rst
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/docs/source/api/climate/map_plots.rst lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/docs/source/api/climate/map_plots.rst
2c2
< climate.map_plot
---
> climate.map_plots
5c5
< .. automodule:: pyunicorn.climate.map_plot
---
> .. automodule:: pyunicorn.climate.map_plots
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/docs/source/changelog.rst lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/docs/source/changelog.rst
2c2,54
< .. include:: ../../CHANGELOG.rst
\ No newline at end of file
---
> Changelog
> =========
> 
> A summary of major changes made in each release of ``pyunicorn``:
> 
> **0.6.1**
>  - Fixed some bugs and compatibility issues.
>  - Improved test framework.
>  - Added pyunicorn description paper reference to all code files.
> 
> **0.6.0**
>  - Migrated from Python 2.7 to Python 3.7.
>  - Completed transition from ``Weave`` to ``Cython``.
>  - Added Event Coincidence Analysis.
> 
> **0.5.2**
>  - Updated test suite and ``Travis CI``.
> 
> **0.5.1**
>  - Added reference to pyunicorn description paper published in the
>    journal Chaos.
> 
> **0.5.0**
>  - Substantial update of ``CouplingAnalysis``.
>  - New methods in ``RecurrenceNetwork``: ``transitivity_dim_single_scale``,
>    ``local_clustering_dim_single_scale``.
>  - Renamed time-directed measures in ``VisibilityGraph``: ``left/right`` ->
>    ``retarded/advanced``.
>  - Improved documentation and extended publication list.
>  - Began transition from ``Weave`` to ``Cython``.
>  - Added unit tests and improved Pylint compliance.
>  - Set up continuous testing with Travis CI.
>  - Fixed some minor bugs.
> 
> **0.4.1**
>  - Removed a whole lot of ``get_`` s from the API. For example,
>    ``Network.get_degree()`` is now ``Network.degree()``.
>  - Fixed some minor bugs.
> 
> **0.4.0**
>  - Restructured package (subpackages: ``core``, ``climate``, ``timeseries``,
>    ``funcnet``, ``utils``).
>  - Removed dependencies: ``Pysparse``, ``PyNio``, ``progressbar``.
>  - Added a module for resistive networks.
>  - Switched to ``tox`` for test suite management.
>  - Ensured PEP8 and PyFlakes compliance.
> 
> **0.3.2**
>  - Fixed some minor bugs.
>  - Switched to ``Sphinx`` documentation system.
> 
> **0.3.1**
>  - First public release of ``pyunicorn``.
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/docs/source/conf.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/docs/source/conf.py
0a1,2
> # -*- coding: utf-8 -*-
> #
41,42c43
<     'sphinx.ext.githubpages',
<     'nbsphinx',
---
>     'sphinx.ext.githubpages'
58c59
< copyright = u'2008-2024, Jonathan F. Donges and pyunicorn authors'
---
> copyright = u'2008-2023, Jonathan F. Donges and pyunicorn authors'
66c67
< version = re.search(r'\d*\.\d*', __version__).group()
---
> version = re.search('\d*\.\d*', __version__).group()
187a189,194
> 
> 
> # -- Options for Extensions -----------------------------------------------
> 
> mathjax_path = \
>     'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/docs/source/contact.rst lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/docs/source/contact.rst
6,7c6,7
<     :start-after: (3 clause).
<     :end-before: Getting Started
---
>     :start-after: ---------
>     :end-before: License
10a11
>     :end-before: **Related publications**
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/docs/source/development.rst lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/docs/source/development.rst
2,3c2,3
< .. include:: ../../README.rst
<     :start-after: $> cd docs; make clean html latexpdf
\ No newline at end of file
---
> Development
> ===========
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/docs/source/download.rst lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/docs/source/download.rst
1a2,4
> Download
> ========
> 
3,4c6,11
<     :start-after: others.
<     :end-before: Development
---
>     :start-after: ``pyunicorn`` is `BSD-licensed <LICENSE.txt>`_ (3 clause).
>     :end-before: Test suite
> 
> .. include:: ../../README.rst
>     :start-after: graphs.
>     :end-before: License
Only in source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/docs/source: examples
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/docs/source/index.rst lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/docs/source/index.rst
2d1
< ============
7,8c6,10
<     :start-after:   :target: https://zenodo.org/badge/latestdoi/33720178
<     :end-before: For information about individual releases,
---
>     :start-after: =========
>     :end-before: Reference
> 
> For example, to generate a recurrence network with 1000 nodes from a sinusoidal
> signal and compute its network transitivity you simply need to type
10,11c12
< Example
< =======
---
> .. literalinclude:: ../../examples/modules/timeseries/recurrence_network.py
13,14c14,15
< To generate a recurrence network with 1000 nodes from a sinusoidal
< signal and to compute its network transitivity, you can simply run:
---
> The package provides special tools to analyze and model **spatially embedded**
> complex networks.
16c17,21
< .. literalinclude:: examples/modules/timeseries/recurrence_network.py
\ No newline at end of file
---
> ``pyunicorn`` is **fast** because all costly computations are performed in
> compiled C, C++ and Fortran code. It can handle **large networks** through the
> use of sparse data structures. The package can be used interactively, from any
> Python script and even for parallel computations on large cluster
> architectures.
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/docs/source/license.rst lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/docs/source/license.rst
5c5,9
< .. literalinclude:: ../../LICENSE.txt
\ No newline at end of file
---
> .. include:: <isonum.txt>
> .. include:: ../../CONTRIBUTIONS.rst
>     :end-before: **Mail**
> 
> .. literalinclude:: ../../LICENSE.txt
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/docs/source/methods.rst lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/docs/source/methods.rst
2,3c2,3
< Package Overview
< ================
---
> Methods
> =======
24c24
< embedded networks (or simply spatial networks) via the ``GeoNetwork`` and ``Grid``
---
> embedded networks (or simply spatial networks) via the GeoNetwork and Grid
30,32c30,32
< Interacting/multiplex networks (networks of networks)
< -----------------------------------------------------
< The ``InteractingNetworks`` class provides a rich collection of network measures and models specifically designed for investigating the structure of networks of
---
> Interacting/interdependent/multiplex networks / networks of networks
> --------------------------------------------------------------------
> The InteractingNetworks class provides a rich collection of network measures and models specifically designed for investigating the structure of networks of
42,43c42,43
< Node-weighted (node-splitting invariant) network measures
< ---------------------------------------------------------
---
> Node-weighted network measures / node-splitting invariance
> ----------------------------------------------------------
55,56c55,56
< (Coupled) Climate networks
< --------------------------
---
> Climate networks / Coupled climate networks
> -------------------------------------------
69,70c69,70
< Recurrence quantification/network analysis
< ------------------------------------------
---
> Recurrence networks / recurrence quantification analysis / recurrence plots
> ---------------------------------------------------------------------------
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/docs/source/publications.rst lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/docs/source/publications.rst
10,15c10,12
< - Review papers: [Newman2003]_, [Boccaletti2006]_, [Costa2007]_.
< - Further network papers: [Watts1998]_, [Newman2001]_, [Newman2002]_,
<   [Arenas2003]_, [Newman2005]_, [Soffer2005]_, [Holme2007]_, [Tsonis2008a]_,
<   [Ueoka2008]_.
< 
< ....
---
> *Review papers*
> ~~~~~~~~~~~~~~~
> [Newman2003]_, [Boccaletti2006]_, [Costa2007]_.
34a32,36
> *Further network papers*
> ~~~~~~~~~~~~~~~~~~~~~~~~
> [Watts1998]_, [Newman2001]_, [Newman2002]_, [Arenas2003]_, [Newman2005]_,
> [Soffer2005]_, [Holme2007]_, [Tsonis2008a]_, [Ueoka2008]_.
> 
94d95
< 
97,99c98
< - [Bartelemy2011]_.
< 
< ....
---
> [Bartelemy2011]_.
107,108c106
< 
< Interacting/interdependent networks (networks of networks)
---
> Interacting/interdependent networks / networks of networks
110,113c108,110
< - Introduction to structural analysis of interacting networks: [Donges2011a]_.
< - Random graph models & network surrogates for interacting networks: [Schultz2010]_.
< 
< ....
---
> *Introduction to structural analysis of interacting networks*
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> [Donges2011a]_.
123,134c120,124
< .. [Schultz2010] H.C.H. Schultz.
<     "Coupled climate networks: Investigating the terrestrial atmosphere's
<     dynamical structure".
<     Diploma thesis, Free University, Berlin (2010)
< 
< 
< Node-weighted (node-splitting invariant) network measures
< =========================================================
< - Introduction: [Heitzig2012]_.
< - Analysis of node-weighted interacting networks: [Wiedermann2011]_, [Wiedermann2013]_.
< 
< ....
---
> Node-weighted network measures / node-splitting invariance
> ==========================================================
> *Introduction*
> ~~~~~~~~~~~~~~
> [Heitzig2012]_.
143a134,146
> *Random graph models and network surrogates for interacting networks*
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> [Schultz2010]_.
> 
> .. [Schultz2010] H.C.H. Schultz.
>     "Coupled climate networks: Investigating the terrestrial atmosphere's
>     dynamical structure".
>     Diploma thesis, Free University, Berlin (2010)
> 
> *Analysis of node-weighted interacting networks*
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> [Wiedermann2011]_, [Wiedermann2013]_.
> 
162d164
< 
165,167c167
< - [Bretherton1992]_.
< 
< ....
---
> [Bretherton1992]_.
176,190c176,180
< 
< (Coupled) Climate networks
< ==========================
< - Comparing linear & nonlinear construction of climate networks: [Donges2009a]_.
< - Studying the dynamical structure of the surface air temperature field:
<   [Donges2009b]_, [Radebach2010]_.
< - Introduction to coupled climate networks & applications:
<   [Schultz2010]_, [Donges2011a]_, [Wiedermann2011]_.
< - Review of climate network analysis (in Chinese!): [Zou2011]_.
< - Visualization of climate networks: [Tominski2011]_.
< - Evolving climate networks: [Radebach2013]_.
< - General: [Tsonis2004]_, [Tsonis2006]_, [Gozolchiani2008]_, [Tsonis2008b]_,
<   [Tsonis2008c]_, [Yamasaki2008]_, [Donges2009c]_, [Yamasaki2009]_.
< 
< ....
---
> Climate networks / Coupled climate networks
> ===========================================
> *Comparing linear and nonlinear construction of climate networks*
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> [Donges2009a]_.
198a189,192
> *Studying the dynamical structure of the surface air temperature field*
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> [Donges2009b]_, [Radebach2010]_.
> 
209a204,211
> *Introduction to coupled climate networks and applications*
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> [Schultz2010]_, [Donges2011a]_, [Wiedermann2011]_.
> 
> *Review of climate network analysis (in Chinese!)*
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> [Zou2011]_.
> 
213a216,219
> *Visualization of climate networks*
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> [Tominski2011]_.
> 
219a226,229
> *Evolving climate networks*
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~
> [Radebach2013]_.
> 
226a237,241
> *General*
> ~~~~~~~~~
> [Tsonis2004]_, [Tsonis2006]_, [Gozolchiani2008]_, [Tsonis2008b]_,
> [Tsonis2008c]_, [Yamasaki2008]_, [Donges2009c]_, [Yamasaki2009]_.
> 
273,278c288,292
< 
< Power grids & Power networks
< ============================
< - Resistance based networks: [Schultz2014]_, [Schultz2014a]_.
< 
< ....
---
> Power Grids/Power Networks
> ===========================================
> *Resistance based networks*
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~
> [Schultz2014]_, [Schultz2014a]_.
288,296c302,307
< 
< Time series analysis & Synchronization
< ======================================
< - General: [Pecora1998]_, [Schreiber2000]_, [Bandt2002]_, [Kraskov2004]_,
<   [Kantz2006]_, [Thiel2006]_, [Bergner2008]_, [Pompe2011]_, [Ribeiro2011]_, [Runge2012b]_.
< - Event synchronization: [Quiroga2002]_, [Boers2014]_.
< - Event coincidence analysis: [Odenweller2020]_.
< 
< ....
---
> Time series analysis and synchronization
> ========================================
> *General*
> ~~~~~~~~~
> [Pecora1998]_, [Schreiber2000]_, [Bandt2002]_, [Kraskov2004]_, [Kantz2006]_, [Thiel2006]_,
> [Bergner2008]_, [Pompe2011]_, [Ribeiro2011]_, [Runge2012b]_.
360a372,375
> *Event synchronization*
> ~~~~~~~~~~~~~~~~~~~~~~~
> [Quiroga2002]_, [Boers2014]_.
> 
376,397c391,395
< .. [Odenweller2020] A. Odenweller, R.V. Donner.
<     "Disentangling synchrony from serial dependency in paired-event time series".
<     In *Pyhsical Review E*, vol. 101, 052213 (2020)
<     `doi:10.1103/PhysRevE.101.052213
<     <https://doi.org/10.1103/PhysRevE.101.052213>`__
< 
< 
< Recurrence quantification/network analysis
< ==========================================
< - Review of recurrence plots & RQA: [Marwan2007]_.
< - Introduction & application of recurrence networks in the context of RQA: [Marwan2009]_.
< - Thorough introduction to recurrence network analysis: [Donner2010b]_.
< - Discussion of choosing an appropriate recurrence threshold: [Donner2010a]_, [Zou2010]_.
< - Review of various methods for network-based time series analysis: [Donner2011a]_.
< - Introduction to measures of (fractal) transitivity dimensions: [Donner2011b]_.
< - Applications of recurrence network analysis to paleoclimate data: [Donges2011b]_,
<   [Donges2011c]_, [Feldhoff2012]_.
< - Theory of recurrence networks: [Donges2012]_, [Zou2012]_.
< - Multivariate extensions of recurrence network analysis: [Feldhoff2012]_, [Feldhoff2013]_.
< - General: [Ngamga2007]_, [Xu2008]_, [Schinkel2009]_.
< 
< ....
---
> Recurrence networks / quantification analysis / plots
> =====================================================
> *Review of recurrence plots and RQA*
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> [Marwan2007]_.
404a403,406
> *Introduction and application of recurrence networks in the context of RQA*
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> [Marwan2009]_.
> 
410a413,416
> *A thorough introduction to recurrence network analysis*
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> [Donner2010b]_.
> 
417a424,427
> *Discussion of choosing an appropriate recurrence threshold*
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> [Donner2010a]_, [Zou2010]_.
> 
431a442,445
> *Review of various methods for network-based time series analysis*
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> [Donner2011a]_.
> 
440a455,458
> *Introduction to measures of (fractal) transitivity dimensions*
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> [Donner2011b]_.
> 
447a466,469
> *Applications of recurrence network analysis to paleoclimate data*
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> [Donges2011b]_, [Donges2011c]_, [Feldhoff2012]_.
> 
464a487,490
> *Theory of recurrence networks*
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> [Donges2012]_, [Zou2012]_.
> 
478a505,508
> *Multivariate extensions of recurrence network analysis*
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> [Feldhoff2012]_, [Feldhoff2013]_.
> 
493a524,527
> *General*
> ~~~~~~~~~
> [Ngamga2007]_, [Xu2008]_, [Schinkel2009]_.
> 
513d546
< 
516,520c549,551
< - Introduction: [Lacasa2008]_.
< - Application to geophysical time series: [Donner2012]_.
< - Tests for time series irreversibility: [Donges2013]_.
< 
< ....
---
> *Introduction*
> ~~~~~~~~~~~~~~
> [Lacasa2008]_.
527a559,562
> *Application to geophysical time series*
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> [Donner2012]_.
> 
533a569,572
> 
> *Tests for time series irreversibility*
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> [Donges2013]_.
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/docs/source/sitemap.rst lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/docs/source/sitemap.rst
6c6,7
<     :maxdepth: 3
---
>     :maxdepth: 2
>     :includehidden:
10d10
<     methods
11a12
>     methods
13,14d13
<     publications
<     changelog
15a15,16
>     changelog
>     publications
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/docs/source/_templates/layout.html lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/docs/source/_templates/layout.html
5,6c5,6
<         <li><a href="{{ pathto('index') }}">home</a>&nbsp;|</li>
<         <li><a href="{{ pathto('search') }}">search</a>&nbsp;|</li>
---
>         <li><a href="{{ pathto('index') }}">home</a>|&nbsp;</li>
>         <li><a href="{{ pathto('search') }}">search</a>|&nbsp;</li>
Only in lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/docs/source: tutorials
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/docs/source/tutorials.rst lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/docs/source/tutorials.rst
5,9c5,6
< The tutorials are designed to be self-explanatory, and are set up as Jupyter
< notebooks that can be accessed read-only through the links below. The original,
< executable notebook files can be found in the folder ``pyunicorn/docs/source/examples``.
< For further details on the used classes and methods, please refer to the
< :doc:`api_doc` documentation.
---
> The tutorials are designed to be self-explanatory. For further details on the
> used classes and methods please refer to the :doc:`api_doc`.
12,13c9,13
<     :maxdepth: 2
<     :glob:
---
>     :maxdepth: 1
> 
>     tutorials/climate_network_1
>     tutorials/recurrence_network_1
> 
15d14
<     examples/tutorials/*
\ No newline at end of file
Only in lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master: examples
Only in source-pyunicorn-master.tar.gz-extracted/pyunicorn-master: .gitattributes
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/.gitignore lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/.gitignore
4d3
< *$py.class
12a12,13
> env/
> bin/
16d16
< downloads/
18d17
< .eggs/
24,25d22
< wheels/
< share/python-wheels/
31,36d27
< # PyInstaller
< #  Usually these files are written by a python script from a template
< #  before PyInstaller builds the exe, so as to inject date/other infos into it.
< *.manifest
< *.spec
< 
44d34
< .nox/
46d35
< .coverage.*
50,54d38
< *.cover
< *.py,cover
< .hypothesis/
< .pytest_cache/
< cover/
58c42,48
< *.pot
---
> 
> # Mr Developer
> .project
> .pydevproject
> 
> # Rope
> .ropeproject
62,71c52
< local_settings.py
< db.sqlite3
< db.sqlite3-journal
< 
< # Flask stuff:
< instance/
< .webassets-cache
< 
< # Scrapy stuff:
< .scrapy
---
> *.pot
76,167c57
< # PyBuilder
< .pybuilder/
< target/
< 
< # Jupyter Notebook
< .ipynb_checkpoints
< 
< # IPython
< profile_default/
< ipython_config.py
< 
< # pyenv
< #   For a library or package, you might want to ignore these files since the code is
< #   intended to run in multiple environments; otherwise, check them in:
< # .python-version
< 
< # pipenv
< #   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
< #   However, in case of collaboration, if having platform-specific dependencies or dependencies
< #   having no cross-platform support, pipenv may install dependencies that don't work, or not
< #   install all needed dependencies.
< #Pipfile.lock
< 
< # poetry
< #   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
< #   This is especially recommended for binary packages to ensure reproducibility, and is more
< #   commonly ignored for libraries.
< #   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
< #poetry.lock
< 
< # pdm
< #   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
< #pdm.lock
< #   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
< #   in version control.
< #   https://pdm.fming.dev/#use-with-ide
< .pdm.toml
< 
< # PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
< __pypackages__/
< 
< # Celery stuff
< celerybeat-schedule
< celerybeat.pid
< 
< # SageMath parsed files
< *.sage.py
< 
< # Environments
< .env
< .venv
< env/
< venv/
< ENV/
< env.bak/
< venv.bak/
< 
< # Spyder project settings
< .spyderproject
< .spyproject
< 
< # Rope project settings
< .ropeproject
< 
< # Eclipse project settings
< .settings/org.eclipse.*
< 
< # mkdocs documentation
< /site
< 
< # mypy
< .mypy_cache/
< .dmypy.json
< dmypy.json
< 
< # Pyre type checker
< .pyre/
< 
< # pytype static type analyzer
< .pytype/
< 
< # Cython debug symbols
< cython_debug/
< 
< # PyCharm
< #  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
< #  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
< #  and can be added to the global gitignore or merged into this file.  For a more nuclear
< #  option (not recommended) you can uncomment the following to ignore the entire idea folder.
< #.idea/
< 
< # cached data
---
> # Calculation dumps
169d58
< docs/source/examples/tutorials/data
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/LICENSE.txt lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/LICENSE.txt
2c2
< Copyright (C) 2008-2024, Jonathan F. Donges (Potsdam-Institute for Climate
---
> Copyright (C) 2008-2023, Jonathan F. Donges (Potsdam-Institute for Climate
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/MANIFEST.in lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/MANIFEST.in
7c7
< prune **/.ipynb_checkpoints
---
> recursive-include examples *.py
9a10
> include pylintrc
Only in lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master: notebooks
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/pyproject.toml lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/pyproject.toml
2,3c2,3
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
12c12
<     "Cython>=3.0",
---
>     "Cython>=3.0.0",
17,33d16
< # unit tests ===================================================================
< 
< [tool.pytest.ini_options]
< minversion = 7.3
< addopts = "-v -r a -n auto"
< testpaths = ["tests"]
< python_files = ["test*.py", "Test*.py"]
< norecursedirs = [".git", ".cache", ".tox", ".ropeproject", "build"]
< filterwarnings = [
<     "ignore:datetime.datetime.utcfromtimestamp():DeprecationWarning:dateutil|tqdm",
< ]
< 
< [tool.coverage.run]
< parallel = true
< concurrency = ["multiprocessing"]
< source = ["src/pyunicorn"]
< 
38,39c21
<     ".cache", "__pycache__", ".pytest_cache", ".tox", ".venv", ".ropeproject",
<     "build", "mpi.py"
---
>     "CVS", ".cache", ".tox", ".ropeproject", "build", "mpi.py", "progressbar"
41c23
< ignore-patterns = ["navigator", "numerics"]
---
> ignore-patterns = ["navigator", "progressbar", "numerics"]
47,49c29,35
<     "duplicate-code", "invalid-name", "fixme",
<     "missing-docstring", "no-else-return",
<     "arguments-differ", "no-name-in-module"
---
>     "duplicate-code", "useless-suppression", "suppressed-message",
>     "import-error", "invalid-name", "no-member", "unused-variable",
>     "unused-argument", "wildcard-import", "no-name-in-module", "fixme",
>     "missing-docstring", "arguments-differ", "unsubscriptable-object",
>     "no-else-return", "keyword-arg-before-vararg", "invalid-unary-operand-type",
>     "import-outside-toplevel", "arguments-out-of-order",
>     "unsupported-assignment-operation"
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/README.rst lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/README.rst
1c1
< =========
---
> 
7d6
< 
11,21d9
< .. image:: https://img.shields.io/pypi/v/pyunicorn
<   :target: https://pypi.org/project/pyunicorn/
< 
< .. image:: https://img.shields.io/pypi/pyversions/pyunicorn
<   :target: https://pypi.org/project/pyunicorn/
< 
< .. image:: https://zenodo.org/badge/33720178.svg
<   :target: https://zenodo.org/badge/latestdoi/33720178
< 
< About
< =====
23,46c11,27
< analysis toolbox) is an object-oriented Python package for the advanced analysis
< and modeling of complex networks. Beyond the standard **measures of complex
< network theory** (such as *degree*, *betweenness* and *clustering coefficients*), it
< provides some uncommon but interesting statistics like *Newman's random walk
< betweenness*. ``pyunicorn`` also provides novel **node-weighted** *(node splitting invariant)*
< network statistics, measures for analyzing networks of **interacting/interdependent
< networks**, and special tools to model **spatially embedded** complex networks.
< 
< Moreover, ``pyunicorn`` allows one to easily *construct networks* from uni- and
< multivariate time series and event data (**functional/climate networks** and
< **recurrence networks**). This involves linear and nonlinear measures of
< **time series analysis** for constructing functional networks from multivariate data
< (e.g., *Pearson correlation*, *mutual information*, *event synchronization* and *event
< coincidence analysis*). ``pyunicorn`` also features modern techniques of
< nonlinear analysis of time series (or pairs thereof), such as *recurrence
< quantification analysis* (RQA), *recurrence network analysis* and *visibility
< graphs*.
< 
< ``pyunicorn`` is **fast**, because all costly computations are performed in
< compiled C code. It can handle **large networks** through the
< use of sparse data structures. The package can be used interactively, from any
< Python script, and even for parallel computations on large cluster architectures.
< For information about individual releases,
< see our `CHANGELOG <CHANGELOG.rst>`_ and `CONTRIBUTIONS <CONTRIBUTIONS.rst>`_.
---
> analysis toolbox) is a fully object-oriented Python package for the advanced
> analysis and modeling of complex networks. Above the standard measures of
> complex network theory such as degree, betweenness and clustering coefficient
> it provides some **uncommon but interesting statistics** like Newman's random
> walk betweenness. ``pyunicorn`` features novel **node-weighted (node splitting
> invariant)** network statistics as well as measures designed for analyzing
> **networks of interacting/interdependent networks**.
> 
> Moreover, ``pyunicorn`` allows to easily **construct networks from uni- and
> multivariate time series and event data** (functional (climate) networks and
> recurrence networks). This involves linear and nonlinear measures of time
> series analysis for constructing functional networks from multivariate data
> (e.g. Pearson correlation, mutual information, event synchronization and event
> coincidence analysis). ``pyunicorn`` also features modern techniques of
> nonlinear analysis of single and pairs of time series such as recurrence
> quantification analysis (RQA), recurrence network analysis and visibility
> graphs.
49,52d29
< License
< -------
< ``pyunicorn`` is `BSD-licensed <LICENSE.txt>`_ (3 clause).
< 
55c32
< *Please acknowledge and cite the use of this software and its authors when
---
> **Please acknowledge and cite the use of this software and its authors when
57c34
< following reference:*
---
> following reference:**
61,66c38,45
<     Kurths.
<     **"Unified functional network and nonlinear time series analysis for complex
<     systems science: The pyunicorn package"**.
<     Chaos 25, 113101 (2015), `doi:10.1063/1.4934554
<     <http://dx.doi.org/10.1063/1.4934554>`_, Preprint: `arxiv.org:1507.01571
<     <http://arxiv.org/abs/1507.01571>`_ [physics.data-an].
---
>     Kurths,
>     **Unified functional network and nonlinear time series analysis for complex
>     systems science: The pyunicorn package**,
>     `Chaos 25, 113101 (2015), doi:10.1063/1.4934554,
>     <http://dx.doi.org/10.1063/1.4934554>`_
>     `Preprint: arxiv.org:1507.01571 [physics.data-an].
>     <http://arxiv.org/abs/1507.01571>`_
> 
72,78c51,56
< <https://www.bmbf.de/bmbf/en/home/home_node.html>`_ (projects `GOTHAM
< <https://www.belmontforum.org/projects>`_ and `CoSy-CC2
< <http://cosy.pik-potsdam.de/>`_), the `Leibniz Association
< <https://www.leibniz-gemeinschaft.de/en/>`_ (projects `ECONS
< <http://econs.pik-potsdam.de/>`_ and `DominoES
< <https://www.pik-potsdam.de/en/institute/departments/activities/dominoes>`_),
< the `German National Academic Foundation <https://www.studienstiftung.de/en/>`_,
---
> <https://www.bmbf.de/en/index.html>`_ (projects `GOTHAM
> <http://belmont-gotham.org/>`_ and `CoSy-CC2 <http://cosy.pik-potsdam.de/>`_),
> the `Leibniz Association <https://www.leibniz-gemeinschaft.de/en/home/>`_
> (projects `ECONS <http://econs.pik-potsdam.de/>`_ and `DominoES
> <https://www.pik-potsdam.de/research/projects/activities/dominoes>`_), the
> `German National Academic Foundation <https://www.studienstiftung.de/en/>`_,
80,81c58
< `Planetary Boundary Research Network
< <https://web.archive.org/web/20200212214011/http://pb-net.org/>`_ (PB.net) among
---
> `Planetary Boundary Research Network <http://www.pb-net.org>`_ (PB.net) among
84,85d60
< Getting Started
< ===============
87,93c62,64
< Installation
< ------------
< Official releases
< .................
< `Stable releases <https://pypi.org/project/pyunicorn/#history>`_ can be
< installed directly from the `Python Package Index (PyPI)
< <https://packaging.python.org/en/latest/tutorials/installing-packages/#installing-from-pypi>`_::
---
> License
> -------
> ``pyunicorn`` is `BSD-licensed <LICENSE.txt>`_ (3 clause).
95d65
<     $> pip install pyunicorn
97,98c67,85
< Alternatively, source distributions can be downloaded from the
< `GitHub Releases <https://github.com/pik-copan/pyunicorn/releases>`_.
---
> Code
> ----
> `Stable releases <https://github.com/pik-copan/pyunicorn/releases>`_,
> `Development version <https://github.com/pik-copan/pyunicorn>`_
> 
> `Changelog <docs/source/changelog.rst>`_, `Contributions <CONTRIBUTIONS.rst>`_
> 
> 
> Documentation
> -------------
> For extensive HTML documentation, jump right to the `pyunicorn homepage
> <http://www.pik-potsdam.de/~donges/pyunicorn/>`_. Recent `PDF versions
> <http://www.pik-potsdam.de/~donges/pyunicorn/docs/>`_ are also available.
> 
> On a local development version, HTML and PDF documentation can be generated
> using ``Sphinx``::
> 
>     $> pip install --user .[docs]
>     $> cd docs; make clean html latexpdf
100,110d86
< On **Windows**, please *first* install the latest version of the `Microsoft C++ Build
< Tools <https://wiki.python.org/moin/WindowsCompilers>`_, which is required for
< compiling Cython modules.
< 
< Current development version
< ...........................
< In order to use a `newer version <https://github.com/pik-copan/pyunicorn>`_,
< please follow the ``pip`` instructions for installing from `version control
< <https://packaging.python.org/en/latest/tutorials/installing-packages/#installing-from-vcs>`_
< or from a `local source tree
< <https://packaging.python.org/en/latest/tutorials/installing-packages/#installing-from-a-local-src-tree>`_.
113,131c89,112
< ............
< ``pyunicorn`` is implemented in `Python 3 <https://docs.python.org/3/>`_ /
< `Cython 3 <https://cython.org/>`_, is `tested
< <https://app.travis-ci.com/github/pik-copan/pyunicorn>`_ on *Linux*, *macOS*
< and *Windows*, and relies on the following packages:
< 
< - Required:
< 
<   - `numpy <http://www.numpy.org/>`_, `scipy <http://www.scipy.org/>`_
<   - `python-igraph <http://igraph.org/>`_ (for ``Network``)
<   - `h5netcdf <https://h5netcdf.org/>`_ (for ``Data``, ``NetCDFDictionary``)
<   - `tqdm <https://tqdm.github.io/>`_ (for progress bars)
< 
< - Optional:
< 
<   - `Matplotlib <http://matplotlib.org/>`_,
<     `Cartopy <https://scitools.org.uk/cartopy/docs/latest/index.html>`_
<     (for plotting features)
<   - `mpi4py <https://github.com/mpi4py/mpi4py>`_
---
> ------------
> ``pyunicorn`` is implemented in `Python 3 <https://docs.python.org/3/>`_ and
> `Cython 3 <https://cython.org/>`_. The software is written and tested on Linux
> and macOS, but it is also in active use on Windows. ``pyunicorn`` relies on the
> following open source or freely available packages, which need to be installed
> on your machine. For exact dependency information, see ``setup.cfg``.
> 
> Required at runtime:
>   - `Numpy <http://www.numpy.org/>`_
>   - `Scipy <http://www.scipy.org/>`_
>   - `python-igraph <http://igraph.org/>`_
>   - `h5netcdf <https://h5netcdf.org/>`_ or
>     `netcdf4-python <http://unidata.github.io/netcdf4-python/>`_
>     (for ``Data`` and ``NetCDFDictionary``)
> 
> Optional *(used only in certain classes and methods)*:
>   - `PyNGL <http://www.pyngl.ucar.edu/Download/>`_
>     (for ``NetCDFDictionary``)
>   - `Matplotlib <http://matplotlib.org/>`_
>   - `Matplotlib Basemap Toolkit <http://matplotlib.org/basemap/>`_
>     (for drawing maps)
>   - `Cartopy <https://scitools.org.uk/cartopy/docs/latest/index.html>`_
>     (for some plotting features)
>   - `mpi4py <https://bitbucket.org/mpi4py/mpi4py>`_
135,136c116,120
<   - `Jupyter Notebook <https://jupyter-notebook.readthedocs.io/en/latest/>`_
<     (for tutorial notebooks)
---
>   
> To install these dependencies, please follow the instructions for your system's
> package manager or consult the libraries' homepages. An easy way to go may be a
> Python distribution like `Anaconda <https://www.anaconda.com/distribution/>`_
> that already includes many libraries.
139,143c123,128
< Documentation
< -------------
< For extensive HTML documentation, jump right to the `homepage
< <http://www.pik-potsdam.de/~donges/pyunicorn/>`_. In a local source tree,
< HTML and PDF documentation can be generated using ``Sphinx``::
---
> Installation
> ------------
> Before installing ``pyunicorn`` itself, we recommend to make sure that the
> required dependencies are installed using your preferred installation method for
> Python libraries. Afterwards, the package can be installed in the standard way
> from the Python Package Index (PyPI).
145,146c130
<     $> pip install .[docs]
<     $> cd docs; make clean html latexpdf
---
> **Linux, macOS**
148,149c132,146
< Tuturials
< ---------
---
> With the ``pip`` package manager::
> 
>     $> pip install pyunicorn
>         
> On Fedora OS, use::
> 
>     $> dnf install python3-pyunicorn
> 
> **Windows**
> 
> First follow the instructions for installing the latest version of the
> `Microsoft C++ Build Tools <https://wiki.python.org/moin/WindowsCompilers>`_ in
> order to be able to compile the Cython modules, and then::
> 
>     $> pip install pyunicorn
151,153c148,154
< For some example applications look into the
< `tutorials <docs/source/examples/tutorials/>`_ provided with the documentation.
< They are designed to be self-explanatory, and are set up as Jupyter notebooks.
---
> **Development version**
> 
> To use a newer version of ``pyunicorn`` than the latest official release on
> PyPI, download the source code from the Github repository and, instead of the
> above, execute::
> 
>     $> pip install -e .
155,156d155
< Development
< ===========
162,163c161,162
< <https://tox.wiki/>`_ and is configured to use system-wide packages
< when available. Install the test dependencies as follows::
---
> <http://tox.readthedocs.io/>`_ and configured to use system-wide packages when
> available. Install the test dependencies as follows::
165c164
<     $> pip install -e .[tests]
---
>     $> pip install .[testing]
180c179,184
<     $> pytest tests/test_core/test_network.py   # unit tests
---
>     $> pytest tests/test_core/TestNetwork.py    # unit tests
> 
> 
> Mailing list
> ------------
> Not implemented yet.
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/setup.cfg lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/setup.cfg
2,3c2,3
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
10c10
< version = 0.8.0
---
> version = 0.7.0a1
21c21
< url = https://www.pik-potsdam.de/members/donges/software-2/software
---
> url = http://www.pik-potsdam.de/~donges/pyunicorn/
39d38
<     Programming Language :: Python :: 3.12
49,52c48,49
<     igraph >= 0.11
<     h5netcdf >= 1.1   ; python_version >= "3.9"
<     h5netcdf == 1.1.* ; python_version <  "3.9"
<     tqdm >= 4.66
---
>     igraph >= 0.10
>     h5netcdf >= 1.1
66,78c63
<     Cython >= 3.0
< tests =
<     matplotlib >= 3.6
<     cartopy >= 0.21   ; python_version >= "3.9"
<     cartopy == 0.21.1 ; python_version <  "3.9"
<     networkx >= 3.1   ; python_version >= "3.9"
<     networkx == 3.1.* ; python_version <  "3.9"
<     tox >= 4.11
<     flake8 >= 7.0
<     pylint >= 3.0
<     pytest >= 8.0
<     pytest-xdist >= 3.5
<     pytest-cov >= 4.1
---
>     Cython >= 3.0.0
81,84c66,75
<     nbsphinx >= 0.9.3
<     ipython >= 8.4
<     pandoc >= 2.3
<     matplotlib >= 3.6
---
> testing =
>     tox >= 4.3
>     flake8 >= 6.0
>     pylint >= 2.17
>     pytest >= 7.3
>     pytest-xdist >= 3.3
>     pytest-cov >= 4.1
>     networkx >= 3.1
>     cartopy >= 0.21
>     matplotlib
88a80
> minversion = 4.3
100,101c92,93
< skip_install = true
< skipsdist = true
---
> extras =
>     testing
106d97
< passenv = WINDIR, LC_ALL
112d102
<     pandoc
114a105
> skipsdist = true
116c107
<     flake8 src/pyunicorn tests
---
>     flake8
118a110
> skipsdist = true
123d114
< extras = tests
125c116
<     pytest --cov
---
>     pytest --cov=src/pyunicorn --import-mode=append
128c119,120
< extras = docs
---
> extras =
>     docs
130c122
<     sphinx-build -v -j 8 -W -b html -d {envtmpdir}/doctrees docs/source {envtmpdir}/html
---
>     sphinx-build -j 8 -W -b html -d {envtmpdir}/doctrees docs/source {envtmpdir}/html
136c128,131
<     .git, .cache, .tox, .ropeproject, build, docs/source/conf.py
---
>     .git, .cache, .tox, .ropeproject, build, progressbar,
>     docs/source/conf.py
> extend-ignore =
>     E121, E123, E126, E226, E24, E704, E731, F401, F403, F405, F812, F841, W503
138c133,144
<     */__init__.py:F401,F403
---
>     */__init__.py:UnusedImport
>     examples/*.py:E305
> 
> [tool:pytest]
> testpaths =
>     tests
> python_files =
>     test*.py Test*.py
> norecursedirs =
>     .git .cache .tox .ropeproject build progressbar
> addopts =
>     -r a -n auto --ignore=src/pyunicorn/utils/navigator.py
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/setup.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/setup.py
2,3c2,3
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
28,30c28,32
<     'extra_compile_args': ['/O2'] if win else ['-O3', '-std=c99', '-Wall'],
<     'define_macros': [('_GNU_SOURCE', None),
<                       ('NPY_NO_DEPRECATED_API', 'NPY_1_7_API_VERSION')]}
---
>     'extra_compile_args': ['-D_GNU_SOURCE'] + (
>         ['/O2']
>         if win else
>         ['-O3', '-std=c99', '-Wall', '-Wconversion']),
>     'define_macros': [('NPY_NO_DEPRECATED_API', 'NPY_1_7_API_VERSION')]}
35c37
<     'warn.unused': True, 'warn.unused_arg': False, 'warn.unused_result': False}
---
>     'warn.unused': True, 'warn.unused_arg': True}
Only in lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate: cartopy_plots.py
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate/climate_data.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate/climate_data.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
19,20c22,24
< from typing import Tuple
< from collections.abc import Hashable
---
> #
> #  Import essential packages
> #
21a26
> #  Import NumPy for the array object and fast numerics
26d30
< from ..core.cache import Cached
29c33,36
< class ClimateData(Data, Cached):
---
> #
> #  Define class ClimateData
> #
> class ClimateData(Data):
72,74d78
<         self._mut_window = 0
<         """mutation count"""
< 
84a89,92
>         #  Set flags
>         self._flag_phase_mean = False
>         self._phase_mean = None
> 
88,93c96,100
<         self.anomalies = anomalies
< 
<     def __cache_state__(self) -> Tuple[Hashable, ...]:
<         # The following attributes are assumed immutable:
<         #   (_full_observable)
<         return (self._mut_window,)
---
>         if anomalies:
>             self._flag_anomaly = True
>             self._anomaly = observable
>         else:
>             self._flag_anomaly = False
100a108,124
>     def clear_cache(self):
>         """
>         Clean up cache.
> 
>         Is reversible, since all cached information can be recalculated from
>         basic data.
>         """
>         Data.clear_cache(self)
> 
>         if self._flag_phase_mean:
>             del self._phase_mean
>             self._flag_phase_mean = False
> 
>         if self._flag_anomaly:
>             del self._anomaly
>             self._flag_anomaly = False
> 
106,108c130,134
<     def Load(cls, file_name, observable_name, file_type="NetCDF",
<              dimension_names=None, window=None, vertical_level=None,
<              silence_level=0, time_cycle=None, data_source=None):
---
>     def Load(cls, file_name, observable_name, time_cycle,
>              time_name="time", latitude_name="lat", longitude_name="lon",
>              data_source=None,
>              file_type="NetCDF", window=None,
>              vertical_level=None, silence_level=0):
124a151,159
>         :arg int time_cycle: The annual cycle length of the data (units of
>             samples).
>         :arg str time_name: The name of the time variable within data file.
>         :arg str latitude_name: The name of the latitude variable within data
>             file.
>         :arg str longitude_name: The name of longitude variable within data
>             file.
>         :arg str data_source: The name of the data source (model, reanalysis,
>             station).
126,127d160
<         :arg dict dimension_names: The names of the dimensions as used in the
<             NetCDF file. Default: {"lat": "lat", "lon": "lon", "time": "time"}
133,136d165
<         :arg int time_cycle: The annual cycle length of the data (units of
<             samples). NOTE: This is a required argument!
<         :arg str data_source: The name of the data source (model, reanalysis,
<             station).
138,143c167,168
<         if time_cycle is None:
<             raise TypeError("ClimateData.Load() is missing required "
<                             "keyword argument: 'time_cycle'")
< 
<         if dimension_names is None:
<             dimension_names = {"lat": "lat", "lon": "lon", "time": "time"}
---
>         dimension_names = {"time": time_name, "lat": latitude_name,
>                            "lon": longitude_name}
286,287c311
<     @Cached.method(name="climatological mean values")
<     def phase_mean(self):
---
>     def _calculate_phase_mean(self):
300a325,353
>         """
>         if self.silence_level <= 1:
>             print("Calculating climatological mean values...")
> 
>         #  Get raw data
>         observable = self.observable()
>         #  Get time cycle
>         time_cycle = self.time_cycle
> 
>         #  Get number of time series
>         N = observable.shape[1]
> 
>         #  Initialize
>         phase_mean = np.zeros((time_cycle, N))
> 
>         #  Calculate mean value for each day (month) on each node
>         for i in range(time_cycle):
>             phase_mean[i, :] = observable[i::time_cycle, :].mean(axis=0)
> 
>         return phase_mean
> 
>     def phase_mean(self):
>         """
>         Return mean values of observable for each phase of the annual cycle.
> 
>         For further comments, see :meth:`_calculate_phase_mean`.
> 
>         .. note::
>            Only the currently selected spatio-temporal window is considered.
309a363,366
> 
>         :rtype: 2D Numpy array [cycle index, node index]
>         :return: the mean values of observable for each phase of the annual
>                  cycle.
311,314c368,370
<         observable = self.observable()
<         time_cycle = self.time_cycle
<         N = observable.shape[1]
<         phase_mean = np.zeros((time_cycle, N))
---
>         if not self._flag_phase_mean:
>             self._phase_mean = self._calculate_phase_mean()
>             self._flag_phase_mean = True
316,319c372
<         #  Calculate mean value for each day (month) on each node
<         for i in range(time_cycle):
<             phase_mean[i, :] = observable[i::time_cycle, :].mean(axis=0)
<         return phase_mean
---
>         return self._phase_mean
321,322c374
<     @Cached.method(name="daily (monthly) anomaly values")
<     def anomaly(self):
---
>     def _calculate_anomaly(self):
335,340d386
< 
<         **Example:**
< 
<         >>> r(ClimateData.SmallTestData().anomaly()[:,0])
<         array([-0.5 , -0.321 , -0.1106,  0.1106,  0.321 ,
<                 0.5 ,  0.321 ,  0.1106, -0.1106, -0.321 ])
342,344c388,389
<         # If data are anomalies skip automatic calculation of anomalies
<         if self.anomalies:
<             return self._full_observable
---
>         if self.silence_level <= 1:
>             print("Calculating daily (monthly) anomaly values...")
345a391
>         #  Get raw data
346a393
>         #  Get time cycle
347a395
>         #  Initialize array
353a402
> 
355a405,428
>     def anomaly(self):
>         """
>         Return anomaly time series from observable.
> 
>         For further comments, see :meth:`_calculate_anomaly`.
> 
>         .. note::
>            Only the currently selected spatio-temporal window is considered.
> 
>         **Example:**
> 
>         >>> r(ClimateData.SmallTestData().anomaly()[:,0])
>         array([-0.5 , -0.321 , -0.1106,  0.1106,  0.321 ,
>                 0.5 ,  0.321 ,  0.1106, -0.1106, -0.321 ])
> 
>         :rtype: 2D Numpy array [time, node index]
>         :return: the anomalized time series.
>         """
>         if not self._flag_anomaly:
>             self._anomaly = self._calculate_anomaly()
>             self._flag_anomaly = True
> 
>         return self._anomaly
> 
360c433
<         For further comments, see :meth:`anomaly`.
---
>         For further comments, see :meth:`_calculate_anomaly`.
441,442c514,516
<         # invalidate cache
<         self._mut_window += 1
---
> 
>         self._flag_phase_mean = False
>         self._flag_anomaly = False
465,466c539,541
<         # invalidate cache
<         self._mut_window += 1
---
> 
>         self._flag_phase_mean = False
>         self._flag_anomaly = False
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate/climate_network.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate/climate_network.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
19,20c22,24
< from typing import Tuple
< from collections.abc import Hashable, Callable
---
> #
> #  Import essential packages
> #
21a26
> #  Import NumPy for the array object and fast numerics
22a28,29
> 
> #  Import iGraph for high performance graph theory tools written in pure ANSI-C
25c32
< from ..core.cache import Cached
---
> #  Import GeoNetwork and GeoGrid classes
26a34
> from ..core.network import cached_const
28a37,40
> #
> #  Define class ClimateNetwork
> #
> 
44,46c56,58
<     def __init__(self, grid: GeoGrid, similarity_measure: np.ndarray,
<                  threshold=None, link_density=None, non_local=False,
<                  directed=False, node_weight_type="surface", silence_level=0):
---
>     def __init__(self, grid, similarity_measure, threshold=None,
>                  link_density=None, non_local=False, directed=False,
>                  node_weight_type="surface", silence_level=0):
75,76c87
<         assert isinstance(grid, GeoGrid)
<         self.grid: GeoGrid = grid
---
>         self.grid = grid
80,85d90
<         # mutation count
<         if not hasattr(self, "_mut_clim"):
<             self._mut_clim: int = 0
<         else:
<             self._mut_clim += 1
< 
106,108d110
<     def __cache_state__(self) -> Tuple[Hashable, ...]:
<         return GeoNetwork.__cache_state__(self) + (self._mut_clim,)
< 
127,128c129,147
<                 f'Threshold: {self.threshold()}\n' +
<                 f'Local connections filtered out: {self.non_local()}')
---
>         f'Threshold: {self.threshold()}\n' +
>         f'Local connections filtered out: {self.non_local()}')
> 
>     def clear_cache(self, irreversible=False):
>         """
>         Clean up cache.
> 
>         If irreversible=True, the network cannot be recalculated using a
>         different threshold, or link density.
> 
>         :arg bool irreversible: The irreversibility of clearing the cache.
>         """
>         GeoNetwork.clear_cache(self)
> 
>         if irreversible:
>             try:
>                 del self._similarity_measure
>             except AttributeError:
>                 pass
132c151
<         Regenerate the current climate network according to a new similarity
---
>         Regenerate the current climate network according to new similarity
135c154
<         ClimateNetwork.__init__(self, grid=self.grid,
---
>         ClimateNetwork.__init__(self, grid=self.data.grid,
148,149c167,169
<     # pylint: disable=keyword-arg-before-vararg
<     def save(self, filename, fileformat=None, *args, **kwds):
---
>     def save(self, filename_network, filename_grid=None,
>              filename_similarity_measure=None, fileformat=None, *args,
>              **kwds):
172,174c192,197
<         :arg tuple/list filename: Tuple or list of three strings, namely
<             the paths to the files where the Network object, the
<             GeoGrid object and the similarity measure matrix are to be stored.
---
>         :arg str filename_network:  The name of the file where the Network
>             object is to be stored.
>         :arg str filename_grid:  The name of the file where the GeoGrid object
>             is to be stored (including ending).
>         :arg str filename_similarity_measure:  The name of the file where the
>             similarity measure matrix is to be stored.
185,186d207
<         :arg str filename_similarity_measure:  The name of the file where the
<             similarity measure matrix is to be stored.
188,195d208
<         try:
<             (filename_network, filename_grid,
<              filename_similarity_measure) = filename
<         except ValueError as e:
<             raise ValueError("'filename' must be a tuple or list of three "
<                              "items: filename_network, filename_grid, "
<                              "filename_similarity_measure") from e
< 
197c210,211
<         GeoNetwork.save(self, filename=(filename_network, filename_grid),
---
>         GeoNetwork.save(self, filename_network=filename_network,
>                         filename_grid=filename_grid,
206d219
<     # pylint: disable=keyword-arg-before-vararg
208c221,222
<     def Load(filename, fileformat=None, silence_level=0, *args, **kwds):
---
>     def Load(filename_network, filename_grid, filename_similarity_measure,
>              fileformat=None, *args, **kwds):
226,229c240,245
<         :arg tuple/list filename: Tuple or list of three strings, namely
<             the paths to the files containing the Network object, the
<             GeoGrid object and the similarity measure matrix.
<             (filename_network, filename_grid, filename_similarity_measure)
---
>         :arg str filename_network:  The name of the file where the Network
>             object is to be stored.
>         :arg str filename_grid:  The name of the file where the GeoGrid object
>             is to be stored (including ending).
>         :arg str filename_similarity_measure:  The name of the file where the
>             similarity measure matrix is to be stored.
240,247d255
<         try:
<             (filename_network, filename_grid,
<              filename_similarity_measure) = filename
<         except ValueError as e:
<             raise ValueError("'filename' must be a tuple or list of three "
<                              "items: filename_network, filename_grid, "
<                              "filename_similarity_measure") from e
< 
270,271c278
<                              directed=graph.is_directed(),
<                              silence_level=silence_level)
---
>                              directed=graph.is_directed())
278,279c285,287
<         #  invalidate cache
<         net._mut_la += 1
---
>         #  Restore link attributes/weights
>         net.clear_paths_cache()
> 
482,486c490,493
<         weighted_similarity = similarity_measure * \
<             (0.5 * (np.tanh(a * (self.grid.angular_distance() - d_min)) + 1))
<         # The above line is a function that provides a smooth
<         # transition of distance weight, centered around distance d_min.
<         # Other sigmoidal type functions could be used as well.
---
>         #  This function provides a smooth transition of distance weight
>         #  centered around distance d_min.
>         #  Other sigmoidal type functions could be used as well.
>         f = lambda x, a, b: 0.5 * (np.tanh(a * (x - b)) + 1)
487a495,496
>         weighted_similarity = similarity_measure * \
>             f(self.grid.angular_distance(), a, d_min)
505,507c514,516
<         except AttributeError as e:
<             raise AttributeError("Similarity matrix was deleted "
<                                  "earlier and cannot be retrieved.") from e
---
>         except AttributeError:
>             print("The similarity matrix was deleted earlier and cannot be "
>                   "returned.")
614c623
<     @Cached.method()
---
>     @cached_const('base', 'correlation_distance')
646c655
<     @Cached.method()
---
>     @cached_const('base', 'inv_correlation_distance')
707,711d715
< 
<     def _weighted_metric(self, attr: str, calc: Callable, metric: str):
<         if not self.find_link_attribute(attr):
<             self.set_link_attribute(attr, calc())
<         return getattr(self, metric)(attr)
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate/coupled_climate_network.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate/coupled_climate_network.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
18a22,26
> #
> #  Import essential packages
> #
> 
> #  Import NumPy for the array object and fast numerics
21c29
< from ..core import InteractingNetworks, GeoNetwork, GeoGrid
---
> #  Import climate_network for ClimateNetwork class
23a32,38
> #  Import grid for GeoGrid class
> from ..core import InteractingNetworks, GeoNetwork, GeoGrid
> 
> 
> #
> #  Define class CoupledClimateNetwork
> #
25a41
> 
110c126
<             self.nodes_1 = list(range(self.N_1))
---
>             self.nodes_1 = range(self.N_1)
112c128
<             self.nodes_2 = list(range(self.N_1, self.N))
---
>             self.nodes_2 = range(self.N_1, self.N)
133,134c149,150
<         return (f'CoupledClimateNetwork:\n{ClimateNetwork.__str__(self)}\n'
<                 f'N1: {self.N_1}\nN2: self.N_2')
---
>         return f'CoupledClimateNetwork:\n{ClimateNetwork.__str__(self)}\
>         	\nN1: {self.N_1}\nN2: self.N_2'
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate/coupled_tsonis.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate/coupled_tsonis.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
18a22,26
> #
> #  Import essential packages
> #
> 
> #  Import NumPy for the array object and fast numerics
20a29
> #  Import climate_network for CoupledClimateNetwork class
24c33,36
< # pylint: disable=too-many-ancestors
---
> #
> #  Define class CoupledClimateNetwork
> #
> 
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate/eventseries_climatenetwork.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate/eventseries_climatenetwork.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
44,45c47,55
<     def __init__(self, data, method='ES', p_value=None, **kwargs):
<         r"""
---
>     def __init__(self, data, method='ES', taumax=np.inf, lag=0.0,
>                  symmetrization='directed', window_type='symmetric',
>                  threshold_method=None, threshold_values=None,
>                  threshold_types=None, p_value=None,
>                  surrogate='shuffle', n_surr=1000,
>                  non_local=False, node_weight_type='surface',
>                  silence_level=0):
> 
>         """
51c61
<         :type data: :class:`..climate.ClimateData`
---
>         :type data: :classL`..climate.ClimateData`
53c63
<         :type method: ``str {'ES', 'ECA', 'ES_pval', 'ECA_pval'}``
---
>         :type method: str {'ES', 'ECA', 'ES_pval', 'ECA_pval'}
56c66,97
<         :type p_value: ``float in [0,1]``
---
>         :type taumax: float
>         :arg taumax: maximum time difference between two events to be
>                     considered synchronous. Caution: For ES, the default is
>                     np.inf because of the intrinsic dynamic coincidence
>                     interval in ES. For ECA, taumax is a parameter of the
>                     method that needs to be defined.
>         :type lag: float
>         :arg lag: extra time lag between the event series.
>         :type symmetrization: str {'directed', 'symmetric', 'antisym',
>                                    'mean', 'max', 'min'} for ES,
>                               str {'directed', 'mean', 'max', 'min'} for ECA
>         :arg symmetrization: determines if and if true, which symmetrization
>                              should be used for the ES/ECA score matrix.
>         :type window_type: str {'retarded', 'advanced', 'symmetric'}
>         :arg window_type: Only for ECA. Determines if precursor coincidence
>                           rate ('advanced'), trigger coincidence rate
>                           ('retarded') or a general coincidence rate with the
>                           symmetric interval [-taumax, taumax] are computed
>                           ('symmetric'). Default: 'symmetric'.
>         :type threshold_method: str 'quantile' or 'value' or 1D numpy array or
>                                 str 'quantile' or 'value'
>         :arg threshold_method: specifies the method for generating a binary
>                                event matrix from an array of continuous time
>                                series. Default: None.
>         :type threshold_values: 1D Numpy array or float
>         :arg threshold_values: quantile or real number determining threshold
>                                for each variable. Default: None.
>         :type threshold_types: str 'above' or 'below' or 1D list of strings
>                                'above' or 'below'
>         :arg threshold_types: determines for each variable if event is below
>                               or above threshold.
>         :type p_value: float in [0,1]
62,113c103,110
<         :arg \**kwargs:
<             optional keyword arguments to specify parent classes' behavior,
<             see below for all options.
< 
<         :Keyword Arguments:
<             * *taumax* (``float``) --
<               maximum time difference between two events to be
<               considered synchronous. Caution: For ES, the default is
<               np.inf because of the intrinsic dynamic coincidence
<               interval in ES. For ECA, taumax is a parameter of the
<               method that needs to be defined.
<             * *lag* (``float``) --
<               extra time lag between the event series.
<             * *symmetrization* (
<               ``str {'directed', 'symmetric', 'antisym',
<               'mean', 'max', 'min'}`` for ES,
<               str {'directed', 'mean', 'max', 'min'}`` for ECA
<               ) --
<               determines if and if true, which symmetrization
<               should be used for the ES/ECA score matrix.
<             * *window_type* (
<               ``str {'retarded', 'advanced', 'symmetric'}``
<               ) --
<               Only for ECA. Determines if precursor coincidence
<               rate ('advanced'), trigger coincidence rate
<               ('retarded') or a general coincidence rate with the
<               symmetric interval [-taumax, taumax] are computed
<               ('symmetric'). Default: 'symmetric'.
<             * *threshold_method* (
<               ``str 'quantile' or 'value' or
<               1D numpy array of 'quantile' or 'value'``
<               ) --
<               specifies the method for generating a binary
<               event matrix from an array of continuous time
<               series. Default: None.
<             * *threshold_values* (``1D Numpy array or float``) --
<               quantile or real number determining threshold
<               for each variable. Default: None.
<             * *threshold_types* (
<               ``str 'above' or 'below' or 1D list
<               of strings of 'above' or 'below'``
<               ) --
<               determines for each variable if event is below
<               or above threshold.
<             * *non_local* (``bool``) --
<               determines whether links between spatially close
<               nodes should be suppressed.
<             * *node_weight_type* (``str``) --
<               The type of geographical node weight to be
<               used.
<             * *arg silence_level* (``int``) --
<               The inverse level of verbosity of the object.
---
>         :type non_local: bool
>         :arg non_local: determines whether links between spatially close
>                         nodes should be suppressed.
>         :type node_weight_type: str
>         :arg node_weight_type: The type of geographical node weight to be
>                                used.
>         :type silence_level: int
>         :arg silence_level: The inverse level of verbosity of the object.
116,142d112
<         # extract ES and CN related optional keyword arguments from **kwargs
<         ES_kwargs = {
<             "taumax": kwargs.get("taumax", np.inf),
<             "lag": kwargs.get("lag", 0.0),
<             "threshold_method": kwargs.get("threshold_method", None),
<             "threshold_values": kwargs.get("threshold_values", None),
<             "threshold_types": kwargs.get("threshold_types", None)
<         }
< 
<         ES_analysis_kwargs = {
<             "symmetrization": kwargs.get("symmetrization", 'directed'),
<             "window_type": kwargs.get("window_type", 'symmetric')
<         }
< 
<         ES_significance_kwargs = {
<             "surrogate": kwargs.get("surrogate", 'shuffle'),
<             "n_surr": kwargs.get("n_surr", 1000),
<             "symmetrization": kwargs.get("symmetrization", 'directed'),
<             "window_type": kwargs.get("window_type", 'symmetric')
<         }
< 
<         CN_kwargs = {
<             "non_local": kwargs.get("non_local", False),
<             "node_weight_type": kwargs.get("node_weight_type", "surface"),
<             "silence_level": kwargs.get("silence_level", 0)
<         }
< 
145,147c115,124
<             raise IOError(f"Method input must be: "
<                           f"{method_types[0]}, {method_types[1]},"
<                           f"{method_types[2]}, or {method_types[3]}!")
---
>             raise IOError(f"Method input must be: {method_types[0]},\
>             		   {method_types[1]},\
>                           {method_types[2]}, or {method_types[3]}!")
> 
>         etypes = ["directed", "symmetric", "antisym", "mean", "max", "min"]
>         if symmetrization not in etypes:
>             raise IOError(f"wrong symmetry...\n \
>                           Available options: {etypes[0]}, {etypes[1]},\
>                           {etypes[2]}, {etypes[3]},\
>                           {etypes[4]}, or {etypes[5]}")
149a127,130
>         self.__symmetry = symmetrization
>         self.directed = (self.__symmetry == "directed")
>         self.__es_type = method
>         self.__window_type = window_type
151,153c132,135
< 
<         self.__symmetry = kwargs.get("symmetrization", 'directed')
<         self.directed = self.__symmetry == "directed"
---
>         self.__surrogate = surrogate
>         self.__n_surr = n_surr
>         self.__taumax = taumax
>         self.__lag = lag
156c138,141
<         EventSeries.__init__(self, data.observable(), **ES_kwargs)
---
>         EventSeries.__init__(self, data.observable(), taumax=self.__taumax,
>                              lag=self.__lag, threshold_method=threshold_method,
>                              threshold_values=threshold_values,
>                              threshold_types=threshold_types)
168c153,154
<                                            **ES_analysis_kwargs)
---
>                                            symmetrization=self.__symmetry,
>                                            window_type=self.__window_type)
181c167,169
<                         method=self.__method, **ES_significance_kwargs)
---
>                         method=self.__method, surrogate=self.__surrogate,
>                         n_surr=self.__n_surr, symmetrization=self.__symmetry,
>                         window_type=self.__window_type)
188c176,183
<         elif self.__method in ['ES_pval', 'ECA_pval']:
---
>         elif self.__method == 'ES_pval':
>             measure_matrix = \
>                 self.event_analysis_significance(
>                     method='ES', surrogate=self.__surrogate,
>                     n_surr=self.__n_surr, symmetrization=self.__symmetry,
>                     window_type=self.__window_type)
> 
>         elif self.__method == 'ECA_pval':
191c186,188
<                     method=self.__method, **ES_significance_kwargs)
---
>                     method='ECA', surrogate=self.__surrogate,
>                     n_surr=self.__n_surr, symmetrization=self.__symmetry,
>                     window_type=self.__window_type)
195,196c192,196
<                                 threshold=0, directed=self.directed,
<                                 **CN_kwargs)
---
>                                 threshold=0,
>                                 non_local=non_local,
>                                 directed=self.directed,
>                                 node_weight_type=node_weight_type,
>                                 silence_level=silence_level)
200c200
<         Return a string representation of EventSeriesClimateNetwork.
---
>         Return a string representation of TsonisClimateNetwork.
230c230
<                        self.__method)
---
>                        self.__es_type)
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate/_ext/__init__.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate/_ext/__init__.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate/_ext/numerics.pyx lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate/_ext/numerics.pyx
0a1,2
> # -*- coding: utf-8 -*-
> #
2,3c4,5
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
21,22c23,24
< from ...core._ext.types import FIELD, INT64TYPE
< from ...core._ext.types cimport MASK_t, FIELD_t, INT64TYPE_t
---
> from ...core._ext.types import FIELD, DFIELD
> from ...core._ext.types cimport MASK_t, FIELD_t, DFIELD_t
41,46c43,48
<         ndarray[INT64TYPE_t, ndim=2, mode='c'] symbolic = np.zeros(
<             (N, n_samples), dtype=INT64TYPE)
<         ndarray[INT64TYPE_t, ndim=2, mode='c'] hist = np.zeros(
<             (N, n_bins), dtype=INT64TYPE)
<         ndarray[INT64TYPE_t, ndim=2, mode='c'] hist2d = np.zeros(
<             (n_bins, n_bins), dtype=INT64TYPE)
---
>         ndarray[long, ndim=2, mode='c'] symbolic = np.zeros(
>             (N, n_samples), dtype=long)
>         ndarray[long, ndim=2, mode='c'] hist = np.zeros(
>             (N, n_bins), dtype=long)
>         ndarray[long, ndim=2, mode='c'] hist2d = np.zeros(
>             (n_bins, n_bins), dtype=long)
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate/_ext/src_numerics.c lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate/_ext/src_numerics.c
6c6
< * Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
---
> * Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate/havlin.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate/havlin.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
19,20c22,24
< from typing import Tuple
< from collections.abc import Hashable
---
> #
> #  Import essential packages
> #
21a26
> #  Import NumPy for the array object and fast numerics
23d27
< from tqdm import trange
25c29,32
< from .climate_data import ClimateData
---
> #  Import progress bar for easy progress bar handling
> from ..utils import progressbar
> 
> #  Import cnNetwork for Network base class
28a36,39
> #
> #  Define class HavlinClimateNetwork
> #
> 
85,87c96,97
<         assert isinstance(data, ClimateData)
<         self.data: ClimateData = data
<         """The climate data used for network construction."""
---
>         self.data = data
>         """(ClimateData) - The climate data used for network construction."""
91d100
<         self._mut_clim: int = 0
102,104d110
<     def __cache_state__(self) -> Tuple[Hashable, ...]:
<         return ClimateNetwork.__cache_state__(self) + (self.data,)
< 
114c120
<     def clear_cache(self):
---
>     def clear_cache(self, irreversible=False):
116a123,127
> 
>         If irreversible=True, the network cannot be recalculated using a
>         different threshold, or link density.
> 
>         :arg bool irreversible: The irreversibility of clearing the cache.
118,121c129,135
<         try:
<             del self._correlation_lag
<         except AttributeError:
<             pass
---
>         ClimateNetwork.clear_cache(self, irreversible)
> 
>         if irreversible:
>             try:
>                 del self._correlation_lag
>             except AttributeError:
>                 pass
143a158,162
>         if self.silence_level <= 1:
>             print("Calculating correlation strength matrix "
>                   "following [Yamasaki2008]_...")
> 
>         #  Initialize
144a164,165
> 
>         #  Normalize anomaly time series to zero mean and unit variance
145a167,168
> 
>         #  Apply cosine window to anomaly data
146a170
> 
149a174
> 
152a178,181
>         #  Initialize progress bar
>         if self.silence_level <= 1:
>             progress = progressbar.ProgressBar(maxval=N).start()
> 
156c185,190
<         for i in trange(N, disable=self.silence_level > 1):
---
>         for i in range(N):
>             # Update progress bar every 10 steps
>             if self.silence_level <= 1:
>                 if (i % 10) == 0:
>                     progress.update(i)
> 
181a216,218
>         if self.silence_level <= 1:
>             progress.finish()
> 
198c235
<         self._mut_clim += 1
---
>         #  Set class variable _max_delay
199a237,238
> 
>         #  Calculate correlation strength and lag
223c262,266
<         return self._similarity_measure
---
>         try:
>             return self._similarity_measure
>         except AttributeError:
>             print("Correlation strength matrix was deleted earlier and "
>                   "cannot be retrieved.")
232c275,279
<         return self._correlation_lag
---
>         try:
>             return self._correlation_lag
>         except AttributeError:
>             print("Lag matrix was deleted earlier and "
>                   "cannot be retrieved.")
244,247c291,295
<         return self._weighted_metric(
<             "correlation_strength",
<             lambda: np.abs(self.correlation_strength()),
<             "average_path_length")
---
>         if "correlation_strength" not in self._path_lengths_cached:
>             self.set_link_attribute("correlation_strength",
>                                     np.abs(self.correlation_strength()))
> 
>         return self.average_path_length("correlation_strength")
256,259c304,308
<         return self._weighted_metric(
<             "correlation_strength",
<             lambda: np.abs(self.correlation_strength()),
<             "closeness")
---
>         if "correlation_strength" not in self._path_lengths_cached:
>             self.set_link_attribute("correlation_strength",
>                                     np.abs(self.correlation_strength()))
> 
>         return self.closeness("correlation_strength")
267,270c316,320
<         return self._weighted_metric(
<             "correlation_lag",
<             lambda: np.abs(self.correlation_lag()),
<             "average_path_length")
---
>         if "correlation_lag" not in self._path_lengths_cached:
>             self.set_link_attribute("correlation_lag",
>                                     np.abs(self.correlation_lag()))
> 
>         return self.average_path_length("correlation_lag")
279,282c329,333
<         return self._weighted_metric(
<             "correlation_lag",
<             lambda: np.abs(self.correlation_lag()),
<             "closeness")
---
>         if "correlation_lag" not in self._path_lengths_cached:
>             self.set_link_attribute("correlation_lag",
>                                     np.abs(self.correlation_lag()))
> 
>         return self.closeness("correlation_lag")
291,294c342,346
<         return self._weighted_metric(
<             "correlation_strength",
<             lambda: np.abs(self.correlation_strength()),
<             "local_vulnerability")
---
>         if "correlation_strength" not in self._path_lengths_cached:
>             self.set_link_attribute("correlation_strength",
>                                     np.abs(self.correlation_strength()))
> 
>         return self.local_vulnerability("correlation_strength")
303,306c355,359
<         return self._weighted_metric(
<             "correlation_lag",
<             lambda: np.abs(self.correlation_lag()),
<             "local_vulnerability")
---
>         if "correlation_lag" not in self._path_lengths_cached:
>             self.set_link_attribute("correlation_lag",
>                                     np.abs(self.correlation_lag()))
> 
>         return self.local_vulnerability("correlation_lag")
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate/hilbert.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate/hilbert.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
19,20c22,24
< from typing import Tuple
< from collections.abc import Hashable
---
> #
> #  Import essential packages
> #
21a26
> #  Import NumPy for the array object and fast numerics
22a28,29
> 
> #  Import scipy.signal for signal processing
30c37
< from .climate_data import ClimateData
---
> #  Import cnNetwork for Network base class
91,93c98,99
<         assert isinstance(data, ClimateData)
<         self.data: ClimateData = data
<         """The climate data used for network construction."""
---
>         self.data = data
>         """(ClimateData) - The climate data used for network construction."""
109,111d114
<     def __cache_state__(self) -> Tuple[Hashable, ...]:
<         return ClimateNetwork.__cache_state__(self) + (self.data,)
< 
118c121
<     def clear_cache(self):
---
>     def clear_cache(self, irreversible=False):
120a124,128
> 
>         If irreversible=True, the network cannot be recalculated using a
>         different threshold, or link density.
> 
>         :arg bool irreversible: The irreversibility of clearing the cache.
122,125c130,136
<         try:
<             del self._coherence_phase
<         except AttributeError:
<             pass
---
>         ClimateNetwork.clear_cache(self, irreversible)
> 
>         if irreversible:
>             try:
>                 del self._coherence_phase
>             except AttributeError:
>                 pass
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate/__init__.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate/__init__.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
24a28,36
> 
> To do
> ~~~~~
>   - A lot - See current product backlog.
> 
> Known Bugs
> ~~~~~~~~~~
>   - ...
> 
35c47,48
< from .map_plot import MapPlot
---
> from .map_plots import MapPlots
> from .cartopy_plots import CartopyPlots
Only in source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate: map_plot.py
Only in lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate: map_plots.py
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate/mutual_info.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate/mutual_info.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
19,20c22,24
< from typing import Tuple
< from collections.abc import Hashable
---
> #
> #  Import essential packages
> #
21a26
> # array object and fast numerics
26c31,35
< from .climate_data import ClimateData
---
> 
> #  Import progress bar for easy progress bar handling
> from ..utils import progressbar
> 
> #  Import cnNetwork for Network base class
28a38,41
> #
> #  Define class MutualInfoClimateNetwork
> #
> 
30a44
> 
82,84c96,97
<         assert isinstance(data, ClimateData)
<         self.data: ClimateData = data
<         """The climate data used for network construction."""
---
>         self.data = data
>         """(ClimateData) - The climate data used for network construction."""
104,106d116
<     def __cache_state__(self) -> Tuple[Hashable, ...]:
<         return ClimateNetwork.__cache_state__(self) + (self.data,)
< 
156a167,256
>     def _calculate_mutual_information(self, anomaly, n_bins=32):
>         """
>         Calculate the mutual information matrix at zero lag.
> 
>         .. note::
>            Slow since solely based on Python and Numpy!
> 
>         :type anomaly: 2D array (time, index)
>         :arg anomaly: The anomaly time series.
>         :arg int n_bins: The number of bins for estimating probability
>                      distributions.
>         :rtype: 2D array (index, index)
>         :return: the mutual information matrix at zero lag.
>         """
>         if self.silence_level <= 1:
>             print("Calculating mutual information matrix at zero lag from "
>                   "anomaly values...")
> 
>         #  Define references to numpy functions for faster function calls
>         histogram = np.histogram
>         histogram2d = np.histogram2d
>         log = np.log
> 
>         #  Normalize anomaly time series to zero mean and unit variance
>         self.data.normalize_time_series_array(anomaly)
> 
>         #  Get faster reference to length of time series = number of samples
>         #  per grid point.
>         n_samples = anomaly.shape[0]
> 
>         #  Initialize mutual information array
>         mi = np.zeros((self.N, self.N))
> 
>         #  Get common range for all histograms
>         range_min = anomaly.min()
>         range_max = anomaly.max()
> 
>         #  Calculate the histograms for each time series
>         p = np.zeros((self.N, n_bins))
> 
>         for i in range(self.N):
>             p[i, :] = histogram(
>                 anomaly[:, i], bins=n_bins, range=(range_min, range_max)
>             )[0].astype("float64")
> 
>         #  Normalize by total number of samples = length of each time series
>         p /= n_samples
> 
>         #  Make sure that bins with zero estimated probability are not counted
>         #  in the entropy measures.
>         p[p == 0] = 1
> 
>         #  Compute the information entropies of each time series
>         H = - (p * log(p)).sum(axis=1)
> 
>         # Initialize progress bar
>         if self.silence_level <= 1:
>             progress = progressbar.ProgressBar(maxval=self.N**2).start()
> 
>         #  Calculate only the lower half of the MI matrix, since MI is
>         #  symmetric with respect to X and Y.
>         for i in range(self.N):
>             # Update progress bar every 10 steps
>             if self.silence_level <= 1:
>                 if (i % 10) == 0:
>                     progress.update(i**2)
> 
>             for j in range(i):
>                 #  Calculate the joint probability distribution
>                 pxy = histogram2d(
>                     anomaly[:, i], anomaly[:, j], bins=n_bins,
>                     range=((range_min, range_max),
>                            (range_min, range_max)))[0].astype("float64")
> 
>                 #  Normalize joint distribution
>                 pxy /= n_samples
> 
>                 #  Compute the joint information entropy
>                 pxy[pxy == 0] = 1
>                 HXY = - (pxy * log(pxy)).sum()
> 
>                 #  ... and store the result
>                 mi.itemset((i, j), H.item(i) + H.item(j) - HXY)
>                 mi.itemset((j, i), mi.item((i, j)))
> 
>         if self.silence_level <= 1:
>             progress.finish()
> 
>         return mi
> 
265,268c365,369
<         return self._weighted_metric(
<             "mutual_information",
<             lambda: np.abs(self.mutual_information()),
<             "average_path_length")
---
>         if "mutual_information" not in self._path_lengths_cached:
>             self.set_link_attribute("mutual_information",
>                                     abs(self.mutual_information()))
> 
>         return self.average_path_length("mutual_information")
277,280c378,382
<         return self._weighted_metric(
<             "mutual_information",
<             lambda: np.abs(self.mutual_information()),
<             "closeness")
---
>         if "mutual_information" not in self._path_lengths_cached:
>             self.set_link_attribute("mutual_information",
>                                     abs(self.mutual_information()))
> 
>         return self.closeness("mutual_information")
289,292c391,395
<         return self._weighted_metric(
<             "mutual_information",
<             lambda: np.abs(self.mutual_information()),
<             "local_vulnerability")
---
>         if "mutual_information" not in self._path_lengths_cached:
>             self.set_link_attribute("mutual_information",
>                                     abs(self.mutual_information()))
> 
>         return self.local_vulnerability("mutual_information")
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate/partial_correlation.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate/partial_correlation.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate/rainfall.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate/rainfall.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
103a107
>         self.time_cycle = self.data.time_cycle
107c111
<             event_threshold, scale_fac, offset)
---
>             event_threshold, scale_fac, offset, time_cycle=self.time_cycle)
128c132,133
<     def _calculate_correlation(self, event_threshold, scale_fac, offset):
---
>     def _calculate_correlation(self, event_threshold, scale_fac, offset,
>                                time_cycle):
147a153,156
>         :type time_cycle: number (int)
>         :arg time_cycle: Length of annual cycle in given data (monthly: 12,
>                          daily: 365 etc.)
> 
236c245
<         no_rain_mask = rainfall != 0
---
>         no_rain_mask = (rainfall != 0)
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate/spearman.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate/spearman.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate/tsonis.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/climate/tsonis.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
18a22,26
> #
> #  Import essential packages
> #
> 
> #  Import NumPy for the array object and fast numerics
23c31
< from ..core.cache import Cached
---
> from ..core.network import cached_const
25a34,37
> #
> #  Define class TsonisClimateNetwork
> #
> 
112,114c124,125
<         return (f'TsonisClimateNetwork:\n{ClimateNetwork.__str__(self)}\n'
<                 f'Use only data points from winter months: '
<                 f'{self.winter_only()}')
---
>         return (f'TsonisClimateNetwork:\n{ClimateNetwork.__str__(self)}\n' +
>         f'Use only data points from winter months: {self.winter_only()}')
191c202
<     @Cached.method()
---
>     @cached_const('base', 'correlation')
Only in source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/core: cache.py
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/core/data.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/core/data.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
20c23,25
< from typing import Optional
---
> #
> #  Imports
> #
22a28
> 
31a38
> 
34a42,45
> #
> #  Define class Data
> #
> 
50,52c61,62
<     def __init__(self, observable: np.ndarray, grid: GeoGrid,
<                  observable_name: str = None, observable_long_name: str = None,
<                  window: Optional[dict] = None, silence_level: int = 0):
---
>     def __init__(self, observable, grid, observable_name=None,
>                  observable_long_name=None, window=None, silence_level=0):
65,66c75,76
<         :arg grid: The GeoGrid representing the spatial coordinates associated
<             to the time series and their temporal sampling.
---
>         :arg grid: The Grid representing the spatial coordinates associated to
>             the time series and their temporal sampling.
76,77d85
< 
<         assert isinstance(grid, GeoGrid)
110,112c118,121
<         return (f"Data: {self.grid.N} grid points, "
<                 f"{self.grid.n_grid_points} measurements.\n"
<                 f"Geographical boundaries:\n{self.grid.print_boundaries()}")
---
>         return ('Data: %i grid points, %i measurements.\n'
>                 'Geographical boundaries:\n%s') % (
>                     self.grid.N, self.grid.n_grid_points,
>                     self.grid.print_boundaries())
126a136,145
>     def clear_cache(self):
>         """
>         Clean up cache.
> 
>         Is reversible, since all cached information can be recalculated from
>         basic data.
>         """
>         self.grid.clear_cache()
>         self._full_grid.clear_cache()
> 
280c299
<             res["grid"] = GeoGrid.RegularGrid(time, (lat_grid, lon_grid),
---
>             res["grid"] = GeoGrid.RegularGrid(time, lat_grid, lon_grid,
374a394
>         print("File format:", f.file_format)
380c400
<             print(f"{name} ({len(obj)})")
---
>             print("%s (%i)" % (name, len(obj)))
575c595,598
<             raise ValueError(f"Data type {var_type} not supported.")
---
>             print("Data type %s variable %s for rescaling array "
>                   "not supported!" % var_type)
>             scale_factor = 1.
>             add_offset = 0.
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/core/_ext/__init__.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/core/_ext/__init__.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/core/_ext/numerics.pyx lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/core/_ext/numerics.pyx
0a1,2
> # -*- coding: utf-8 -*-
> #
2,3c4,5
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
29a32,35
>     void _do_nsi_hamming_clustering_fast(int n2, int nActiveIndices,
>         float mind0, float minwp0, int lastunited, int part1, int part2,
>         double *distances, int *theActiveIndices, double *linkedWeights,
>         double *weightProducts, double *errors, double *result, int *mayJoin)
39,60c45,71
< # parameters for `_randomly_rewire_geomodel()`
< ctypedef bint (*rewire_cond_len)(FIELD_t[:,:], float, int, int, int, int)
< ctypedef bint (*rewire_cond_deg)(DEGREE_t[:], int, int, int, int)
< 
< cdef:
<     # condition C1
<     inline bint cond_len_c1(
<         FIELD_t[:,:] D, float eps, int s, int t, int k, int l):
<         return (
<             (abs(D[s,t] - D[k,t]) < eps and abs(D[k,l] - D[s,l]) < eps) or
<             (abs(D[s,t] - D[s,l]) < eps and abs(D[k,l] - D[k,t]) < eps))
<     # condition C2
<     inline bint cond_len_c2(
<         FIELD_t[:,:] D, float eps, int s, int t, int k, int l):
<         return (
<             abs(D[s,t] - D[s,l]) < eps and abs(D[t,s] - D[t,k]) < eps and
<             abs(D[k,l] - D[k,t]) < eps and abs(D[l,k] - D[l,s]) < eps)
<     # invariance of degree-degree correlations
<     inline bint cond_deg_corr(DEGREE_t[:] degree, int s, int t, int k, int l):
<         return (degree[s] == degree[k] and degree[t] == degree[l])
<     # tautology
<     rewire_cond_deg cond_deg_true = NULL
---
> ctypedef bint (*rewire_cond_len)(
>     ndarray[FIELD_t, ndim=2], float, int, int, int, int)
> ctypedef bint (*rewire_cond_deg)(
>     ndarray[DEGREE_t, ndim=1], int, int, int, int)
> 
> 
> # condition C1
> cdef bint cond_len_c1(
>     ndarray[FIELD_t, ndim=2] D, float eps, int s, int t, int k, int l):
>     return (
>         (abs(D[s,t] - D[k,t]) < eps and abs(D[k,l] - D[s,l]) < eps) or
>         (abs(D[s,t] - D[s,l]) < eps and abs(D[k,l] - D[k,t]) < eps))
> 
> # condition C2
> cdef bint cond_len_c2(
>     ndarray[FIELD_t, ndim=2] D, float eps, int s, int t, int k, int l):
>     return (
>         abs(D[s,t] - D[s,l]) < eps and abs(D[t,s] - D[t,k]) < eps and
>         abs(D[k,l] - D[k,t]) < eps and abs(D[l,k] - D[l,s]) < eps)
> 
> # invariance of degree-degree correlations
> cdef bint cond_deg_corr(
>     ndarray[DEGREE_t, ndim=1] degree, int s, int t, int k, int l):
>     return (degree[s] == degree[k] and degree[t] == degree[l])
> 
> # tautology
> cdef rewire_cond_deg cond_deg_true = NULL
99,100c110,112
<     cdef:
<         ndarray[DEGREE_t, ndim=1] null = np.array([], dtype=DEGREE)
---
> 
>     cdef ndarray[DEGREE_t, ndim=1] null = np.array([], dtype=DEGREE)
> 
103a116
> 
107,108c120,122
<     cdef:
<         ndarray[DEGREE_t, ndim=1] null = np.array([], dtype=DEGREE)
---
> 
>     cdef ndarray[DEGREE_t, ndim=1] null = np.array([], dtype=DEGREE)
> 
111a126
> 
114a130
> 
400c416,419
<     ndarray[DEGREE_t, ndim=1] k,
---
>     ndarray[DEGREE_t, ndim=1] k, int j,
>     ndarray[DFIELD_t, ndim=1] betweenness_to_j,
>     ndarray[DFIELD_t, ndim=1] excess_to_j,
>     ndarray[NODE_t, ndim=1] offsets,
403,406c422
<     ndarray[NODE_t, ndim=1] targets):
<     """
<     Performs Newman's algorithm. [Newman2001]_
<     """
---
>     ndarray[NODE_t, ndim=1] flat_predecessors):
409,410c425
<         long int E = len(flat_neighbors)
<         int j, qi, oi, queue_len, l_index, ql
---
>         int qi, oi, queue_len, l_index, ql
413,414c428,429
<         ndarray[NODE_t, ndim=1] offsets = np.zeros(N, dtype=NODE)
<         ndarray[NODE_t, ndim=1] distances_to_j = np.ones(N, dtype=NODE)
---
>         ndarray[NODE_t, ndim=1] distances_to_j =\
>             2 * N * np.ones(N, dtype=NODE)
416d430
<         ndarray[NODE_t, ndim=1] flat_predecessors = np.zeros(E, dtype=NODE)
419,491d432
<         ndarray[DFIELD_t, ndim=1] betweenness_to_j = np.zeros(N, dtype=DFIELD)
<         ndarray[DFIELD_t, ndim=1] excess_to_j = np.zeros(N, dtype=DFIELD)
<         ndarray[DFIELD_t, ndim=1] betweenness_times_w = np.zeros(N, dtype=DFIELD)
< 
<     # init node offsets
<     # NOTE: We don't use k.cumsum() since that uses too much memory!
<     for i in range(1, N):
<         offsets[i] = offsets[i-1] + k[i-1]
< 
<     for j in targets:
<         # init distances to j and queue of nodes by distance from j
<         distances_to_j.fill(2 * N)
<         n_predecessors.fill(0)
<         flat_predecessors.fill(0)
<         queue.fill(0)
<         multiplicity_to_j.fill(0)
< 
<         # init contribution of paths ending in j to the betweenness of l
<         for l in range(N):
<             excess_to_j[l] = betweenness_to_j[l] = is_source[l] * w[l]
< 
<         distances_to_j[j] = 0
<         queue[0] = j
<         queue_len = 1
<         multiplicity_to_j[j] = w[j]
< 
<         # process the queue forward and grow it on the way: (this is the
<         # standard breadth-first search giving all the shortest paths to j)
<         qi = 0
<         while qi < queue_len:
<             i = queue[qi]
<             if i == -1:
<                 # this should never happen ...
<                 print("Opps: %d,%d,%d\n" % qi, queue_len, i)
<                 break
<             next_d = distances_to_j[i] + 1
<             # iterate through all neighbors l of i
<             oi = offsets[i]
<             for l_index in range(oi, oi+k[i]):
<                 # if on a shortest j-l-path, register i as predecessor of l
<                 l = flat_neighbors[l_index]
<                 dl = distances_to_j[l]
<                 if dl >= next_d:
<                     fi = offsets[l] + n_predecessors[l]
<                     n_predecessors[l] += 1
<                     flat_predecessors[fi] = i
<                     multiplicity_to_j[l] += w[l] * multiplicity_to_j[i]
<                     if dl > next_d:
<                         distances_to_j[l] = next_d
<                         queue[queue_len] = l
<                         queue_len += 1
<             qi += 1
< 
<         # process the queue again backward: (this is Newman's 2nd part where the
<         # contribution of paths ending in j to the betweenness of all nodes is
<         # computed recursively by traversing the shortest paths backwards)
<         for ql in range(queue_len-1, -1, -1):
<             l = queue[ql]
<             if l == -1:
<                 print("Opps: %d,%d,%d\n" % ql, queue_len, l)
<                 break
<             if l == j:
<                 # set betweenness and excess to zero
<                 betweenness_to_j[l] = excess_to_j[l] = 0
<             else:
<                 # otherwise, iterate through all predecessors i of l:
<                 base_factor = w[l] / multiplicity_to_j[l]
<                 ol = offsets[l]
<                 for fi in range(ol, ol+n_predecessors[l]):
<                     # add betweenness to predecessor
<                     i = flat_predecessors[fi]
<                     betweenness_to_j[i] += betweenness_to_j[l] * base_factor * \
<                         multiplicity_to_j[i]
493,494c434,494
<         betweenness_times_w += w[j] * (betweenness_to_j - excess_to_j)
<     return betweenness_times_w
---
>     # init distances to j and queue of nodes by distance from j
>     for l in range(N):
>         # distances_to_j[l] = 2 * N
>         # n_predecessors[l] = 0
>         # multiplicity_to_j[l] = 0.0
>         # initialize contribution of paths ending in j to the betweenness of l
>         excess_to_j[l] = betweenness_to_j[l] = is_source[l] * w[l]
> 
>     distances_to_j[j] = 0
>     queue[0] = j
>     queue_len = 1
>     multiplicity_to_j[j] = w[j]
> 
>     # process the queue forward and grow it on the way: (this is the standard
>     # breadth-first search giving all the shortest paths to j)
>     qi = 0
>     while qi < queue_len:
>     #for qi in range(queue_len):
>         i = queue[qi]
>         if i == -1:
>             # this should never happen ...
>             print("Opps: %d,%d,%d\n" % qi, queue_len, i)
>             break
>         next_d = distances_to_j[i] + 1
>         #iterate through all neighbors l of i
>         oi = offsets[i]
>         for l_index in range(oi, oi+k[i]):
>             # if on a shortes j-l-path, register i as predecessor of l
>             l = flat_neighbors[l_index]
>             dl = distances_to_j[l]
>             if dl >= next_d:
>                 fi = offsets[l] + n_predecessors[l]
>                 n_predecessors[l] += 1
>                 flat_predecessors[fi] = i
>                 multiplicity_to_j[l] += w[l] * multiplicity_to_j[i]
>                 if dl > next_d:
>                     distances_to_j[l] = next_d
>                     queue[queue_len] = l
>                     queue_len += 1
>         qi += 1
> 
>     # process the queue again backward: (this is Newman's 2nd part where
>     # the contribution of paths ending in j to the betweenness of all nodes
>     # is computed recursively by traversing the shortest paths backwards)
>     for ql in range(queue_len-1, -1, -1):
>         l = queue[ql]
>         if l == -1:
>             print("Opps: %d,%d,%d\n" % ql, queue_len, l)
>             break
>         if l == j:
>             # set betweenness and excess to zero
>             betweenness_to_j[l] = excess_to_j[l] = 0
>         else:
>             # otherwise, iterate through all predecessors i of l:
>             base_factor = w[l] / multiplicity_to_j[l]
>             ol = offsets[l]
>             for fi in range(ol, ol+n_predecessors[l]):
>                 # add betweenness to predecessor
>                 i = flat_predecessors[fi]
>                 betweenness_to_j[i] += betweenness_to_j[l] * base_factor * \
>                     multiplicity_to_j[i]
563a564,822
> 
> 
> def _do_nsi_clustering_I(
>     int n_cands, ndarray[NODE_t, ndim=1] cands,
>     ndarray[DEGREE_t, ndim=1] D_cluster,
>     ndarray[DFIELD_t, ndim=1] w, double d0,
>     ndarray[NODE_t, ndim=1] D_firstpos, ndarray[NODE_t, ndim=1] D_nextpos,
>     int N, dict dict_D, dict dict_Delta):
> 
>     cdef:
>         int ca, ij, i, j, posh, h, ih, jh
>         double wi, wj, wc, wjd0, Delta_inc, wh, whd0, Dih, Djh, Dch_wc, \
>             Dch_wc_Dih_wi, Dch_wc_Djh_wj, Dch_wc_whd0
> 
>     # loop thru candidates:
>     for ca in range(n_cands):
>         ij = cands[ca]
>         i = int(ij/N)
>         j = ij%N
>         wi = w[i]
>         wj = w[j]
>         wc = wi + wj
>         wjd0 = wj * d0
>         Delta_inc = 0.0
>         # loop thru all nbs of i other than j:
>         posh = D_firstpos[i]
>         while (posh > 0):
>             h = D_cluster[posh]
>             if (h != j):
>                 ih = N*i+h
>                 jh = N*j+h
>                 wh = w[h]
>                 Dih = dict_D[ih]
>                 if (dict_D.has_key(jh)):
>                     Djh = dict_D[jh]
>                 else:
>                     Djh = wh * wjd0
> 
>                 Dch_wc = (Dih + Djh) / wc
>                 Dch_wc_Dih_wi = Dch_wc - Dih/wi
>                 Dch_wc_Djh_wj = Dch_wc - Djh/wj
> 
>                 Delta_inc += (wi * Dch_wc_Dih_wi*Dch_wc_Dih_wi +
>                               wj * Dch_wc_Djh_wj*Dch_wc_Djh_wj) / wh
> 
>             posh = D_nextpos[posh]
> 
>         # loop thru all nbs of j other than i which are not nbs of i:
>         posh = D_firstpos[j]
>         while (posh > 0):
>             h = D_cluster[posh]
>             ih = N*i+h
>             wh = w[h]
>             whd0 = wh * d0
>             if (h != i and not dict_D.has_key(ih)):
>                 jh = N*j+h
>                 Djh = dict_D[jh]
>                 Dch_wc = (wi*whd0 + Djh) / wc
>                 Dch_wc_whd0 = Dch_wc - whd0
>                 Dch_wc_Djh_wj = Dch_wc - Djh/wj
> 
>                 Delta_inc += (wi * Dch_wc_whd0*Dch_wc_whd0 +
>                               wj * Dch_wc_Djh_wj*Dch_wc_Djh_wj) / wh
> 
>             posh = D_nextpos[posh]
> 
>         dict_Delta[ij] = float(dict_Delta[ij]) + Delta_inc
> 
>     return dict_Delta
> 
> 
> def _do_nsi_clustering_II(int a, int b,
>     ndarray[DEGREE_t, ndim=1] D_cluster,
>     ndarray[DFIELD_t, ndim=1] w, double d0,
>     ndarray[NODE_t, ndim=1] D_firstpos, ndarray[NODE_t, ndim=1] D_nextpos,
>     int N, dict dict_D, dict dict_Delta):
> 
>     cdef:
>         double  wa = w[a], wb = w[b], wc = wa+wb, wad0 = wa*d0, wbd0 = wb*d0, \
>                 wa1, wa1sq, wa1d0, Da1a1, Da1a, Da1b, Da1c, wb1, wb1d0, wb1sq, \
>                 wa1b1, wc1, Db1b1, Da1b1, Dc1c1_wc1sq, \
>                 Dc1c1_wc1sq_Da1a1_wa1sq, Dc1c1_wc1sq_Da1b1_wa1b1, \
>                 Dc1c1_wc1sq_Db1b1_wb1sq, \
>                 Delta_new, Delta_inc, wc2 = 0, Da1c2, Db1c2, \
>                 Dc1c2_wc1, Dc1c2_wc1_Da1c2_wa1, Dc1c2_wc1_Db1c2_wb1, \
>                 Dc1c2_wc1_wc2_d0, Db1a, Db1b, Db1c, Dc1a_wc1, Dc1b_wc1, \
>                 Dc1c_wc1, Dc1c_wc1_Da1c_wa1, Dc1a_wc1_Da1a_wa1, \
>                 Dc1b_wc1_Da1b_wa1, Dc1c_wc1_Db1c_wb1, Dc1a_wc1_Db1a_wb1, \
>                 Dc1b_wc1_Db1b_wb1
>         int N1 = N+1, posa1 = D_firstpos[a] # a meaning c!
>         int a1, a1N, a1a1, a1a, a1b, posb1, b1, b1N, posc2, b1c2 = 0, b1a, b1b, \
>             a1b1key, b1b1
>         DEGREE_t c2
> 
>     while (posa1 > 0):
>         a1 = D_cluster[posa1]
>         a1N = a1*N
>         a1a1 = a1*N1
>         a1a = a1N+a
>         a1b = a1N+b
>         wa1 = w[a1]
>         wa1sq = wa1*wa1
>         wa1d0 = wa1*d0
>         if (dict_D.has_key(a1a1)):
>             Da1a1 = dict_D[a1a1]
>         else:
>             Da1a1 = 0.0
>         if (dict_D.has_key(a1a)):
>             Da1a = dict_D[a1a]
>         else:
>             Da1a = wa1*wad0
>         if (dict_D.has_key(a1b)):
>             Da1b = dict_D[a1b]
>         else:
>             Da1b = wa1*wbd0
> 
>         Da1c = Da1a + Da1b
>         posb1 = D_firstpos[a1]
> 
>         while (posb1 > 0):
>             b1 = D_cluster[posb1]
>             b1N = b1*N
>             b1b1 = b1*N1
> 
>             if (a1 < b1):
>                 a1b1key = a1N+b1
>             else:
>                 a1b1key = b1N+a1
>             if (dict_Delta.has_key(a1b1key)):
>                 wb1 = w[b1]
>                 wb1d0 = wb1 * d0
>                 wb1sq = wb1*wb1
>                 wa1b1 = wa1*wb1
>                 wc1 = wa1+wb1
>                 if (dict_D.has_key(b1b1)):
>                     Db1b1 = dict_D[b1b1]
>                 else:
>                     Db1b1 = 0.0
>                 if (dict_D.has_key(a1b1key)):
>                     Da1b1 = dict_D[a1b1key]
>                 else:
>                     Da1b1 = wb1 * wa1d0
>                 Dc1c1_wc1sq = (Da1a1+Db1b1+<float>2*Da1b1) / (wc1*wc1)
>                 if (b1 == a): # a meaning c!
>                     Dc1c1_wc1sq_Da1a1_wa1sq = Dc1c1_wc1sq - Da1a1/wa1sq
>                     Dc1c1_wc1sq_Db1b1_wb1sq = Dc1c1_wc1sq - Db1b1/wb1sq
>                     Dc1c1_wc1sq_Da1b1_wa1b1 = Dc1c1_wc1sq - Da1b1/wa1b1
>                     Delta_new = wa1sq * Dc1c1_wc1sq_Da1a1_wa1sq * \
>                                 Dc1c1_wc1sq_Da1a1_wa1sq + \
>                                 wb1sq * Dc1c1_wc1sq_Db1b1_wb1sq * \
>                                 Dc1c1_wc1sq_Db1b1_wb1sq + \
>                                 <float>2 * wa1b1 * Dc1c1_wc1sq_Da1b1_wa1b1 * \
>                                 Dc1c1_wc1sq_Da1b1_wa1b1
>                     # loop thru all nbs c2 of a1 other than b1:
>                     posc2 = D_firstpos[a1]
>                     while (posc2 > 0):
>                         c2 = D_cluster[posc2]
>                         if (c2 != b1):
>                             b1c2 = b1N+c2
>                             wc2 = w[c2]
>                             Da1c2 = dict_D[a1N+c2]
>                             if (dict_D.has_key(b1c2)):
>                                 Db1c2 = dict_D[b1c2]
>                             else:
>                                 Db1c2 = wc2*wb1d0
>                             Dc1c2_wc1 = (Da1c2 + Db1c2) / wc1
>                             Dc1c2_wc1_Da1c2_wa1 = Dc1c2_wc1 - Da1c2/wa1
>                             Dc1c2_wc1_Db1c2_wb1 = Dc1c2_wc1 - Db1c2/wb1
>                             Delta_new += (wa1 * Dc1c2_wc1_Da1c2_wa1 * \
>                                           Dc1c2_wc1_Da1c2_wa1 + \
>                                           wb1 * Dc1c2_wc1_Db1c2_wb1 * \
>                                           Dc1c2_wc1_Db1c2_wb1) / wc2
> 
>                         posc2 = D_nextpos[posc2]
> 
>                     #  loop thru all nbs of b1 other than a1
>                     #  which are not nbs of a1:
>                     posc2 = D_firstpos[b1]
>                     while (posc2 > 0):
>                         c2 = D_cluster[posc2]
>                         if (c2 != a1):
>                             if not (dict_D.has_key(a1N+c2)):
>                                 b1c2 = b1N+c2
>                                 wc2 = w[c2]
> 
>                             if (dict_D.has_key(b1c2)):
>                                 Db1c2 = dict_D[b1c2]
>                             else:
>                                 Db1c2 = wc2*wb1d0
>                             Dc1c2_wc1 = (wc2*wa1d0 + Db1c2) / wc1
>                             Dc1c2_wc1_wc2_d0 = Dc1c2_wc1 - wc2*d0
>                             Dc1c2_wc1_Db1c2_wb1 = Dc1c2_wc1 - Db1c2/wb1
>                             Delta_new += (wa1 * Dc1c2_wc1_wc2_d0 * \
>                                           Dc1c2_wc1_wc2_d0 + \
>                                           wb1 * Dc1c2_wc1_Db1c2_wb1 * \
>                                           Dc1c2_wc1_Db1c2_wb1) / wc2
> 
>                         posc2 = D_nextpos[posc2]
>                     dict_Delta[a1b1key] = Delta_new
> 
>                 else:
>                     b1a = b1N+a
>                     b1b = b1N+b
>                     if (dict_D.has_key(b1a)):
>                         Db1a = dict_D[b1a]
>                     else:
>                         Db1a = wb1*wad0
>                     if (dict_D.has_key(b1b)):
>                         Db1b = dict_D[b1b]
>                     else:
>                         Db1b = wb1*wbd0;
>                     Db1c = Db1a + Db1b
>                     wc1 = wa1 + wb1
>                     Dc1a_wc1 = (Da1a + Db1a) / wc1
>                     Dc1b_wc1 = (Da1b + Db1b) / wc1
>                     Dc1c_wc1 = (Da1c + Db1c) / wc1
>                     Dc1c_wc1_Da1c_wa1 = Dc1c_wc1 - Da1c/wa1
>                     Dc1a_wc1_Da1a_wa1 = Dc1a_wc1 - Da1a/wa1
>                     Dc1b_wc1_Da1b_wa1 = Dc1b_wc1 - Da1b/wa1
>                     Dc1c_wc1_Db1c_wb1 = Dc1c_wc1 - Db1c/wb1
>                     Dc1a_wc1_Db1a_wb1 = Dc1a_wc1 - Db1a/wb1
>                     Dc1b_wc1_Db1b_wb1 = Dc1b_wc1 - Db1b/wb1
>                     Delta_inc = (Dc1c_wc1_Da1c_wa1*Dc1c_wc1_Da1c_wa1/wc - \
>                                  Dc1a_wc1_Da1a_wa1*Dc1a_wc1_Da1a_wa1/wa - \
>                                  Dc1b_wc1_Da1b_wa1*Dc1b_wc1_Da1b_wa1/wb)/ \
>                                 wa1 + \
>                                 (Dc1c_wc1_Db1c_wb1*Dc1c_wc1_Db1c_wb1/wc - \
>                                  Dc1a_wc1_Db1a_wb1*Dc1a_wc1_Db1a_wb1/wa - \
>                                  Dc1b_wc1_Db1b_wb1*Dc1b_wc1_Db1b_wb1/wb)/ \
>                                 wb1
>                     Delta_inc *= <float>2
>                     dict_Delta[a1b1key] = float(dict_Delta[a1b1key])+Delta_inc
> 
>             posb1 = D_nextpos[posb1]
> 
>         posa1 = D_nextpos[posa1]
> 
>     return dict_Delta
> 
> 
> def _do_nsi_hamming_clustering(int n2, int nActiveIndices, float mind0,
>     float minwp0, int lastunited, int part1, int part2,
>     ndarray[DFIELD_t, ndim=2] distances,
>     ndarray[NODE_t, ndim=1] theActiveIndices,
>     ndarray[DFIELD_t, ndim=2] linkedWeights,
>     ndarray[DFIELD_t, ndim=2] weightProducts,
>     ndarray[DFIELD_t, ndim=2] errors,
>     ndarray[DFIELD_t, ndim=1] results,
>     ndarray[MASK_t, ndim=2] mayJoin):
> 
>     _do_nsi_hamming_clustering_fast(n2, nActiveIndices, mind0, minwp0,
>         lastunited, part1, part2,
>         <DFIELD_t*> cnp.PyArray_DATA(distances),
>         <int*> cnp.PyArray_DATA(theActiveIndices),
>         <DFIELD_t*> cnp.PyArray_DATA(linkedWeights),
>         <DFIELD_t*> cnp.PyArray_DATA(weightProducts),
>         <DFIELD_t*> cnp.PyArray_DATA(errors),
>         <DFIELD_t*> cnp.PyArray_DATA(results),
>         <int*> cnp.PyArray_DATA(mayJoin))
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/core/_ext/src_numerics.c lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/core/_ext/src_numerics.c
6c6
< * Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
---
> * Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
17a18,113
> void _do_nsi_hamming_clustering_fast(int n2, int nActiveIndices, float mind0,
>     float minwp0, int lastunited, int part1, int part2, double *distances,
>     int *theActiveIndices, double *linkedWeights, double *weightProducts,
>     double *errors, double *result, int *mayJoin)  {
> 
>     
>     int i1, i2, i3, c3;
>     int newpart1=0;
>     int newpart2=0;
>     double d, lw, mind=mind0, minwp=minwp0;
>     for (i1=0; i1<nActiveIndices; i1++) {
>         int c1 = theActiveIndices[i1];
>         if ((lastunited==-1) || (c1==lastunited)) {
>             for (i2=0; i2<i1; i2++) {
>                 int c2 = theActiveIndices[i2];
>                 if (mayJoin[c1*n2+c2]>0) {
>                     d = 0.0;
>                     for (i3=0; i3<i2; i3++) {
>                         c3 = theActiveIndices[i3];
>                         lw = linkedWeights[c1*n2+c3]
>                                 + linkedWeights[c2*n2+c3];
>                         d += fmin(lw,weightProducts[c1*n2+c3]
>                                 + weightProducts[c2*n2+c3]-lw)
>                                 - errors[c1*n2+c3] - errors[c2*n2+c3];
>                     }
>                     for (i3=i2+1; i3<i1; i3++) {
>                         c3 = theActiveIndices[i3];
>                         lw = linkedWeights[c1*n2+c3]
>                                 + linkedWeights[c2*n2+c3];
>                         d += fmin(lw,weightProducts[c1*n2+c3]
>                                 + weightProducts[c2*n2+c3]-lw)
>                                 - errors[c1*n2+c3] - errors[c2*n2+c3];
>                     }
>                     for (i3=i1+1; i3<nActiveIndices; i3++) {
>                         c3 = theActiveIndices[i3];
>                         lw = linkedWeights[c1*n2+c3]
>                                 + linkedWeights[c2*n2+c3];
>                         d += fmin(lw,weightProducts[c1*n2+c3]
>                                 + weightProducts[c2*n2+c3]-lw)
>                                 - errors[c1*n2+c3] - errors[c2*n2+c3];
>                     }
>                     double e = weightProducts[c1*n2+c2]
>                                 - 2.0*linkedWeights[c1*n2+c2];
>                     if (e>0.0) d += e;
>                     distances[c1*n2+c2] = d;
>                     if ((d<mind) ||
>                             ((d==mind) &&
>                                 (weightProducts[c1*n2+c2]<minwp))) {
>                         mind = d;
>                         minwp = weightProducts[c1*n2+c2];
>                         newpart1 = c1;
>                         newpart2 = c2;
>                     }
>                 }
>             }
>         } else {
>             for (i2=0; i2<i1; i2++) {
>                 int c2 = theActiveIndices[i2];
>                 if (mayJoin[c1*n2+c2]>0) {
>                     double lw_united = linkedWeights[c1*n2+lastunited]
>                                        + linkedWeights[c2*n2+lastunited],
>                             lw_part1 = linkedWeights[c1*n2+part1]
>                                        + linkedWeights[c2*n2+part1],
>                             lw_part2 = linkedWeights[c1*n2+part2]
>                                        + linkedWeights[c2*n2+part2];
>                     distances[c1*n2+c2] += (
>                         (fmin(lw_united, weightProducts[c1*n2+lastunited]
>                               + weightProducts[c2*n2+lastunited]
>                               - lw_united)
>                            - errors[c1*n2+lastunited]
>                            - errors[c2*n2+lastunited])
>                         - (fmin(lw_part1,weightProducts[c1*n2+part1]
>                                 + weightProducts[c2*n2+part1] - lw_part1)
>                            - errors[c1*n2+part1] - errors[c2*n2+part1])
>                         - (fmin(lw_part2,weightProducts[c1*n2+part2]
>                                 + weightProducts[c2*n2+part2] -lw_part2)
>                            - errors[c1*n2+part2] - errors[c2*n2+part2]));
>                     d = distances[c1*n2+c2];
>                     if ((d<mind) ||
>                             ((d==mind) &&
>                                 (weightProducts[c1*n2+c2]<minwp))) {
>                         mind = d;
>                         minwp = weightProducts[c1*n2+c2];
>                         newpart1 = c1;
>                         newpart2 = c2;
>                     }
>                 }
>             }
>         }
>     }
>     result[0] = mind;
>     result[1] = newpart1;
>     result[2] = newpart2;
> }
> 
> 
25c121
<     double J=0;
---
>     double I=0;
29c125
<             J = 0.0;
---
>             I = 0.0;
35c131
<                     J += admittance[i*N+j]*
---
>                     I += admittance[i*N+j]*
41c137
<             VCFB += 2.0*J/(N*(N-1));
---
>             VCFB += 2.0*I/(N*(N-1));
56c152
<     double J = 0.0;
---
>     double I = 0.0;
60c156
<             J = 0.0;
---
>             I = 0.0;
63c159
<                     J += admittance[i*N+j]*\
---
>                     I += admittance[i*N+j]*\
68c164
<             ECFB[i*N+j] += (float) (2.* J/(N*(N-1)));
---
>             ECFB[i*N+j] += (float) (2.* I/(N*(N-1)));
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/core/_ext/types.pxd lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/core/_ext/types.pxd
0a1,2
> # -*- coding: utf-8 -*-
> #
2,3c4,5
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/core/_ext/types.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/core/_ext/types.py
0a1,2
> # -*- coding: utf-8 -*-
> #
2,3c4,5
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/core/geo_grid.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/core/geo_grid.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
18a22,27
> #
> #  Import essential packages
> #
> 
> 
> #  array object and fast numerics
19a29
> 
26a37
> #  Cythonized functions
29c40,41
< from .cache import Cached
---
> 
> #  Grid base class
32a45,49
> #
> #  Define class GeoGrid
> #
> 
> 
33a51
> 
46,48c64
<     def __init__(self, time_seq: np.ndarray,
<                  lat_seq: np.ndarray, lon_seq: np.ndarray,
<                  silence_level: int = 0):
---
>     def __init__(self, time_seq, lat_seq, lon_seq, silence_level=0):
66a83,86
>         #  Cache
>         self._angular_distance = None
>         self._angular_distance_cached = False
> 
71,72c91,103
<         return (f"GeoGrid: {self._grid_size['space']} grid points, "
<                 f"{self._grid_size['time']} timesteps.")
---
>         return 'GeoGrid: %i grid points, %i timesteps.' % (
>             self._grid_size['space'], self._grid_size['time'])
> 
>     def clear_cache(self):
>         """
>         Clean up cache.
> 
>         Is reversible, since all cached information can be recalculated from
>         basic data.
>         """
>         if self._angular_distance_cached:
>             del self._angular_distance
>             self._angular_distance_cached = False
144c175
<     def RegularGrid(time_seq, space_grid, silence_level=0):
---
>     def RegularGrid(time_seq, lat_grid, lon_grid, silence_level=0):
151,154c182,183
<         ...      time_seq=np.arange(2),
<         ...      space_grid=(np.array([0.,5.]),
<         ...                  np.array([1.,2.])),
<         ...      silence_level=2).lat_sequence()
---
>         ...      time_seq=np.arange(2), lat_grid=np.array([0.,5.]),
>         ...      lon_grid=np.array([1.,2.]), silence_level=2).lat_sequence()
157,160c186,187
<         ...      time_seq=np.arange(2),
<         ...      space_grid=(np.array([0.,5.]),
<         ...                  np.array([1.,2.])),
<         ...      silence_level=2).lon_sequence()
---
>         ...     time_seq=np.arange(2), lat_grid=np.array([0.,5.]),
>         ...     lon_grid=np.array([1.,2.]), silence_level=2).lon_sequence()
165,168c192,198
<         :type space_grid: tuple or list of two 1D Numpy arrays
<             ([n_lat], [n_lon])
<         :arg space_grid: The spatial grid, consisting of the latitudinal
<             and the longitudinal grid.
---
> 
>         :type lat_grid: 1D Numpy array [n_lat]
>         :arg lat_grid: The latitudinal grid.
> 
>         :type lon_grid: 1D Numpy array [n_lon]
>         :arg lon_grid: The longitudinal grid.
> 
175,180d204
<         try:
<             (lat_grid, lon_grid) = space_grid
<         except ValueError as e:
<             raise ValueError("'space_grid' must be a tuple or list of two "
<                              "items: lat_grid, lon_grid") from e
< 
387c411,439
<     @Cached.method(name="angular great circle distance")
---
>     def calculate_angular_distance(self):
>         """
>         Calculate and return the angular great circle distance matrix.
> 
>         **No normalization applied anymore!** Return values are in the range
>         0 to Pi.
> 
>         :rtype: 2D Numpy array [index, index]
>         :return: the angular great circle distance matrix (unit radians).
>         """
>         if self.silence_level <= 1:
>             print("Calculating angular great circle distance using Cython...")
> 
>         #  Get number of nodes
>         N = self.N
> 
>         #  Get sequences of cosLat, sinLat, cosLon and sinLon for all nodes
>         cos_lat = to_cy(self.cos_lat(), FIELD)
>         sin_lat = to_cy(self.sin_lat(), FIELD)
>         cos_lon = to_cy(self.cos_lon(), FIELD)
>         sin_lon = to_cy(self.sin_lon(), FIELD)
> 
>         #  Initialize cython cof of angular distance matrix
>         cosangdist = np.zeros((N, N), dtype=FIELD)
> 
>         _calculate_angular_distance(cos_lat, sin_lat, cos_lon, sin_lon,
>                                     cosangdist, N)
>         return np.arccos(cosangdist)
> 
390c442
<         Calculate the angular great circle distance matrix.
---
>         Return the angular great circle distance matrix.
408,419c460,464
<         #  Get number of nodes
<         N = self.N
<         #  Initialize cython cof of angular distance matrix
<         cosangdist = np.zeros((N, N), dtype=FIELD)
<         _calculate_angular_distance(
<             #  Get sequences of cosLat, sinLat, cosLon and sinLon for all nodes
<             to_cy(self.cos_lat(), FIELD),
<             to_cy(self.sin_lat(), FIELD),
<             to_cy(self.cos_lon(), FIELD),
<             to_cy(self.sin_lon(), FIELD),
<             cosangdist, N)
<         return np.arccos(cosangdist)
---
>         if not self._angular_distance_cached:
>             self._angular_distance = self.calculate_angular_distance()
>             self._angular_distance_cached = True
> 
>         return self._angular_distance
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/core/geo_network.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/core/geo_network.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
18a22
> # array object and fast numerics
19a24,26
> # random number generation
> from numpy import random
> # high performance graph theory tools written in pure ANSI-C
25a33,36
> #
> #  Define class GeoNetwork
> #
> 
26a38
> 
41,42c53,54
<     def __init__(self, grid: GeoGrid, adjacency=None, edge_list=None,
<                  directed=False, node_weight_type="surface", silence_level=0):
---
>     def __init__(self, grid, adjacency=None, edge_list=None, directed=False,
>                  node_weight_type="surface", silence_level=0):
65,67c77,82
<         assert isinstance(grid, GeoGrid)
<         self.grid: GeoGrid = grid
<         """GeoGrid object describing the network's spatial embedding"""
---
>         if grid.__class__.__name__ != "GeoGrid":
>             raise TypeError("GeoNetwork can only be created with GeoGrid!")
> 
>         self.grid = grid
>         """(Grid) - GeoGrid object describing the network's spatial
>         embedding"""
88a104,112
>     def clear_cache(self):
>         """
>         Clean up cache.
> 
>         Is reversible, since all cached information can be recalculated from
>         basic data.
>         """
>         SpatialNetwork.clear_cache(self)
> 
121d144
<     # pylint: disable=keyword-arg-before-vararg
123c146,147
<     def Load(filename, fileformat=None, silence_level=0, *args, **kwds):
---
>     def Load(filename_network, filename_grid, fileformat=None,
>              silence_level=0, *args, **kwds):
141,143c165,168
<         :arg tuple/list filename: Tuple or list of two strings, namely
<             the paths to the files containing the Network object
<             and the GeoGrid object (filename_network, filename_grid)
---
>         :arg str filename_network:  The name of the file where the Network
>             object is to be stored.
>         :arg str filename_grid:  The name of the file where the GeoGrid object
>             is to be stored (including ending).
156,161d180
<         try:
<             (filename_network, filename_grid) = filename
<         except ValueError as e:
<             raise ValueError("'filename' must be a tuple or list of two "
<                              "items: filename_network, filename_grid") from e
< 
175,176c194
<                          directed=graph.is_directed(),
<                          silence_level=silence_level)
---
>                          directed=graph.is_directed())
187,188c205,207
<         #  invalidate cache
<         net._mut_la += 1
---
>         #  Restore link attributes/weights
>         net.clear_paths_cache()
> 
262a282,349
>     #  Graph randomization methods
>     #
> 
>     #  FIXME: Check this method and implement in C++ via Cython for speed.
>     #  FIXME: Also improve documentation.
>     #  FIXME: Add example
>     def shuffled_by_distance_copy(self):
>         """
>         Return a copy of the network where all links in each node-distance
>         class have been randomly re-assigned.
> 
>         In other words, the result is a random network in which the link
>         probability only depends on the nodes' distance and is the same as in
>         the original network.
> 
>         :rtype: GeoNetwork
>         :return: the distance shuffled copy.
>         """
>         N = self.N
>         A = self.adjacency
>         D = self.grid.distance()
> 
>         #  Count pairs and links by distance
>         n_pairs_by_dist = {}
>         n_links_by_dist = {}
>         for j in range(0, N):
>             print(j)
>             for i in range(0, j):
>                 d = D[i, j]
>                 try:
>                     n_pairs_by_dist[d] += 1
>                 except KeyError:
>                     n_pairs_by_dist[d] = 1
>                 if A[i, j]:
>                     try:
>                         n_links_by_dist[d] += 1
>                     except KeyError:
>                         n_links_by_dist[d] = 1
> 
>         #  Determine link probabilities
>         p_by_dist = {}
>         for d in n_pairs_by_dist:
>             try:
>                 p_by_dist[d] = n_links_by_dist[d] * 1.0 / n_pairs_by_dist[d]
>             except KeyError:
>                 p_by_dist[d] = 0.0
>             print(d, p_by_dist[d])
>         del n_links_by_dist, n_pairs_by_dist
> 
>         #  Link new pairs with respective probability
>         A_new = np.zeros((N, N))
>         for j in range(0, N):
>             print("new ", j)
>             for i in range(0, j):
>                 d = D[i, j]
>                 if p_by_dist[d] >= np.random.random():
>                     A_new[i, j] = A_new[j, i] = 1
>                     print(i, j, d, p_by_dist[d])
> 
>         #  Create new GeoNetwork object based on A_new
>         net = GeoNetwork(adjacency=A_new, grid=self.grid,
>                          directed=self.directed,
>                          node_weight_type=self.node_weight_type,
>                          silence_level=self.silence_level)
> 
>         return net
> 
>     #
670c757
<         A = self.undirected_adjacency().toarray()
---
>         A = self.undirected_adjacency().A
757c844
<                                                           degree):
---
>                                                           degrees):
766,767c853,854
<         :type degree: 1D array [index]
<         :arg degree: The degree sequence.
---
>         :type degrees: 1D array [index]
>         :arg degrees: The degree sequence.
782,783c869,870
<         connectivity_weighted_distance[degree != 0] /= \
<             degree[degree != 0] * norm
---
>         connectivity_weighted_distance[degrees != 0] /= \
>             degrees[degrees != 0] * norm
806c893
<         A = self.undirected_adjacency().toarray()
---
>         A = self.undirected_adjacency().A
888a976,1021
>     #  TODO: Experimental code!
>     #  TODO: Improve documentation (Jobst).
>     def nsi_connected_hamming_cluster_tree(self, lon_closed=True,
>                                            lat_closed=False, alpha=0.01):
>         """
>         Perform NSI agglomerative clustering.
> 
>         Minimize in each step the Hamming distance between the original and
>         the clustered network, but only joins connected clusters.
> 
>         Return c,h where c[i,j] = i iff node j is in cluster no. i,
>         and 0 otherwise, and h is the corresponding list of total resulting
>         relative Hamming distance between 0 and 1. The cluster numbers for all
>         nodes and a k clusters solution is then c[:2*N-k,:].max(axis=0)
> 
>         :arg bool lon_closed: TODO
>         :arg bool lat_closed: TODO
>         :arg float alpha: TODO
> 
>         :rtype: TODO
>         :return: TODO
>         """
>         N = self.N
>         B = np.zeros((N, N)).astype("int")
>         width = self.grid.grid()["lon"].size
> 
>         for i in range(0, N):
>             if i % width > 0:
>                 B[i, i - 1] = 1
>             elif lon_closed:
>                 B[i, i - 1 + width] = 1
>             if i % width < width - 1:
>                 B[i, i + 1] = 1
>             elif lon_closed:
>                 B[i, i + 1 - width] = 1
>             if i >= width:
>                 B[i, i - width] = 1
>             elif lat_closed:
>                 B[i, i - width + N] = 1
>             if i < N - width:
>                 B[i, i + width] = 1
>             elif lat_closed:
>                 B[i, i + width - N] = 1
> 
>         return self.do_nsi_hamming_clustering(admissible_joins=B, alpha=alpha)
> 
899a1033,1187
> 
>     def boundary(self, nodes, geodesic=True, gap=0.0):
>         """
>         Return a list of ordered lists of nodes on the connected parts of the
>         boundary of a subset of nodes and a list of ordered lists of (lat,lon)
>         coordinates of the corresponding polygons
> 
>         * EXPERIMENTAL! *
>         """
>         #  Optional import for this experimental method
>         try:
>             import stripack  # @UnresolvedImport
>             # tries to import stripack.so which must have been compiled with
>             # f2py -c -m stripack stripack.f90
>         except ImportError:
>             raise RuntimeError("NOTE: stripack.so not available, boundary() \
>                                won't work.")
> 
>         N = self.N
>         nodes_set = set(nodes)
>         if len(nodes_set) >= N:
>             return [], [], [], [(0.0, 0.0)]
>         # find grid neighbours:
>         if geodesic:
>             if self.cartesian is not None:
>                 pos = self.cartesian
>             else:
>                 # find cartesian coordinates of nodes,
>                 # assuming a perfect unit radius sphere:
>                 lat = self.grid.lat_sequence() * np.pi / 180
>                 lon = self.grid.lon_sequence() * np.pi / 180
>                 pos = self.cartesian = np.zeros((N, 3))
>                 coslat = np.cos(lat)
>                 self.cartesian[:, 0] = coslat * np.sin(lon)
>                 self.cartesian[:, 1] = coslat * np.cos(lon)
>                 self.cartesian[:, 2] = np.sin(lat)
> 
>                 # find neighbours of each node in Delaunay triangulation,
>                 # sorted in counter-clockwise order, using stripack fortran
>                 # library:
>                 #  will contain 1-based node indices
>                 list_ = np.zeros(6*(N-2)).astype("int32")
>                 #  will contain 1-based list_ indices
>                 lptr = np.zeros(6*(N-2)).astype("int32")
>                 #  will contain 1-based list_ indices
>                 lend = np.zeros(N).astype("int32")
>                 lnew = 0
>                 near = np.zeros(N).astype("int32")
>                 foll = np.zeros(N).astype("int32")
>                 dist = np.zeros(N)
>                 ier = 0
>                 stripack.trmesh(self.cartesian[:, 0],
>                                 self.cartesian[:, 1],
>                                 self.cartesian[:, 2],
>                                 list_, lptr, lend, lnew,  # output vars
>                                 near, foll, dist,
>                                 ier)  # output var
>                 self.grid_neighbours = [None for i in range(N)]
>                 self.grid_neighbours_set = [None for i in range(N)]
>                 rN = range(N)
>                 for i in rN:
>                     nbsi = []
>                     ptr0 = ptr = lend[i]-1
>                     for j in rN:
>                         nbsi.append(list_[ptr]-1)
>                         ptr = lptr[ptr]-1
>                         if ptr == ptr0:
>                             break
>                     self.grid_neighbours[i] = nbsi
>                     self.grid_neighbours_set[i] = set(nbsi)
>         else:
>             raise NotImplementedError("Not yet implemented for \
>                                       lat-lon-regular grids!")
> 
>         remaining = nodes_set.copy()
>         boundary = []
>         shape = []
>         fullshape = []
>         representative = []
>         # find a node on the boundary and an outer neighbour:
>         lam = 0.5 + gap/2
>         lam1 = 1-lam
>         while remaining:
>             i = list(remaining)[0]
>             this_remove = [i]
>             cont = False
>             while self.grid_neighbours_set[i] <= nodes_set:
>                 i = self.grid_neighbours[i][int(np.floor(
>                     len(self.grid_neighbours[i])*random.uniform()))]
>                 if i not in remaining:  # we had this earlier
>                     cont = True
>                     break
>                 this_remove.append(i)
>             remaining -= set(this_remove)
>             # if len(nodes_set)==151: print(i,this_remove,remaining,cont)
>             if cont:
>                 continue
>             o = list(self.grid_neighbours_set[i] - nodes_set)[0]
> 
>             # traverse boundary:
>             partial_boundary = [i]
>             partial_shape = [lam*pos[i] + lam1*pos[o]]
>             partial_fullshape = [0.49*pos[i] + 0.51*pos[o]]
>             print(partial_shape)
>             steps = [(i, o)]
>             for it in range(N):  # at most this many steps we need
>                 nbi = self.grid_neighbours[i]
>                 j = nbi[0]
>                 try:
>                     j = nbi[(nbi.index(o)-1) % len(nbi)]
>                 except IndexError:
>                     print("O!", i, o, j, nbi, self.grid_neighbours[o], steps)
>                     raise
>                 if j in nodes_set:
>                     i = j
>                     partial_boundary.append(i)
>                     try:
>                         remaining.remove(i)
>                     except KeyError:
>                         pass
>                 else:
>                     partial_fullshape.append(
>                         0.32*pos[i]+0.34*pos[o]+0.34*pos[j])
>                     o = j
>                 partial_shape.append(lam*pos[i] + lam1*pos[o])
>                 partial_fullshape.append(0.49*pos[i] + 0.51*pos[o])
>                 if (i, o) in steps:
>                     break
>                 steps.append((i, o))
> 
>             mind2 = np.inf
>             latlon_shape = []
>             latlon_fullshape = []
>             length = len(partial_shape)-1
>             off = length/2
>             for it in range(length):
>                 pos1 = partial_shape[it]
>                 pos2 = partial_shape[int((it+off) % length)]
>                 latlon_shape.append(self.cartesian2latlon(pos1))
>                 d2 = ((pos2-pos1)**2).sum()
>                 if d2 < mind2:
>                     rep = self.cartesian2latlon((pos1+pos2)/2)
>                     mind2 = d2
>             latlon_shape.append(self.cartesian2latlon(partial_shape[-1]))
>             for it, _ in enumerate(partial_fullshape):
>                 pos1 = partial_fullshape[it]
>                 latlon_fullshape.append(self.cartesian2latlon(pos1))
> 
>             boundary.append(partial_boundary)
>             shape.append(latlon_shape)
>             fullshape.append(latlon_fullshape)
>             representative.append(rep)
> 
>         # TODO: sort sub-regions by descending size!
>         return boundary, shape, fullshape, representative
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/core/grid.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/core/grid.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
19,20c22,26
< from typing import Tuple
< from collections.abc import Hashable
---
> #
> #  Import essential packages
> #
> 
> #  Import pickle for loading and saving Python objects
22a29
> #  array object and fast numerics
25c32
< from .cache import Cached
---
> #  Cythonized functions
30c37,43
< class Grid(Cached):
---
> #
> #  Define class Grid
> #
> 
> 
> class Grid:
> 
42,43c55
<     def __init__(self, time_seq: np.ndarray, space_seq: np.ndarray,
<                  silence_level: int = 0):
---
>     def __init__(self, time_seq, space_seq, silence_level=0):
50,51c62,63
<         :type space_seq: 2D Numpy array [dim, index]
<         :arg space_seq: The sequences of spatial sampling points.
---
>         :type lat_seq: 2D Numpy array [dim, index]
>         :arg lat_seq: The sequences of spatial sampling points.
82,85c94,96
<     def __cache_state__(self) -> Tuple[Hashable, ...]:
<         # The following attributes are assumed immutable:
<         #   (N, _grid)
<         return ()
---
>         #  Cache
>         self._euclidean_distance = None
>         self._euclidean_distance_cached = False
91,92c102,103
<         return (f"Grid: {self._grid_size['space']} grid points, "
<                 f"{self._grid_size['time']} timesteps.")
---
>         return 'Grid: %i grid points, %i timesteps.' % (
>             self._grid_size['space'], self._grid_size['time'])
106,107c117,119
<             with open(filename, 'wb') as f:
<                 pickle.dump(self, f)
---
>             f = open(filename, 'wb')
>             pickle.dump(self, f)
>             f.close()
122c134,135
<         with open(filename, 'rb') as f:
---
>         try:
>             f = open(filename, 'rb')
123a137
>             f.close()
125c139,142
<         return grid
---
>             return grid
>         except IOError:
>             print("An error occurred while loading Grid instance from "
>                   f"pickle file {filename}")
286c303,323
<     @Cached.method()
---
>     def calculate_euclidean_distance(self):
>         """
>         Calculate and return the euclidean distance matrix.
> 
>         :rtype: 2D Numpy array [index, index]
>         :return: the euclidean distance matrix.
>         """
>         #  Get number of nodes
>         N_nodes = self.N
> 
>         #  Get sequences of coordinates
>         sequences = to_cy(self._grid["space"], FIELD)
> 
>         #  Get number of dimensions
>         N_dim = sequences.shape[0]
> 
>         distance = np.zeros((N_nodes, N_nodes), dtype=FIELD)
>         _calculate_euclidean_distance(sequences, distance, N_dim, N_nodes)
> 
>         return distance
> 
304,311c341,343
<         #  Get number of nodes
<         N_nodes = self.N
< 
<         #  Get sequences of coordinates
<         sequences = to_cy(self._grid["space"], FIELD)
< 
<         #  Get number of dimensions
<         N_dim = sequences.shape[0]
---
>         if not self._euclidean_distance_cached:
>             self._euclidean_distance = self.calculate_euclidean_distance()
>             self._euclidean_distance_cached = True
313,315c345
<         distance = np.zeros((N_nodes, N_nodes), dtype=FIELD)
<         _calculate_euclidean_distance(sequences, distance, N_dim, N_nodes)
<         return distance
---
>         return self._euclidean_distance
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/core/__init__.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/core/__init__.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
24a28,37
> 
> To do
> ~~~~~
>   - A lot - See current product backlog.
>   - Clean up MapPlots class -> Alex!?
> 
> Known Bugs
> ~~~~~~~~~~
>   - ...
> 
31c44
< from .network import Network, NetworkError, nz_coords
---
> from .network import Network, NetworkError, nz_coords, cached_const
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/core/interacting_networks.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/core/interacting_networks.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
19a23
> #  Import NumPy for the array object and fast numerics
23c27
< from ._ext.types import to_cy, ADJ, NODE, DWEIGHT, DFIELD
---
> from ._ext.types import to_cy, ADJ, NODE, WEIGHT, DWEIGHT, FIELD, DFIELD
27a32
> #  Import Network base class
30a36,39
> #
> #  Define class InteractingNetworks
> #
> 
67a77
>         #  Call constructor of parent class Network
464c474
<         return self.sp_A[node_list1, :][:, node_list2].toarray()
---
>         return self.sp_A[node_list1, :][:, node_list2].A
729c739
<         n_links = InteractingNetworks.number_internal_links(self, node_list)
---
>         n_links = self.number_internal_links(node_list)
798,799c808,809
<         cc = InteractingNetworks.cross_local_clustering(self,
<                                                         node_list1, node_list2)
---
>         cc = InteractingNetworks.cross_local_clustering(self, node_list1,
>          node_list2)
988c998
<             self, node_list1, node_list2, link_attribute)
---
>         self, node_list1, node_list2, link_attribute)
1139d1148
<         # pylint: disable=arguments-out-of-order
1328a1338,1339
>         #  Convert node lists to Numpy arrays
>         nodes1, nodes2 = np.array(node_list1), np.array(node_list2)
1419,1421c1430,1431
<         path_lengths = InteractingNetworks.cross_path_lengths(
<             self, node_list1, node_list2, link_attribute)
< 
---
>         path_lengths = InteractingNetworks.cross_path_lengths(self, node_list1,
>          node_list2, link_attribute)
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/core/netcdf_dictionary.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/core/netcdf_dictionary.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
84,86c87,88
<         text = (f'NetCDFDictionary:\nGlobal attributes:\n'
<                 f'{self.dict["global_attributes"]}\nVariables:')
< 
---
>         text = ('NetCDFDictionary:\nGlobal attributes:\n' +
>         f'{self.dict["global_attributes"]}\nVariables:')
88,89c90,91
<             text += (f'\n\t{key}\t-> array shape'
<                      f'{self.dict["variables"][key]["array"].shape}')
---
>             text += f'\n\t{key}\t-> array shape\
>              {self.dict["variables"][key]["array"].shape}'
97c99
<     def from_file(file_name, with_array='all'):
---
>     def from_file(file_name, with_array=('all')):
121c123
<         for dim_name, _ in cdf.dimensions.iteritems():
---
>         for dim_name, dim_obj in cdf.dimensions.iteritems():
136c138
<             if var in with_array or with_array == 'all':
---
>             if var in with_array or 'all' in with_array:
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/core/network.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/core/network.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
19a23,36
> # general TODO:
> # - find segfault problem in a.w. shortest path betweenness
> # - rename aw... to nsi... (node splitting invariant)
> # - implement "corrected" node splitting invariant measures named cnsi...
> #   (see paper)
> # - implement Newman modularity and iterative division
> # - treat type-related ambiguities more thoroughly
> #   (flatten(), list(...), astype(...) etc.)
> 
> #
> #  Import essential packages
> #
> 
> 
22,25c39
< from functools import partial
< from typing import Tuple, Optional
< from collections.abc import Hashable
< from multiprocessing import get_context, cpu_count
---
> from functools import wraps         # helper function for decorators
33d46
< from tqdm import tqdm, trange       # easy progress bar handling
37d49
< from .cache import Cached
41c53
<     to_cy, ADJ, MASK, NODE, DEGREE, DWEIGHT, DFIELD
---
>     to_cy, ADJ, MASK, NODE, DEGREE, WEIGHT, DWEIGHT, FIELD, DFIELD
44c56,60
<     _nsi_betweenness, _mpi_newman_betweenness, _mpi_nsi_newman_betweenness
---
>     _nsi_betweenness, _mpi_newman_betweenness, _mpi_nsi_newman_betweenness, \
>     _do_nsi_clustering_I, _do_nsi_clustering_II, _do_nsi_hamming_clustering
> 
> from ..utils import progressbar     # easy progress bar handling
> 
47d62
< #  Utilities
59a75,118
> def cache_helper(self, cat, key, msg, func, *args, **kwargs):
>     """
>     Cache result of a function in a subdict of :attr:`self.cache`.
> 
>     :arg str cat: cache category
>     :arg str key: cache key
>     :arg str msg: message to be displayed during first calculation
>     :arg func func: function to be cached
>     """
>     # categories can be added on the fly?!?!
>     self.cache.setdefault(cat, {})
> 
>     if self.cache[cat].setdefault(key) is None:
>         if msg is not None and self.silence_level <= 1:
>             print('Calculating ' + msg + '...')
>         self.cache[cat][key] = func(self, *args, **kwargs)
>     return self.cache[cat][key]
> 
> 
> def cached_const(cat, key, msg=None):
>     """
>     Cache result of decorated method in a fixed subdict of :attr:`self.cache`.
>     """
>     def wrapper(func):
>         @wraps(func)
>         def wrapped(self, *args, **kwargs):
>             return cache_helper(self, cat, key, msg, func, *args, **kwargs)
>         return wrapped
>     return wrapper
> 
> 
> def cached_var(cat, msg=None):
>     """
>     Cache result of decorated method in a variable subdict of
>     :attr:`self.cache`, specified as first argument to the decorated method.
>     """
>     def wrapper(func):
>         @wraps(func)
>         def wrapped(self, key=None, **kwargs):
>             return cache_helper(self, cat, key, msg, func, key, **kwargs)
>         return wrapped
>     return wrapper
> 
> 
73d131
< #  Doctest helpers
82c140
<             rounded = np.around(obj.astype(np.longdouble),
---
>             rounded = np.around(obj.astype(np.float128),
86,87d143
<         else:
<             raise TypeError('obj is of unsupported dtype kind.')
109a166,168
> #
> #  Define class Network
> #
111c170
< class Network(Cached):
---
> class Network:
168,182c227,235
<         self.directed: bool = directed
<         """indicates whether the network is directed"""
<         self.silence_level: int = silence_level
<         """higher -> less progress info"""
< 
<         self._mut_A: int = 0
<         """mutation count tracking `self.adjcency`"""
<         self._mut_nw: int = 0
<         """mutation count tracking `self.node_weights`"""
<         self._mut_la: int = 0
<         """mutation count tracking `self.graph.es`"""
< 
<         self.N: int = 0
<         """number of nodes"""
<         if n_nodes is not None:
---
>         self.directed = directed
>         """(bool) Indicates whether the network is directed."""
>         self.silence_level = silence_level
>         """(int>=0) higher -> less progress info"""
> 
>         if n_nodes is None:
>             self.N = 0
>             """(int>0) number of nodes"""
>         else:
185,188c238,241
<         self.n_links: int = 0
<         """number of links"""
<         self.link_density: float = 0
<         """proportion of linked node pairs"""
---
>         self.n_links = 0
>         """(int>0) number of links"""
>         self.link_density = 0
>         """(0<float<1) proportion of linked node pairs"""
190,191c243,244
<         self.sp_A: sp.csc_matrix = None
<         """
---
>         self.sp_A = None
>         """(sparse.csc_matrix([[int,int]]) with entries 0,1)
193,194c246
<         network is undirected.
<         """
---
>         network is undirected."""
197,201c249,255
<         self.graph: igraph.Graph = None
<         """embedded graph object providing some standard network measures"""
< 
<         self._node_weights: Optional[np.ndarray] = None
<         self.mean_node_weight: float = 0
---
>         self.graph = None
>         """(igraph.Graph) Embedded graph object providing some standard network
>         measures."""
> 
>         self._node_weights = None
>         """(array([double>=0])) array of node weights"""
>         self.mean_node_weight = 0
203c257
<         self.total_node_weight: float = 0
---
>         self.total_node_weight = 0
205a260,262
>         self.cache = {'base': {}, 'nsi': {}, 'paths': {}}
>         """(dict) cache of re-usable computation results"""
> 
207c264
<             self.adjacency = adjacency
---
>             self._set_adjacency(adjacency)
211,212c268,269
<             raise NetworkError("An adjacency matrix or edge list has to be "
<                                "given to initialize an instance of Network.")
---
>             raise NetworkError("An adjacency matrix or edge list has to be \
>                                given to initialize an instance of Network.")
214c271
<         self.node_weights = node_weights
---
>         self._set_node_weights(node_weights)
217,219d273
<     def __cache_state__(self) -> Tuple[Hashable, ...]:
<         return (self.directed, self._mut_A,)
< 
233,234c287,288
<                 f"{self.N} nodes, {self.n_links} links, "
<                 f"link density {self.link_density:.3f}.")
---
>         f"{self.N} nodes, {self.n_links} links, "
>         f"link density {self.link_density:.3f}.")
248a303,325
>     def clear_cache(self):
>         """
>         Clear cache of information that can be recalculated from basic data.
>         """
>         self.cache['base'] = {}
>         self.clear_nsi_cache()
>         self.clear_paths_cache()
> 
>     def clear_nsi_cache(self):
>         """
>         Clear cache of information that can be recalculated from basic data
>         and depends on the node weights.
>         """
>         self.cache['nsi'] = {}
> 
>     def clear_paths_cache(self):
>         """
>         Clear cache of path legths for link attributes.
>         """
>         for attr in self.cache['paths']:
>             self.clear_link_attribute(attr)
>         self.cache['paths'] = {}
> 
377c454
<         return self.sp_A.toarray()
---
>         return self.sp_A.A
379,380c456
<     @adjacency.setter
<     def adjacency(self, adjacency):
---
>     def _set_adjacency(self, adjacency):
421a498
>         Network.clear_cache(self)
423,424c500,502
<         # invalidate cache
<         self._mut_A += 1
---
>     @adjacency.setter
>     def adjacency(self, adjacency):
>         self._set_adjacency(adjacency)
460c538
<         """array of node weights"""
---
>         """(array([double>=0])) array of node weights"""
463,464c541
<     @node_weights.setter
<     def node_weights(self, weights: Optional[np.ndarray]):
---
>     def _set_node_weights(self, weights):
479a557
>         self.clear_nsi_cache()
492,493c570,572
<         # invalidate cache
<         self._mut_nw += 1
---
>     @node_weights.setter
>     def node_weights(self, node_weights):
>         self._set_node_weights(node_weights)
518d596
<     # pylint: disable=keyword-arg-before-vararg
559d636
<     # pylint: disable=keyword-arg-before-vararg
645,646c722,723
<         #  invalidate cache
<         net._mut_la += 1
---
>         net.clear_paths_cache()
> 
784,785c861
<             raise ValueError("`ErdosRenyi()` requires either a "
<                              "`link_probability` or `n_links` argument.")
---
>             return None
786a863
>         # return adjacency matrix
790c867
<     def BarabasiAlbert_igraph(n_nodes=100, n_links_each=5):
---
>     def BarabasiAlbert_igraph(n_nodes=100, n_links_each=5, silence_level=0):
813a891,893
>         :type silence_level: int >= 0
>         :arg  silence_level: The higher, the less progress info is output.
> 
822a903
>         # return adjacency matrix
827c908
<     def BarabasiAlbert(n_nodes=100, n_links_each=5):
---
>     def BarabasiAlbert(n_nodes=100, n_links_each=5, silence_level=0):
832,837c913,914
<         :type n_nodes: int > 0
<         :arg  n_nodes: Number of nodes. (Default: 100)
< 
<         :type n_links_each: int > 0
<         :arg  n_links_each: Number of links to existing nodes each new node
<                             gets during construction. (Default: 5)
---
>         :type silence_level: int >= 0
>         :arg  silence_level: The higher, the less progress info is output.
863a941
>         # Return adjacency matrix
867c945
<     def Configuration(degree):
---
>     def Configuration(degrees, silence_level=0):
878,879c956,957
<         :type degree: 1d numpy array or list [node]
<         :arg  degree: Array or list of degree wanted.
---
>         :type degrees: 1d numpy array or list [node]
>         :arg  degrees: Array or list of degrees wanted.
890c968
<         graph = igraph.Graph.Degree_Sequence(out=list(degree))
---
>         graph = igraph.Graph.Degree_Sequence(out=list(degrees))
895a974
>         #  Return adjacency matrix
928a1008
>         #  Return adjacency matrix
931a1012,1300
>     def GrowPreferentially_old(n_nodes=100, m=2, silence_level=0):
>         """
>         EXPERIMENTAL: Return a random network grown with preferential weight
>         increase and preferential attachment.
> 
>         Return a random network grown as follows: Starting with a clique
>         of m+1 unit weight nodes, iteratively add a unit weight node and then
>         m times increase the weight of an existing node by one unit, for
>         n=m+2...N. Choose the growing node with probabilities proportional to
>         the node's weight. After each node addition or weight increase, add one
>         link from the respective node to another node, chosen with probability
>         proportional to that node's n.s.i. degree.
> 
>         :type silence_level: int >= 0
>         :arg  silence_level: The higher, the less progress info is output.
>         """
>         N = n_nodes
>         w, A = np.zeros(N, int), sp.lil_matrix((N, N))
>         nbs = [[] for i in range(N)]
> 
>         # start with m+1 fully connected nodes
>         w[:m+1] = 1
> 
>         # total weight now and in the end
>         last_W = m+1
>         # this is also approx. the total no. of links in the end!
>         W = (m+1) * (N-m)
>         inc_target = np.zeros(W, "int")  # inverse cum. w distribution
>         inc_target[:m+1] = range(m+1)
> 
>         # max. possible w before step n: 1 + m(n-m-2),
>         # so the addition in step n increases total n.s.i. degree by at most
>         # 3 + m(n-m-2) <= nm,
>         # each of the m weight increases increases it by at most n, totalling
>         # mn,
>         # and each of the m additional links increases it by at most
>         # 2 * (1 + m(n-m-2) + m), totalling <= 2nm, all totalling <= 2nm^2
>         # total n.s.i. degree now and max. in the end:
>         # last_Kstar = (m+1)**2
>         last_Kstar = (m+1)*m
>         max_Kstar = N**2 * (m+1)**2
>         # inverse cum. k* distribution
>         link_target = np.zeros(max_Kstar, "int")
> 
>         for i in range(m+1):
>             for j in range(i):
>                 A[i, j] = A[j, i] = 1
>             nbs[i] = range(m+1)
>             nbs[i].remove(i)
>             link_target[(m+1)*i:(m+1)*(i+1)] = i
> 
>         for n in range(m+2, N+1):
>             # add node n-1 with unit weight:
>             w[n-1] = 1
>             inc_target[last_W] = n-1
>             last_W += 1
>             # link it to some i:
>             i = int(link_target[int(random.uniform(last_Kstar))])
>             print("n", n, "i", i)
>             A[i, n-1] = A[n-1, i] = 1
>             nbs[n-1] = [i]
>             nbs[i].append(n-1)
>             link_target[last_Kstar] = i
>             # link_target[last_Kstar+1:last_Kstar+2+w[i]] = n-1
>             # last_Kstar += 2+w[i]
>             link_target[last_Kstar+1] = n-1
>             last_Kstar += 2
> 
>             for jj in range(m):
>                 # increase weight of some j not already linked to all:
>                 j = int(inc_target[int(random.uniform(last_W))])
>                 while len(nbs[j]) == n-1:
>                     print(" not j", j)
>                     j = int(inc_target[int(random.uniform(last_W))])
>                 w[j] += 1
>                 print(" jj", jj, "j", j, "w[j]", w[j])
>                 inc_target[last_W] = j
>                 last_W += 1
>                 # link_target[last_Kstar] = j
>                 # last_Kstar += 1
>                 # for i in nbs[j]:
>                 #     print("  i", i)
>                 #     link_target[last_Kstar] = i
>                 #     last_Kstar += 1
> 
>                 # link it to some i not already linked to it:
>                 i = int(link_target[int(random.uniform(last_Kstar))])
>                 while i == j or A[i, j] == 1:
>                     # print("  not i",i)
>                     i = int(link_target[int(random.uniform(last_Kstar))])
>                 A[i, j] = A[j, i] = 1
>                 nbs[j].append(i)
>                 nbs[i].append(j)
>                 # print("  i",i,"nbs[i]",nbs[i],"nbs[j]",nbs[j])
>                 # link_target[last_Kstar:last_Kstar+w[j]] = i
>                 # last_Kstar += w[j]
>                 # link_target[last_Kstar:last_Kstar+w[i]] = j
>                 # last_Kstar += w[i]
>                 link_target[last_Kstar] = i
>                 link_target[last_Kstar+1] = j
>                 last_Kstar += 2
> 
>         del nbs, link_target, inc_target
>         return Network(A, node_weights=w, silence_level=silence_level)
> 
>     @staticmethod
>     def GrowPreferentially(n_nodes=100, n_growths=1, n_links_new=1,
>                            n_links_old=1, nsi=True, preferential_exponent=1,
>                            n_initials=1, silence_level=0):
>         """
>         EXPERIMENTAL: Return a random network grown with preferential weight
>         increase and n.s.i. preferential attachment.
> 
>         Return a random network grown as follows: Starting with a clique
>         of 2*n_links_new+1 unit weight nodes, iteratively add a unit weight
>         node, connect it with n_links_new different existing nodes chosen
>         with probabilities proportional to their current n.s.i. degree, then
>         increase the weights of n_growths nodes chosen with probabilities
>         proportional to their current weight (with replacement), then add
>         n_links_old new links between pairs of nodes chosen with probabilities
>         proportional to their current weight.
> 
>         :type silence_level: int >= 0
>         :arg  silence_level: The higher, the less progress info is output.
>         """
>         N = n_nodes
>         w, A = np.zeros(N, dtype=int), sp.lil_matrix((N, N))
>         nbs = [[] for i in range(N)]
>         inc_target = list(range(n_initials))
> 
>         if nsi:
>             kstar = np.zeros(N)
>             link_prob = np.zeros(N)  # w * kstar
> 
>             w[:n_initials] = 1
>             link_prob[:n_initials] = 1
>             total_link_prob = link_prob.sum()
> 
>             def _link_target():
>                 thd = random.uniform(low=0, high=total_link_prob)
>                 i = 0
>                 cum = link_prob[0]
>                 while cum < thd:
>                     i += 1
>                     cum += link_prob[i]
>                 return i
> 
>             progress = progressbar.ProgressBar(maxval=N).start()
>             for j in range(n_initials, N):
>                 # add node j with unit weight:
>                 link_prob[j] = kstar[j] = w[j] = 1
>                 total_link_prob += 1
>                 inc_target.append(j)
>                 # link it to some i's:
>                 for _ in range(n_links_new):
>                     i = _link_target()
>                     # print(j,i)
>                     while i == j:
>                         # print("not i",i)
>                         i = _link_target()
>                     if A[i, j]:
>                         continue
>                     # print("j", j, "i", i)
>                     A[i, j] = A[j, i] = 1
>                     nbs[i].append(j)
>                     nbs[j] = [i]
>                     total_link_prob -= link_prob[i] + link_prob[j]
>                     kstar[i] += w[j]
>                     kstar[j] += w[i]
>                     link_prob[i] = w[i] * kstar[i]**preferential_exponent
>                     link_prob[j] = w[j] * kstar[j]**preferential_exponent
>                     total_link_prob += link_prob[i] + link_prob[j]
>                 # print(total_link_prob, link_prob.sum())
> 
>                 for _ in range(n_growths):
>                     # increase weight of some i:
>                     i = inc_target[int(
>                         random.uniform(low=0, high=len(inc_target)))]
>                     # print(i,inc_target)
>                     total_link_prob -= link_prob[nbs[i]].sum() + link_prob[i]
>                     w[i] += 1
>                     inc_target.append(i)
>                     kstar[i] += 1
>                     kstar[nbs[i]] += 1
>                     link_prob[i] = w[i] * kstar[i]**preferential_exponent
>                     link_prob[nbs[i]] = \
>                         w[nbs[i]] * kstar[nbs[i]]**preferential_exponent
>                     total_link_prob += link_prob[nbs[i]].sum() + link_prob[i]
>                     # print(" ii",ii,"i",i,"w[i]",w[i])
>                 # print(total_link_prob, link_prob.sum())
>                 for ii in range(n_links_old):
>                     # j2 = _link_target()
>                     j2 = inc_target[int(
>                         random.uniform(low=0, high=len(inc_target)))]
>                     i = _link_target()
>                     while i == j2:
>                         i = _link_target()
>                     if A[i, j2]:
>                         continue
>                     A[i, j2] = A[j2, i] = 1
>                     nbs[i].append(j2)
>                     nbs[j2].append(i)
>                     total_link_prob -= link_prob[i] + link_prob[j2]
>                     kstar[i] += w[j2]
>                     kstar[j2] += w[i]
>                     link_prob[i] = w[i] * kstar[i]**preferential_exponent
>                     link_prob[j2] = w[j2] * kstar[j2]**preferential_exponent
>                     total_link_prob += link_prob[i] + link_prob[j2]
>                 # print(total_link_prob, link_prob.sum())
> 
>                 if j % 10:
>                     progress.update(j)
> 
>             progress.finish()
> 
>         else:
>             link_target = []
> 
>             # start with (2*n_links_new+1) fully connected nodes:
>             n_initials = n_links_new+2*n_links_old+2
>             # max(n_links_new+2*n_links_old+2, n_growths-1)
>             w[:n_initials] = 1
> 
>             for i in range(n_initials):
>                 for j in range(i):
>                     if min(i-j, j+n_initials-i) <= \
>                             np.ceil((n_links_new + n_links_old)/2.0):
>                         A[i, j] = A[j, i] = 1
>                         nbs[i].append(j)
>                         nbs[j].append(i)
>                 link_target += [i for _ in range(n_links_new + n_links_old)]
> 
>             # last_grown = np.zeros(N)
>             for j in range(n_initials, N):
>                 # add node j with unit weight:
>                 w[j] = 1
>                 inc_target.append(j)
>                 link_target.append(j)
>                 # link it to some i's:
>                 for ii in range(n_links_new):
>                     i = int(link_target[int(
>                         random.uniform(low=0, high=len(link_target)))])
>                     while i == j or A[i, j] == 1:
>                         # print("not i",i)
>                         i = int(link_target[int(
>                             random.uniform(low=0, high=len(link_target)))])
>                     # print("j", j, "i", i)
>                     A[i, j] = A[j, i] = 1
>                     nbs[j] = [i]
>                     nbs[i].append(j)
>                     link_target += [j for iii in range(w[i])] + [i]
>                 for ii in range(n_growths):
>                     # increase weight of some i:
>                     i = int(inc_target[int(
>                         random.uniform(low=0, high=len(inc_target)))])
>                     # while last_grown[i] == j:
>                     #    i = int(inc_target[int(
>                     #        random.uniform(len(inc_target)))])
>                     # last_grown[i] = j
>                     w[i] += 1
>                     # print(" ii",ii,"i",i,"w[i]",w[i])
>                     inc_target.append(i)
>                     link_target += nbs[i] + [i]
>                 for ii in range(n_links_old):
>                     # j2 = int(inc_target[int(
>                     #      random.uniform(len(inc_target)))])
>                     j2 = int(link_target[int(
>                         random.uniform(low=0, high=len(link_target)))])
>                     # i = int(inc_target[int(
>                     #     random.uniform(len(inc_target)))])
>                     i = int(link_target[int(
>                         random.uniform(low=0, high=len(link_target)))])
>                     while i == j2 or A[i, j2] == 1:
>                         # i = int(inc_target[int(
>                         #     random.uniform(len(inc_target)))])
>                         i = int(link_target[int(
>                             random.uniform(low=0, high=len(link_target)))])
>                     A[i, j2] = A[j2, i] = 1
>                     nbs[j2].append(i)
>                     nbs[i].append(j2)
>                     link_target += [j2 for iii in range(w[i])] + \
>                         [i for iii in range(w[j2])]
> 
>             del link_target
> 
>         del nbs, inc_target
>         return Network(A, node_weights=w, silence_level=silence_level)
> 
>     @staticmethod
957a1327
>         progress = progressbar.ProgressBar(maxval=N).start()
959,962c1329,1339
<         with tqdm(total=N) as pbar:
<             while this_N < N and it < n_increases:
<                 it += 1
<                 i = _inc_target()
---
>         while this_N < N and it < n_increases:
>             it += 1
>             i = _inc_target()
>             total_inc_prob -= inc_prob[i]
>             w[i] += 1
>             inc_prob[i] = w[i]**exponent
>             total_inc_prob += inc_prob[i]
>             if (mode == "exp" and random.uniform() > hold_prob**w[i]) or \
>                     (mode == "rec" and random.uniform()
>                      < w[i]*1.0/(split_weight+w[i])):  # reciprocal
>                 # split i into i,this_N:
964c1341,1343
<                 w[i] += 1
---
>                 w[this_N] = w[i]*random.beta(beta, beta)
>                 w[i] -= w[this_N]
>                 inc_prob[this_N] = w[this_N]**exponent
966,978c1345,1350
<                 total_inc_prob += inc_prob[i]
<                 if (mode == "exp" and random.uniform() > hold_prob**w[i]) or \
<                         (mode == "rec" and random.uniform()
<                          < w[i]*1.0/(split_weight+w[i])):  # reciprocal
<                     # split i into i,this_N:
<                     total_inc_prob -= inc_prob[i]
<                     w[this_N] = w[i]*random.beta(beta, beta)
<                     w[i] -= w[this_N]
<                     inc_prob[this_N] = w[this_N]**exponent
<                     inc_prob[i] = w[i]**exponent
<                     total_inc_prob += inc_prob[this_N] + inc_prob[i]
<                     this_N += 1
<                     pbar.update()
---
>                 total_inc_prob += inc_prob[this_N] + inc_prob[i]
>                 this_N += 1
>             if this_N % 10:
>                 progress.update(this_N)
> 
>         progress.finish()
1021a1394
>     # TODO: deprecate this and rather use undirected_copy()
1031c1404
<         >>> print(net.undirected_adjacency().toarray())
---
>         >>> print(net.undirected_adjacency().A)
1067c1440,1441
<                     raise ValueError('direction must be "in" or "out".')
---
>                     print("ERROR: argument direction of Network.laplacian "
>                           "can only take values <<in>> or <<out>>.")
1093,1097c1467
<         if self.directed:
<             raise NotImplementedError("Not implemented for directed networks.")
<         return (
<             self.sp_nsi_diag_k() - self.sp_Aplus() * self.sp_diag_w()
<             ).toarray()
---
>         return (self.sp_nsi_diag_k() - self.sp_Aplus() * self.sp_diag_w()).A
1184c1554
<     def set_node_attribute(self, attribute_name: str, values):
---
>     def set_node_attribute(self, attribute_name, values):
1194a1565,1568
>         # TODO: add example
> 
>         #  Test whether the data vector has the same length as the number of
>         #  nodes in the graph.
1195a1570
>             #  Add node property to igraph Graph object
1203c1578
<     def node_attribute(self, attribute_name: str):
---
>     def node_attribute(self, attribute_name):
1213a1589
>         # TODO: add example
1216c1592
<     def del_node_attribute(self, attribute_name: str):
---
>     def del_node_attribute(self, attribute_name):
1222,1223c1598,1599
<         if attribute_name in self.graph.vs.attributes():
<             del self.graph.vs[attribute_name]
---
>         # TODO: add example
>         self.graph.es.__delattr__(attribute_name)
1229,1230c1605
<     def find_link_attribute(self, attribute_name: str):
<         return attribute_name in self.graph.es.attributes()
---
>     # TODO: verify whether return types are list or numpy array
1232c1607
<     def average_link_attribute(self, attribute_name: str):
---
>     def average_link_attribute(self, attribute_name):
1240a1616
>         # TODO: add example
1243c1619
<     def link_attribute(self, attribute_name: str):
---
>     def link_attribute(self, attribute_name):
1251a1628,1629
>         # TODO: add example
>         # TODO: test this for directed graphs
1262a1641
> 
1265c1644,1653
<     def del_link_attribute(self, attribute_name: str):
---
>     def clear_link_attribute(self, attribute_name):
>         """
>         Clear cache of a link attribute.
> 
>         :arg str attribute_name: name of link attribute
>         """
>         if attribute_name in self.cache['paths']:
>             del self.cache['paths'][attribute_name]
> 
>     def del_link_attribute(self, attribute_name):
1271,1274c1659,1664
<         if self.find_link_attribute(attribute_name):
<             del self.graph.es[attribute_name]
<             # invalidate cache
<             self._mut_la += 1
---
>         # TODO: add example
>         if attribute_name in self.cache['paths']:
>             self.clear_link_attribute(attribute_name)
>             self.graph.es.__delattr__(attribute_name)
>         else:
>             print("WARNING: Link attribute", attribute_name, "not found!")
1290a1681,1683
>         # TODO: add example and sparse version
>         # TODO: test this for directed graphs
>         #  Set link attribute in igraph
1293,1294c1686,1688
<         # invalidate cache
<         self._mut_la += 1
---
> 
>         #  Set Network specific attributes
>         self.clear_link_attribute(attribute_name)
1300c1694,1695
<     @Cached.method()
---
>     # @cached_const('base', 'degree')
>     @cached_var('degree')
1305c1700
<         If a link attribute key is specified, return the associated strength.
---
>         If a link attribute key is specified, return the associated strength
1320c1715,1716
<     @Cached.method(attrs=("_mut_la",))
---
>     # TODO: use directed example here and elsewhere
>     @cached_var('indegree')
1325,1326c1721
<         If a link attribute key is specified, return the associated
<         in-strength.
---
>         If a link attribute key is specified, return the associated in strength
1337c1732
<             return self.sp_A.toarray().sum(axis=0).astype(int)
---
>             return self.sp_A.sum(axis=0).A.squeeze().astype(int)
1341c1736
<     @Cached.method(attrs=("_mut_la",))
---
>     @cached_var('outdegree')
1346,1347c1741,1742
<         If a link attribute key is specified, return the associated
<         out-strength.
---
>         If a link attribute key is specified, return the associated out
>         strength
1358c1753
<             return self.sp_A.toarray().sum(axis=1).T.astype(int)
---
>             return self.sp_A.sum(axis=1).T.A.squeeze().astype(int)
1362c1757
<     @Cached.method(attrs=("_mut_la",))
---
>     @cached_var('bildegree')
1369c1764
<         strength.
---
>         strength
1384a1780,1799
>     @cached_var('nsi_degree', 'n.s.i. degree')
>     def nsi_degree_uncorr(self, key=None):
>         """
>         For each node, return its uncorrected n.s.i. degree.
> 
>         If a link attribute key is specified, return the associated nsi
>         strength
> 
>         :arg str key: link attribute key [optional]
>         :rtype: array([float])
>         """
>         if self.directed:
>             return self.nsi_indegree(key) + self.nsi_outdegree(key)
>         else:
>             if key is None:
>                 return self.sp_Aplus() * self.node_weights
>             else:
>                 w = self.link_attribute(key)
>                 return (self.node_weights @ w).squeeze()
> 
1387c1802
<         return sp.diags([self.nsi_degree()], [0],
---
>         return sp.diags([self.nsi_degree_uncorr()], [0],
1392c1807
<         return sp.diags([np.power(self.nsi_degree(), -1)], [0],
---
>         return sp.diags([np.power(self.nsi_degree_uncorr(), -1)], [0],
1395,1396c1810
<     @Cached.method(attrs=("_mut_nw", "_mut_la"))
<     def nsi_degree(self, key=None, typical_weight=None):
---
>     def nsi_degree(self, typical_weight=None, key=None):
1400,1401c1814,1816
<         If a link attribute key is specified, return the associated n.s.i.
<         strength.
---
>         If a link attribute key is specified, return the associated nsi
>         strength
> 
1426d1840
<         :arg str key: link attribute key (optional)
1428,1431c1842,1845
<         :arg float typical_weight: Optional typical node weight to be used for
<             correction. If None, the uncorrected measure is
<             returned. (Default: None)
< 
---
>         :arg  typical_weight: Optional typical node weight to be used for
>                               correction. If None, the uncorrected measure is
>                               returned. (Default: None)
>         :arg str key: link attribute key (optional)
1434,1442d1847
<         if self.directed:
<             res = self.nsi_indegree(key) + self.nsi_outdegree(key)
<         else:
<             if key is None:
<                 res = self.sp_Aplus() * self.node_weights
<             else:
<                 w = self.link_attribute(key)
<                 res = (self.node_weights @ w).squeeze()
< 
1444c1849
<             return res
---
>             return self.nsi_degree_uncorr(key)
1446c1851
<             return res/typical_weight - 1.0
---
>             return self.nsi_degree_uncorr(key)/typical_weight - 1.0
1448,1449c1853,1854
<     @Cached.method(attrs=("_mut_nw", "_mut_la"))
<     def nsi_indegree(self, key=None, typical_weight=None):
---
>     @cached_var('nsi_indegree')
>     def nsi_indegree(self, key=None):
1451c1856
<         For each node, return its n.s.i. indegree.
---
>         For each node, return its n.s.i. indegree
1453,1454c1858,1859
<         If a link attribute key is specified, return the associated n.s.i.
<         in-strength.
---
>         If a link attribute key is specified, return the associated nsi in
>         strength
1473,1478d1877
<         :type typical_weight: float > 0
<         :arg  typical_weight: Optional typical node weight to be used for
<             correction. If None, the uncorrected measure is
<             returned. (Default: None)
< 
<         :rtype: array([float])
1481c1880
<             res = self.node_weights * self.sp_Aplus()
---
>             return self.node_weights * self.sp_Aplus()
1483,1487c1882,1883
<             res = (self.node_weights @ self.link_attribute(key)).squeeze()
<         if typical_weight is None:
<             return res
<         else:
<             return res/typical_weight - 1.0
---
>             w = self.link_attribute(key)
>             return (self.node_weights @ w).squeeze()
1489,1490c1885,1886
<     @Cached.method(attrs=("_mut_nw", "_mut_la"))
<     def nsi_outdegree(self, key=None, typical_weight=None):
---
>     @cached_var('nsi_outdegree')
>     def nsi_outdegree(self, key=None):
1492c1888
<         For each node, return its n.s.i. outdegree.
---
>         For each node, return its n.s.i.outdegree
1494,1495c1890,1891
<         If a link attribute key is specified, return the associated n.s.i.
<         out-strength.
---
>         If a link attribute key is specified, return the associated nsi out
>         strength
1514,1519d1909
<         :type typical_weight: float > 0
<         :arg  typical_weight: Optional typical node weight to be used for
<             correction. If None, the uncorrected measure is
<             returned. (Default: None)
< 
<         :rtype: array([float])
1522,1550c1912
<             res = self.sp_Aplus() * self.node_weights
<         else:
<             res = (self.link_attribute(key) @ self.node_weights).squeeze()
<         if typical_weight is None:
<             return res
<         else:
<             return res/typical_weight - 1.0
< 
<     @Cached.method(attrs=("_mut_nw",))
<     def nsi_bildegree(self, key=None, typical_weight=None):
<         """
<         For each node, return its n.s.i. bilateral degree.
< 
<         If a link attribute key is specified, return the associated n.s.i.
<         bilateral strength.
< 
<         :arg str key: link attribute key [optional]
<         :type typical_weight: float > 0
<         :arg  typical_weight: Optional typical node weight to be used for
<             correction. If None, the uncorrected measure is
<             returned. (Default: None)
< 
<         :rtype: array([float])
<         """
<         assert key is None, "nsi_bildegree is not implemented with key yet"
<         Ap = self.sp_Aplus()
<         res = (Ap * sp.diags(self.node_weights) * Ap).diagonal()
<         if typical_weight is None:
<             return res
---
>             return self.sp_Aplus() * self.node_weights
1552c1914,1915
<             return res/typical_weight - 1.0
---
>             w = self.link_attribute(key)
>             return (w @ self.node_weights.transpose()).transpose().squeeze()
1554c1917
<     @Cached.method(name="the degree frequency distribution")
---
>     @cached_const('base', 'degree df', 'the degree frequency distribution')
1571c1934
<     @Cached.method(name="the in-degree frequency distribution")
---
>     @cached_const('base', 'indegree df', 'in-degree frequency distribution')
1588c1951
<     @Cached.method(name="the out-degree frequency distribution")
---
>     @cached_const('base', 'outdegree df', 'out-degree frequency distribution')
1605c1968
<     @Cached.method(name="the cumulative degree distribution")
---
>     @cached_const('base', 'degree cdf', 'the cumulative degree distribution')
1622c1985,1986
<     @Cached.method(name="the cumulative in-degree distribution")
---
>     @cached_const('base', 'indegree cdf',
>                   'the cumulative in-degree distribution')
1639c2003,2004
<     @Cached.method(name="the cumulative out-degree distribution")
---
>     @cached_const('base', 'outdegree cdf',
>                   'the cumulative out-degree distribution')
1656,1658c2021,2023
<     @Cached.method(name="a n.s.i. degree frequency histogram",
<                    attrs=("_mut_nw",))
<     def nsi_degree_histogram(self, typical_weight=None):
---
>     # FIXME: should rather return the weighted distribution!
>     @cached_const('nsi', 'degree hist', 'a n.s.i. degree frequency histogram')
>     def nsi_degree_histogram(self):
1670,1674d2034
<         :type typical_weight: float > 0
<         :arg  typical_weight: Optional typical node weight to be used for
<             correction. If None, the uncorrected measure is
<             returned. (Default: None)
< 
1678c2038
<         nsi_k = self.nsi_degree(typical_weight=typical_weight)
---
>         nsi_k = self.nsi_degree()
1682,1684c2042,2045
<     @Cached.method(name="a cumulative n.s.i. degree frequency histogram",
<                    attrs=("_mut_nw",))
<     def nsi_degree_cumulative_histogram(self, typical_weight=None):
---
>     # FIXME: should rather return the weighted distribution!
>     @cached_const('nsi', 'degree hist',
>                   'a cumulative n.s.i. degree frequency histogram')
>     def nsi_degree_cumulative_histogram(self):
1695,1699d2055
<         :type typical_weight: float > 0
<         :arg  typical_weight: Optional typical node weight to be used for
<             correction. If None, the uncorrected measure is
<             returned. (Default: None)
< 
1703c2059
<         nsi_k = self.nsi_degree(typical_weight=typical_weight)
---
>         nsi_k = self.nsi_degree()
1707c2063
<     @Cached.method(name="average neighbours' degrees")
---
>     @cached_const('base', 'avg nbr degree', "average neighbours' degrees")
1725c2081
<     @Cached.method(name="maximum neighbours' degrees")
---
>     @cached_const('base', 'max nbr degree', "maximum neighbours' degree")
1741c2097
<         return nbks.toarray().max(axis=1).T
---
>         return nbks.max(axis=1).T.A.squeeze()
1743,1744c2099
<     @Cached.method(name="n.s.i. average neighbours' degrees",
<                    attrs=("_mut_nw",))
---
>     @cached_const('nsi', 'avg nbr degree', "n.s.i. average neighbours' degree")
1781d2135
<         # TODO: enable correction by typical_weight
1783,1784c2137
<     @Cached.method(name="n.s.i. maximum neighbours' degrees",
<                    attrs=("_mut_nw",))
---
>     @cached_const('nsi', 'max nbr degree', "n.s.i. maximum neighbour degree")
1811,1813c2164
<         return (
<             self.sp_Aplus() * self.sp_nsi_diag_k()).toarray().max(axis=1).T
<         # TODO: enable correction by typical_weight
---
>         return (self.sp_Aplus() * self.sp_nsi_diag_k()).max(axis=1).T.A[0]
1819c2170
<     @Cached.method(name="local clustering coefficients")
---
>     @cached_const('base', 'local clustering', 'local clustering coefficients')
1841c2192,2193
<     @Cached.method(name="the global clustering coefficient (C_2)")
---
>     @cached_const('base', 'global clustering',
>                   'global clustering coefficient (C_2)')
1860,1861c2212
<     def _motif_clustering_helper(self, t_func, T, key=None, nsi=False,
<                                  typical_weight=None, ksum=None):
---
>     def _motif_clustering_helper(self, t_func, T, key=None, nsi=False):
1871,1875d2221
<         :type typical_weight: float > 0
<         :arg float typical_weight: Optional typical node weight to be used for
<             correction. If None, the uncorrected measure is
<             returned. (Default: None)
< 
1881d2226
<             # pylint: disable=possibly-used-before-assignment
1892,1900c2237,2239
<         if typical_weight is None:
<             C = t / (self.node_weights * T) if nsi else t / T
<             C[np.isnan(C)] = 0
<             return C
<         else:
<             bilk = self.nsi_bildegree(typical_weight=typical_weight)
<             numerator = t / self.node_weights
<             return ((numerator/typical_weight**2 - 3.0*bilk - 1.0)
<                     / (T - ksum/typical_weight - bilk + 2))
---
>         C = t / (self.node_weights * T) if nsi else t / T
>         C[np.isnan(C)] = 0
>         return C
1902c2241
<     @Cached.method(name="the local cycle motif clustering coefficients")
---
>     @cached_var('local cyclemotif', 'local cycle motif clustering coefficient')
1909c2248
<         weighted version.
---
>         weighted version
1920c2259
<         def t_func(x, xT):  # pylint: disable=unused-argument
---
>         def t_func(x, xT):
1925c2264
<     @Cached.method(name="the local mid. motif clustering coefficients")
---
>     @cached_var('local midmotif', 'local mid. motif clustering coefficient')
1932c2271
<         weighted version.
---
>         weighted version
1948c2287
<     @Cached.method(name="the local in motif clustering coefficients")
---
>     @cached_var('local inmotif', 'local in motif clustering coefficient')
1955c2294
<         weighted version.
---
>         weighted version
1971c2310
<     @Cached.method(name="the local out motif clustering coefficients")
---
>     @cached_var('local outmotif', 'local out motif clustering coefficient')
1978c2317
<         weighted version.
---
>         weighted version
1994,1996c2333,2335
<     @Cached.method(name="the local n.s.i. cycle motif clustering coefficients",
<                    attrs=("_mut_nw",))
<     def nsi_local_cyclemotif_clustering(self, key=None, typical_weight=None):
---
>     @cached_var('nsi local cyclemotif',
>                 'local nsi cycle motif clustering coefficient')
>     def nsi_local_cyclemotif_clustering(self, key=None):
2002c2341
<         weighted version.
---
>         weighted version
2027,2030d2365
<         :type typical_weight: float > 0
<         :arg float typical_weight: Optional typical node weight to be used for
<             correction. If None, the uncorrected measure is
<             returned. (Default: None)
2032c2367
<         def t_func(x, xT):  # pylint: disable=unused-argument
---
>         def t_func(x, xT):
2034,2044c2369,2374
<         ink = self.nsi_indegree(typical_weight=typical_weight)
<         outk = self.nsi_outdegree(typical_weight=typical_weight)
<         T = ink * outk
<         ksum = ink + outk
<         return self._motif_clustering_helper(
<             t_func, T, key=key, nsi=True,
<             typical_weight=typical_weight, ksum=ksum)
< 
<     @Cached.method(name="the local n.s.i. mid. motif clustering coefficients",
<                    attrs=("_mut_nw",))
<     def nsi_local_midmotif_clustering(self, key=None, typical_weight=None):
---
>         T = self.nsi_indegree() * self.nsi_outdegree()
>         return self._motif_clustering_helper(t_func, T, key=key, nsi=True)
> 
>     @cached_var('nsi local midemotif',
>                 'local nsi mid. motif clustering coefficient')
>     def nsi_local_midmotif_clustering(self, key=None):
2050c2380
<         weighted version.
---
>         weighted version
2075,2078d2404
<         :type typical_weight: float > 0
<         :arg float typical_weight: Optional typical node weight to be used for
<             correction. If None, the uncorrected measure is
<             returned. (Default: None)
2082,2092c2408,2413
<         ink = self.nsi_indegree(typical_weight=typical_weight)
<         outk = self.nsi_outdegree(typical_weight=typical_weight)
<         T = ink * outk
<         ksum = ink + outk
<         return self._motif_clustering_helper(
<             t_func, T, key=key, nsi=True,
<             typical_weight=typical_weight, ksum=ksum)
< 
<     @Cached.method(name="the local n.s.i. in motif clustering coefficients",
<                    attrs=("_mut_nw",))
<     def nsi_local_inmotif_clustering(self, key=None, typical_weight=None):
---
>         T = self.nsi_indegree() * self.nsi_outdegree()
>         return self._motif_clustering_helper(t_func, T, key=key, nsi=True)
> 
>     @cached_var('nsi local inemotif',
>                 'local nsi in motif clustering coefficient')
>     def nsi_local_inmotif_clustering(self, key=None):
2098c2419
<         weighted version.
---
>         weighted version
2124,2127d2444
<         :type typical_weight: float > 0
<         :arg float typical_weight: Optional typical node weight to be used for
<             correction. If None, the uncorrected measure is
<             returned. (Default: None)
2131,2140c2448,2453
<         ink = self.nsi_indegree(typical_weight=typical_weight)
<         T = ink**2
<         ksum = ink * 2
<         return self._motif_clustering_helper(
<             t_func, T, key=key, nsi=True,
<             typical_weight=typical_weight, ksum=ksum)
< 
<     @Cached.method(name="the local n.s.i. out motif clustering coefficients",
<                    attrs=("_mut_nw",))
<     def nsi_local_outmotif_clustering(self, key=None, typical_weight=None):
---
>         T = self.nsi_indegree()**2
>         return self._motif_clustering_helper(t_func, T, key=key, nsi=True)
> 
>     @cached_var('nsi local outemotif',
>                 'local nsi out motif clustering coefficient')
>     def nsi_local_outmotif_clustering(self, key=None):
2146c2459
<         weighted version.
---
>         weighted version
2171,2174d2483
<         :type typical_weight: float > 0
<         :arg float typical_weight: Optional typical node weight to be used for
<             correction. If None, the uncorrected measure is
<             returned. (Default: None)
2178,2183c2487,2488
<         outk = self.nsi_outdegree(typical_weight=typical_weight)
<         T = outk**2
<         ksum = outk * 2
<         return self._motif_clustering_helper(
<             t_func, T, key=key, nsi=True,
<             typical_weight=typical_weight, ksum=ksum)
---
>         T = self.nsi_outdegree()**2
>         return self._motif_clustering_helper(t_func, T, key=key, nsi=True)
2185c2490
<     @Cached.method(name="transitivity coefficient (C_1)")
---
>     @cached_const('base', 'transitivity', 'transitivity coefficient (C_1)')
2374c2679
<         return Ap.toarray() * commons.toarray() / np.maximum(kk, kk.T)
---
>         return Ap.A * commons.A / np.maximum(kk, kk.T)
2391c2696
<         :rtype: float
---
>         :rtype: float between 0 and 1
2393,2394c2698,2699
<         degree = self.graph.degree()
<         degree_sq = [deg**2 for deg in degree]
---
>         degrees = self.graph.degree()
>         degrees_sq = [deg**2 for deg in degrees]
2400,2402c2705,2707
<             num1 += degree[source] * degree[target]
<             num2 += degree[source] + degree[target]
<             den1 += degree_sq[source] + degree_sq[target]
---
>             num1 += degrees[source] * degrees[target]
>             num2 += degrees[source] + degrees[target]
>             den1 += degrees_sq[source] + degrees_sq[target]
2409c2714,2731
<     @Cached.method(name="n.s.i. local clustering", attrs=("_mut_nw",))
---
>     @cached_const('nsi', 'local clustering')
>     def nsi_local_clustering_uncorr(self):
>         """
>         For each node, return its uncorrected n.s.i. clustering coefficient
>         (between 0 and 1).
> 
>         (not yet implemented for directed networks)
> 
>         :rtype: array([float])
>         """
>         if self.directed:
>             raise NotImplementedError("Not implemented for directed networks.")
> 
>         w, k = self.node_weights, self.nsi_degree()
>         A_Dw = self.sp_A * self.sp_diag_w()
>         numerator = (A_Dw * self.sp_Aplus() * A_Dw.T).diagonal()
>         return (numerator + 2*k*w - w**2) / k**2
> 
2439,2440c2761,2762
<             correction. If None, the uncorrected measure is
<             returned. (Default: None)
---
>                               correction. If None, the uncorrected measure is
>                               returned. (Default: None)
2444,2448d2765
<         if self.directed:
<             raise NotImplementedError("Not implemented for directed networks.")
< 
<         k = self.nsi_degree(typical_weight=typical_weight)
< 
2450,2456c2767
<             if self.silence_level <= 1:
<                 print("Calculating uncorrected n.s.i. "
<                       "local clustering coefficients...")
<             w = self.node_weights
<             A_Dw = self.sp_A * self.sp_diag_w()
<             numerator = (A_Dw * self.sp_Aplus() * A_Dw.T).diagonal()
<             return (numerator + 2*k*w - w**2) / k**2
---
>             return self.nsi_local_clustering_uncorr()
2457a2769
>             k = self.nsi_degree(typical_weight=typical_weight)
2460a2773
> 
2466,2467c2779,2780
<     @Cached.method(name="the n.s.i. global topological clustering coefficient",
<                    attrs=("_mut_nw",))
---
>     @cached_const('nsi', 'global clustering',
>                   'n.s.i. global topological clustering coefficient')
2496c2809
<     @Cached.method(name="n.s.i. transitivity", attrs=("_mut_nw",))
---
>     @cached_const('nsi', 'transitivity', 'n.s.i. transitivity')
2516,2517c2829,2830
<     @Cached.method(name="the n.s.i. local Soffer clustering coefficients",
<                    attrs=("_mut_nw",))
---
>     @cached_const('nsi', 'soffer clustering',
>                   'n.s.i. local Soffer clustering coefficients')
2564c2877
<     @Cached.method(name="path lengths")
---
>     @cached_var('paths')
2657,2658c2970,2971
<     @Cached.method(name="the n.s.i. average shortest path length",
<                    attrs=("_mut_nw",))
---
>     @cached_const('nsi', 'avg path length',
>                   'n.s.i. average shortest path length')
2726c3039
<     @Cached.method(name="matching index matrix")
---
>     @cached_const('base', 'matching idx', 'matching index matrix')
2747c3060
<         commons = (self.sp_A * self.sp_A).astype(DFIELD).toarray()
---
>         commons = (self.sp_A * self.sp_A).astype(DFIELD).A
2751c3064
<     @Cached.method(name="link betweenness")
---
>     @cached_const('base', 'link btw', 'link betweenness')
2769c3082
<         :rtype:  square numpy array [node,node] of floats
---
>         :rtype:  square numpy array [node,node] of floats between 0 and 1
2808c3121
<         :rtype:  square numpy array [node,node] of floats
---
>         :rtype:  square numpy array [node,node] of floats between 0 and 1
2818c3131
<     @Cached.method(name="node betweenness")
---
>     @cached_const('base', 'btw', 'node betweenness')
2841a3155
>     # @cached_const('base', 'inter btw', 'interregional betweenness')
2873c3187
<         :rtype: 1d numpy array [node] of floats
---
>         :rtype: 1d numpy array [node] of floats between 0 and 1
2876c3190
<                                     nsi=False)
---
>                                     aw=0, silent=1)
2877a3192
>     # @cached_const('nsi', 'inter btw', 'n.s.i. interregional betweenness')
2901c3216
<         :rtype: 1d numpy array [node] of floats
---
>         :rtype: 1d numpy array [node] of floats between 0 and 1
2903c3218
<         return self.nsi_betweenness(sources=sources, targets=targets)
---
>         return self.nsi_betweenness(sources=sources, targets=targets, silent=1)
2905,2906c3220
<     def nsi_betweenness(self, sources=None, targets=None,
<                         nsi: bool = True, parallelize: bool = False):
---
>     def nsi_betweenness(self, **kwargs):
2908c3222
<         For each node, return its n.s.i. betweenness. [Newman2001]_
---
>         For each node, return its n.s.i. betweenness.
2933,2934c3247
<         :arg bool parallelize: Toggle multiprocessing
<         :rtype: 1d numpy array [node] of floats
---
>         :rtype: 1d numpy array [node] of floats between 0 and 1
2936,2939c3249,3265
<         # initialize node lists
<         is_source = np.zeros(self.N, dtype=MASK)
<         if sources is not None:
<             is_source[sources] = 1
---
>         if self.silence_level <= 1:
>             if "silent" not in kwargs:
>                 print("Calculating n.s.i. betweenness...")
> 
>         w = self.node_weights
>         if "aw" in kwargs:
>             if kwargs["aw"] == 0:
>                 w = np.ones_like(w)
> 
>         N, k = self.N, self.degree()
>         rN = range(0, N)
>         betweenness_times_w = np.zeros(N, dtype=DFIELD)
> 
>         # initialize node lists:
>         is_source = np.zeros(N, dtype=MASK)
>         if "sources" in kwargs and kwargs["sources"] is not None:
>             is_source[kwargs["sources"]] = 1
2941,2943c3267,3269
<             is_source[range(0, self.N)] = 1
<         if targets is not None:
<             targets = np.array(list(map(int, targets)))
---
>             is_source[rN] = 1
>         if "targets" in kwargs and kwargs["targets"] is not None:
>             targets = kwargs["targets"]
2945c3271
<             targets = np.arange(0, self.N)
---
>             targets = rN
2947,2962c3273,3277
<         # call cached worker method with hashable arguments
<         return self._nsi_betweenness(
<             tuple(is_source), tuple(targets), nsi, parallelize)
< 
<     @Cached.method(name="n.s.i. betweenness", attrs=("_mut_nw",))
<     def _nsi_betweenness(self, is_source: Tuple[MASK], targets: Tuple[NODE],
<                          nsi: bool, parallelize: bool):
<         # type cast inputs
<         assert all(isinstance(arg, tuple) for arg in [is_source, targets])
<         is_source = np.array(is_source, dtype=MASK)
<         targets = np.array(targets, dtype=NODE)
<         k = to_cy(self.outdegree(), DEGREE)
< 
<         # initialize node weights
<         w = to_cy(self.node_weights, DWEIGHT)
<         w = w if nsi else np.ones_like(w)
---
>         # node offsets for flat arrays:
>         # NOTE: We don't use k.cumsum() since that uses too much memory!
>         offsets = np.zeros(N, dtype=NODE)
>         for i in range(1, N):
>             offsets[i] = offsets[i-1] + k[i-1]
2964c3279
<         # sort links by node indices (contains each link twice!)
---
>         # sort links by node indices (contains each link twice!):
2967c3282
<         # neighbours of each node
---
>         # neighbours of each node:
2969c3284,3317
<         assert k.sum() == len(flat_neighbors) == 2 * self.n_links
---
>         E = len(flat_neighbors)
> 
>         # this main loop might be parallelized:
>         for j0 in targets:
>             j = int(j0)
> 
>             betweenness_to_j = to_cy(w, DFIELD)
>             excess_to_j = to_cy(w, DFIELD)
>             flat_predecessors = np.zeros(E, dtype=NODE)
>             _nsi_betweenness(
>                 N, to_cy(w, DWEIGHT), to_cy(k, DEGREE), j,
>                 betweenness_to_j, excess_to_j, offsets, flat_neighbors,
>                 is_source, flat_predecessors)
>             del flat_predecessors
>             betweenness_times_w += w[j] * (betweenness_to_j - excess_to_j)
> 
>         return betweenness_times_w / w
> 
>     def _eigenvector_centrality_slow(self, link_attribute=None):
>         """
>         For each node, return its (weighted) eigenvector centrality.
> 
>         This is the load on this node from the eigenvector corresponding to the
>         largest eigenvalue of the (weighted) adjacency matrix, normalized to a
>         maximum of 1.
> 
>         :arg str link_attribute: Optional name of the link attribute to be used
>             as the links' weight. If None, links have weight 1. (Default: None)
>         :rtype: 1d numpy array [node] of floats
>         """
>         if link_attribute == "topological":
>             print("WARNING: link_attribute='topological' is deprecated.\n"
>                   + "Use link_attribute=None instead.")
>             link_attribute = None
2971,2981c3319,3323
<         # call Cython implementation
<         worker = partial(_nsi_betweenness,
<                          self.N, w, k, flat_neighbors, is_source)
<         if parallelize:
<             # (naively) parallelize loop over nodes
<             n_workers = cpu_count()
<             batches = np.array_split(targets, n_workers)
<             with get_context("spawn").Pool() as pool:
<                 betw_w = np.sum(pool.map(worker, batches), axis=0)
<                 pool.close()
<                 pool.join()
---
>         if link_attribute is None:
>             if self.silence_level <= 1:
>                 print("Calculating topological eigenvector centrality...")
> 
>             return np.array(self.graph.eigenvector_centrality(weights=None))
2983,2984c3325,3326
<             betw_w = worker(targets)
<         return betw_w / w
---
>             if self.silence_level <= 1:
>                 print("Calculating weighted eigenvector centrality...")
2986c3328,3332
<     @Cached.method(name="eigenvector centrality")
---
>             return np.array(self.graph.eigenvector_centrality(
>                 weights=link_attribute))
> 
>     # faster version of the above:
>     @cached_const('base', 'ev centrality', 'eigenvector centrality')
3010c3356
<     @Cached.method(name="n.s.i. eigenvector centrality", attrs=("_mut_nw",))
---
>     @cached_const('nsi', 'ev centrality', 'n.s.i. eigenvector centrality')
3019,3021d3364
<         For a directed network, this uses the right eigenvectors. To get the
<         values for the left eigenvectors, apply this to the inverse network!
< 
3101c3444
<         # TODO: check and describe behaviour for unconnected networks
---
>         # TODO: check and describe behaviour for unconnected networks.
3140c3483
<     @Cached.method(name="n.s.i. closeness", attrs=("_mut_nw",))
---
>     @cached_const('nsi', 'closeness', 'n.s.i. closeness')
3177c3520
<     @Cached.method(name="n.s.i. harmonic closeness", attrs=("_mut_nw",))
---
>     @cached_const('nsi', 'harm closeness', 'n.s.i. harmonic closeness')
3205,3206c3548,3549
<     @Cached.method(name="n.s.i. exponential closeness centrality",
<                    attrs=("_mut_nw",))
---
>     @cached_const('nsi', 'exp closeness',
>                   'n.s.i. exponential closeness centrality')
3234c3577
<     @Cached.method(name="Arenas-type random walk betweenness")
---
>     @cached_const('base', 'arenas btw', 'Arenas-type random walk betweenness')
3330a3674,3759
>     # TODO: remove this slow version after regression test:
>     def _arenas_betweenness_slow(self):
>         print("WARNING: _arenas_betweenness_slow() is deprecated!")
> 
>         t0 = time.time()
> 
>         #  Initialize the array to hold random walk betweenness
>         awRandomWalkBetweenness = np.zeros(self.N)
> 
>         #  Random walk betweenness has to be calculated for each component
>         #  separately. Therefore get different components of the graph first
>         components = self.graph.connected_components()
> 
>         #  Print giant component size
>         if self.silence_level <= 1:
>             print("   (giant component size: "
>                   + str(components.giant().vcount()) + " ("
>                   + str(components.giant().vcount()
>                         / float(self.graph.vcount())) + "))")
> 
>         for i, comp in enumerate(components):
>             #  If the component has size 1, set random walk betweenness to zero
>             if len(comp) == 1:
>                 awRandomWalkBetweenness[comp[0]] = 0
>             #  For larger components, continue with the calculation
>             else:
>                 #  Get the subgraph corresponding to component i
>                 subgraph = components.subgraph(i)
> 
>                 #  Get the subgraph adjacency matrix
>                 adjacency = np.array(subgraph.get_adjacency(type=2).data)
> 
>                 #  Get the list of vertex numbers in the subgraph
>                 vertexList = comp
> 
>                 # Extract corresponding area weight vector:
>                 aw = np.zeros(len(vertexList))
>                 for j, vs in enumerate(vertexList):
>                     aw[j] = self.node_weights[vs]
> 
>                 #  Generate a Network object representing the subgraph
>                 subnetwork = Network(adjacency, directed=False)
> 
>                 #  Get the number of nodes of the subgraph (the component size)
>                 nNodes = subnetwork.N
> 
>                 #  Initialize the RWB array
>                 rwb = np.zeros(nNodes)
> 
>                 #  Get the subnetworks degree sequence
>                 awDegreeSequence = subnetwork.nsi_degree()
> 
>                 #  Clean up
>                 del subgraph, subnetwork
> 
>                 #  Get the pMatrix that is modified and inverted
>                 Identity = np.identity(nNodes)
>                 Ap = adjacency + Identity
>                 pMatrix = np.diag(1/awDegreeSequence).dot(Ap).dot(np.diag(aw))
> 
>                 for k in range(nNodes):
>                     #  For k and each neighbour of it, set the corresponding
>                     #  row of the pMatrix to zero to account for the absorption
>                     #  of random walkers at their destination
>                     mask = 1-Ap[k, :]
>                     pMk = pMatrix*(mask.reshape((nNodes, 1)))
> 
>                     #  Calculate the b^k matrix
>                     bMatrix = np.dot(np.linalg.inv(Identity-pMk), pMk)
> 
>                     #  Perform the summation over source node i
>                     rwb += aw[k] * np.dot(aw.reshape((1, self.N)),
>                                           bMatrix).flatten() * mask
> 
>                 rwb /= aw
> 
>                 #  Copy results into randomWalkBetweennessArray at the correct
>                 #  positions
>                 for j, vs in enumerate(vertexList):
>                     awRandomWalkBetweenness[vs] = rwb[j]
> 
>         if self.silence_level <= 1:
>             print("...took", time.time()-t0, "seconds")
> 
>         return awRandomWalkBetweenness
> 
3358,3360c3787
<                 V = splu(
<                     sp.identity(N, format='csc') - sp_Pi
<                 ).solve(sp_Pi.toarray())
---
>                 V = splu(sp.identity(N, format='csc') - sp_Pi).solve(sp_Pi.A)
3565c3992
<     @Cached.method(name="Newman's random walk betweenness")
---
>     @cached_const('base', 'newman btw', "Newman's random walk betweenness")
3624c4051
<                 V = V.toarray()
---
>                 V = V.A
3799c4226
<                 V = ((DkI * Ap) * sp_M_inv).T.astype(DFIELD).toarray()
---
>                 V = ((DkI * Ap) * sp_M_inv).T.astype(DFIELD).A
3901c4328
<     @Cached.method(name="n.s.i. global efficiency", attrs=("_mut_nw",))
---
>     @cached_const('nsi', 'global eff', 'n.s.i. global efficiency')
3914a4342
>         # TODO: check results of examples!
4018c4446,4455
<         for i in trange(self.N, disable=self.silence_level > 1):
---
>         #  Initialize progress bar
>         if self.silence_level <= 1:
>             progress = progressbar.ProgressBar(maxval=self.N).start()
> 
>         for i in range(self.N):
>             # Update progress bar every 10 steps
>             if self.silence_level <= 1:
>                 if (i % 10) == 0:
>                     progress.update(i)
> 
4035a4473,4476
>         #  Terminate progress bar
>         if self.silence_level <= 1:
>             progress.finish()
> 
4042c4483
<     @Cached.method(name="coreness")
---
>     @cached_const('base', 'coreness', 'coreness')
4066c4507,4508
<     @Cached.method(name="the master stability function synchronizability")
---
>     @cached_const('base', 'msf sync',
>                   'master stability function synchronizability')
4075,4076c4517,4518
<         undirected climate networks (with symmetric Laplacian matrix).
<         For directed networks, the undirected Laplacian matrix is used.
---
>         undirected climate networks (with symmetric laplacian matrix).
>         For directed networks, the undirected laplacian matrix is used.
4173c4615
<         N, Aplus = self.N, self.sp_Aplus().toarray()
---
>         N, Aplus = self.N, self.sp_Aplus().A
4179a4622,5265
> 
>     def do_nsi_pca_clustering(self, max_n_clusters=None):
>         """
>         Perform a clustering of the nodes using principal components analysis.
> 
>         Perform a PCA for the columns of the adjacency matrix, extract the
>         largest eigenvalues, and assign each node to that eigenvalue whose
>         eigenvector explains the largest amount of the node's column's
>         variance, i.e. the one that maximizes the value of eigenvalue *
>         corresponding factor load on that node's column.
> 
>         .. note::
>            This is still EXPERIMENTAL!
> 
>         :type max_n_clusters: int >= 1
>         :arg  max_n_clusters: Number of clusters to find at most.
>                               (Default: ceil(sqrt(N)))
> 
>         :rtype: tuple (list[node], list[node], list[cluster], 2d numpy array)
>         :return: A list of cluster indices for each node,
>                  a list with the fraction of the node's column's variance
>                  explained by chosen eigenvector, for each node,
>                  a list of eigenvalues corresponding to each cluster,
>                  and an array whose columns are the corresponding eigenvectors
>         """
>         # TODO: works only for undirected graphs so far. For directed, A
>         # stacked with its transpose would have to be used!
> 
>         # CSS (corrected sum of squares); proportional to covariance matrix
>         DwR = self.sp_diag_sqrt_w()
>         DAD = DwR * self.sp_Aplus() * DwR
>         corr = self.nsi_degree() * np.sqrt(self.node_weights)
>         CSS = DAD * DAD - np.outer(corr, corr) / self.total_node_weight
> 
>         # extract max_n_clusters largest eigenvalues and eigenvectors from CSS
>         N = self.N
>         if max_n_clusters is None:
>             max_n_clusters = int(np.ceil(np.sqrt(N)))
>         # total variance is proportional to trace of CSS
>         var = CSS.diagonal().A[0]
> 
>         # target eigenvalue (known upper bound) -> largest eigenvalues
>         tau = sum(var)
>         evals, evecs = eigsh(CSS, k=max_n_clusters, sigma=tau,
>                              maxiter=100*max_n_clusters, tol=1e-8)
> 
>         # fraction of node's variance explained by each eigenvector
>         explained_var = np.power(evecs, 2.0) * evals.reshape((1, evals.size))
> 
>         # assign each node to cluster 2*i or 2*i+1
>         # for that eigenvector i which explains the largest part of the node's
>         # variance.  assign node to cluster 2*i if eigenvector positive at the
>         # node, otherwise to cluster 2*i+1:
>         cluster_index = 2 * np.argmax(explained_var, axis=1)
>         for i in range(0, N):
>             if evecs[i, cluster_index[i]/2] < 0.0:
>                 cluster_index[i] += 1
> 
>         cluster_explained_var = np.max(explained_var, axis=1)
>         cluster_index_set = set(cluster_index)
>         cluster_sizes = np.zeros(max(cluster_index_set)+1)
>         for i in range(0, N):
>             cluster_sizes[cluster_index[i]] += self.node_weights[i]
>         cluster_sizes = cluster_sizes[list(cluster_index_set)]
>         cluster_fit = cluster_explained_var / var
>         if self.silence_level <= 1:
>             print("max_n_clusters was", max_n_clusters)
>             print(f"found {len(evals)} eigenvalues and "
>                   f"{len(cluster_index_set)} clusters")
>             print(f"cluster sizes range from {cluster_sizes.min()} to "
>                   f"{cluster_sizes.max()} with median "
>                   f"{np.median(cluster_sizes)}: {cluster_sizes}")
>             print(f"max and min found eigenvalues are {max(evals)} and "
>                   f"{min(evals)} (average of all was {tau/N})")
>             print(f"pca and clusters explain {sum(evals)/tau} and "
>                   f"{sum(cluster_explained_var)/tau} of total variance.")
> 
>         return (cluster_index,  # cluster_index for each node
>                 cluster_fit,    # fraction of node's variance explained by
>                                 # chosen eigenvector, for each node
>                 evals,          # eigenvalues
>                 evecs)          # matrix with columns=eigenvectors
> 
>     def do_nsi_clustering(self, d0=None, tree_dotfile=None,
>                           distances=None, candidates=None):
>         """
>         Perform agglomerative clustering based on representation accuracy.
> 
>         This minimizes in each step the mean squared error of representing the
>         pairwise node distances by their cluster averages.
> 
>         .. note::
>            This is still EXPERIMENTAL!
> 
>         See the code for arguments and return value.
> 
>         Clusters 0...n-1 are the singletons (cluster i containing just node i).
>         Clusters n...2n-2 are numbered in the order in which clusters are
>         joined (a cluster with id c is a union of two earlier clusters with
>         ids c1,c2 < c). In particular, cluster 2n-2 is the full set of nodes.
> 
>         :rtype:  dictionary
>         :return: A dictionary containing the following keys:
> 
>            - "min_clusters": int > 0. Smallest number of clusters generated.
>            - "error": array(n+1). Entry [k] is the representation error for the
>              solution with k clusters.
>            - "node2cluster": array(n,n+1). Entry [i,k] is the id of the cluster
>              that contains node i in the solution with k clusters.
>            - "cluster_weight": array(2n-1). Entry [c] is the total weight of
>              cluster c.
>            - "cluster2rank": array(2n-1,n+1). Entry [c,k] is the descending
>              order rank of cluster c in the k-cluster solution, i.e., the
>              number of larger clusters in that solution. Use this to convert
>              cluster ids in 0...2n-1 to cluster ids in 0...k-1.
>            - "node_in_cluster": array(n,2n-1). Entry [i,c] indicates whether
>              node i is in the cluster with id c.
>            - "children": array(2n-1,2). Entries [c,0] and [c,1] are the ids of
>              the two clusters that were joined to give cluster c.
>            - "sibling": array(2n-2). Entry [c] is the id of the cluster with
>              which cluster c is joined.
>            - "parent": array(2n-2). Entry [c] is the id of the cluster that
>              results from joining cluster c with its sibling.
>         """
>         N = self.N
>         N2 = 2*N - 1
>         rN = range(N)
>         w = to_cy(self.node_weights, FIELD)
>         k = self.nsi_degree()  # TODO: link weight
> 
>         # init result structures:
>         error = np.zeros(N+1) + np.inf
>         error[-1] = 0.0
>         node2cluster = np.zeros((N, N+1), dtype=np.int16)
>         node2cluster[:, 0] = -1
>         node2cluster[:, N] = rN
>         cluster_weight = np.zeros(N2)
>         cluster_weight[0:N] = w
>         cluster2rank = np.zeros((N2, N+1), dtype=np.int16) - 1
>         cluster2rank[0, 1] = 0
>         node_in_cluster = np.zeros((N, N2), dtype=np.int8)  # or bool?
>         children = np.zeros((N2, 2), dtype=np.int16)
>         children[:N] = -1
>         sibling = np.zeros(N2-1, dtype=np.int16) - 1
>         parent = np.zeros(N2-1, dtype=np.int16) - 1
>         clid = np.arange(N)
> 
>         # a dynamic doubly linked list of distance matrix entries:
>         #  D_firstpos[cl] = pos. of first nb. of cl.
>         #  D_lastpos[cl] = pos. of last nb. of cl.
>         #  D_nextpos[pos] = pos. of next nb. of the same cl.
>         #  D_prevpos[pos] = pos. of previous nb. of the samle cl.
>         #  D_cluster[pos] = cluster index of the neighbour at this pos.
>         #  D_invpos[pos] = pos. of cl. in nbs. list of nbs.
>         # all needed link attributes are stored with the same pos.
>         # when cls are joined, the resp. lists are concatenated and duplicates
>         # are unlinked (but their pos. not reused), so we need 2M many
>         # pos., 1...2M, where pos 0 remains empty:
>         if distances is None:
>             # contains each link twice!
>             distance_keys = nz_coords(self.sp_A)
>         else:
>             try:
>                 distance_keys = distances.keys()
>             except AttributeError:
>                 distance_keys = [(i, j) for i in range(N) for j in range(N)]
>         M = len(distance_keys)
>         rM = range(M)
>         rpos = range(1, M+1)
> 
>         D_firstpos = np.zeros(N, NODE)  # pos. of first nb. of cl.
>         D_lastpos = np.zeros(N, NODE)  # pos. of last nb. of cl.
>         # pos. of next nb. of the same cl.
>         D_nextpos = np.zeros(2*M+1, NODE)
>         # pos. of previous nb. of the samle cl.
>         D_prevpos = np.zeros(2*M+1, NODE)
>         # pos. of cl. in nbs. list of nbs.
>         D_invpos = np.zeros(2*M+1, NODE)
>         # cluster index of the neighbour at this pos.
>         D_cluster = np.zeros(2*M+1, DEGREE)
> 
>         # compute average distance of unconnected pairs,
>         # which will be used as an estimate for them:
>         n_pairs = N * (N-1) / 2
>         if d0 is None:
>             t0 = time.time()
>             if n_pairs > M:
>                 d0 = (self.average_path_length()*1.0 * n_pairs - M) /\
>                      (n_pairs - M)  # TODO: link weight
>             else:
>                 d0 = 1.0 * N
>             print(f"calculated {d0} as average non-linked distance, "
>                   f"needed {time.time()-t0} sec.")
> 
>         dict_D = {}  # weighted sum of distances between clusters
>         dict_Delta = {}  # error increase upon join, only i<j
> 
>         # init the list:
>         t0 = time.time()
>         posj = 0
>         posi = M
>         for i0, j0 in distance_keys:
>             if i0 == j0:
>                 dict_D[(N+1)*i0] = w[i0] * distances[i0, i0]
>                 continue
>             if i0 < j0:
>                 i, j = i0, j0
>             else:
>                 i, j = j0, i0
>             ij = i*N+j
>             if ij in dict_D:
>                 continue
>             posj = posj + 1
>             if D_firstpos[i] == 0:
>                 D_firstpos[i] = D_lastpos[i] = posj
>             else:
>                 D_prevpos[posj] = lpos = D_lastpos[i]
>                 D_nextpos[lpos] = D_lastpos[i] = posj
>             D_cluster[posj] = j
>             if distances is None:
>                 # i.e., use dist 1 if linked, d0 otherwise
>                 Dij = dict_D[ij] = dict_D[j*N+i] = w[i] * w[j]
>             else:
>                 Dij = dict_D[ij] = dict_D[j*N+i] = \
>                     w[i] * w[j] * distances[i0, j0]
>             D_invpos[posj] = posi = posi + 1
>             if D_firstpos[j] == 0:
>                 D_firstpos[j] = D_lastpos[j] = posi
>             else:
>                 D_prevpos[posi] = lpos = D_lastpos[j]
>                 D_nextpos[lpos] = D_lastpos[j] = posi
>             D_cluster[posi] = i
>             D_invpos[posi] = posj
>         del distance_keys
>         print("initialization of distances needed", time.time()-t0, "sec.")
> 
>         # init candidates:
>         t0 = time.time()
>         if candidates is None:
>             candidates = nz_coords(self.sp_A)
>         for i0, j0 in candidates:
>             if i0 < j0:
>                 i, j = i0, j0
>             else:
>                 i, j = j0, i0
>             ij = i*N+j
>             if ij in dict_Delta:
>                 continue
>             wi = w[i]
>             wj = w[j]
>             wc = wi + wj
>             wiwj = wi * wj
>             if ij in dict_D:
>                 Dcc_wc2 = 2 * dict_D.get(ij, wiwj*d0) / wc**2
>             else:
>                 dict_D[ij] = dict_D[j*N+i] = wiwjd0 = wiwj*d0
>                 Dcc_wc2 = 2 * wiwjd0 / wc**2
>             dict_Delta[ij] = (wi**2 + wj**2) * (Dcc_wc2)**2 + \
>                 2 * wiwj * (Dcc_wc2-1)**2
>         print("initialization of candidates needed", time.time()-t0, "sec.")
> 
>         t0 = time.time()
>         cands = to_cy(dict_Delta.keys(), NODE)
>         n_cands = len(cands)
> 
>         dict_Delta = _do_nsi_clustering_I(n_cands, cands, D_cluster, w, d0,
>                                           D_firstpos, D_nextpos, N, dict_D,
>                                           dict_Delta)
>         print(f"initialization of error increments needed"
>               f"{time.time()-t0} sec.")
> 
>         # successively join the best pair:
>         sumt1 = sumt2 = sumt3 = 0.0
>         actives = range(N)
>         min_clusters = 1
>         for n_clusters in range(N-1, 0, -1):
> 
>             # find best pair a<b:
>             t0 = time.time()
>             vals = dict_Delta.values()
>             if not vals:
>                 min_clusters = n_clusters + 1
>                 break
>             minpos = np.argmin(vals)
>             ab = dict_Delta.keys()[minpos]
>             del dict_Delta[ab]
>             a = ab / N
>             b = ab % N
>             this_error = vals[minpos]
>             sumt1 += time.time()-t0
> 
>             # remove duplicates in D and rewire nbs c1 of b to point to a:
>             delkeys = [(b, b)]
>             t0 = time.time()
>             lpos = D_lastpos[a]
>             D_nextpos[lpos] = posc1 = D_firstpos[b]
>             D_prevpos[posc1] = lpos
>             D_lastpos[a] = D_lastpos[b]
>             while posc1 != 0:
>                 c1 = D_cluster[posc1]
>                 delkeys += [(c1, b)]
>                 if c1 < a:
>                     c1akey = c1*N+a
>                 else:
>                     c1akey = a*N+c1
>                 if c1 < b:
>                     c1bkey = c1*N+b
>                 else:
>                     c1bkey = b*N+c1
>                 if c1bkey in dict_Delta:  # rewire cand. c1-b to c1-a:
>                     del dict_Delta[c1bkey]
>                     dict_Delta[c1akey] = 0.0  # will later be recomputed!
>                 if c1 == a or c1akey in dict_D:
>                     iposc1 = D_invpos[posc1]
>                     ippos = D_prevpos[iposc1]
>                     inpos = D_nextpos[iposc1]
>                     if ippos > 0:
>                         D_nextpos[ippos] = inpos
>                     else:
>                         D_firstpos[c1] = inpos
>                     if inpos > 0:
>                         D_prevpos[inpos] = ippos
>                     else:
>                         D_lastpos[c1] = ippos
>                     ppos = D_prevpos[posc1]
>                     posc1 = D_nextpos[posc1]
>                     if ppos > 0:
>                         D_nextpos[ppos] = posc1
>                     else:
>                         D_firstpos[a] = posc1
>                     if posc1 > 0:
>                         D_prevpos[posc1] = ppos
>                     else:
>                         D_lastpos[a] = ppos
>                 else:
>                     D_cluster[D_invpos[posc1]] = a
>                     posc1 = D_nextpos[posc1]
>             D_firstpos[b] = D_lastpos[b] = 0
>             sumt2 += time.time()-t0
> 
>             # TODO: this is the bottleneck, so speed it up:
>             # first update Delta[a1,b1] for each pair a1,b1 with a1 linked to c
>             # and b1 != c, and compute Delta[a1,c] for each a1 linked to c:
>             wa = w[a]
>             wb = w[b]
>             wc = wa + wb
>             wad0 = wa * d0
>             wbd0 = wb * d0
> 
>             t0 = time.time()
> 
>             dict_Delta = _do_nsi_clustering_II(a, b, D_cluster, w, d0,
>                                                D_firstpos, D_nextpos, N,
>                                                dict_D, dict_Delta)
>             sumt3 = time.time()-t0
> 
>             # finally update D:
>             Daa = dict_D.get(a*(N+1), 0.0)
>             Dbb = dict_D.get(b*(N+1), 0.0)
>             dict_D[a*(N+1)] = Daa + Dbb + 2*dict_D[a*N+b]
>             posc1 = D_firstpos[a]
>             while posc1 > 0:
>                 c1 = D_cluster[posc1]
>                 Dac1 = dict_D.get(a*N+c1, w[c1]*wad0)
>                 Dbc1 = dict_D.get(b*N+c1, w[c1]*wbd0)
>                 dict_D[c1*N+a] = dict_D[a*N+c1] = Dac1 + Dbc1
>                 posc1 = D_nextpos[posc1]
> 
>             # update result structures:
>             c = N2 - n_clusters
>             error[n_clusters] = error[n_clusters+1] + this_error
>             # TODO: node2cluster
>             cluster_weight[c] = wc
>             # TODO: cluster2rank
>             # TODO: node_in_cluster
>             children[c, 0] = ca = clid[a]
>             children[c, 1] = sibling[ca] = cb = clid[b]
>             sibling[cb] = ca
>             parent[ca] = parent[cb] = c
>             parent[c] = N2 - 1  # initially, until joined.
> 
>             # remove b and replace a by c:
>             for k1, k2 in delkeys:
>                 try:
>                     del dict_D[k1*N+k2], dict_D[k2*N+k1]
>                 except KeyError:
>                     pass
>             actives.remove(b)
>             clid[a] = c
>             w[a] = wc
> 
>             print(n_clusters, ": joining", ca, cb, "to", c, "at", this_error)
>             if n_clusters < 10:
>                 print("D", dict_D)
>                 print("Delta", dict_Delta)
> 
>         print("part 1 needed", sumt1, "sec.")
>         print("part 2 needed", sumt2, "sec.")
>         print("part 3 needed", sumt3, "sec.")
> 
>         if tree_dotfile is not None:
>             # use penwidth and len!
>             edges = [(int(n), int(parent[n])) for n in range(N2-1)]
>             minlen = [int(parent[n]-max(n, N-1)) for n in range(N2-1)]
>             # TODO: eps + error difference
>             edgelen = np.array(
>                 [max(0.0, error[N2-parent[n]]) for n in range(N)]
>                 + [max(0.0, error[N2-parent[n]]-error[N2-n])
>                    for n in range(N, N2-1)])  # minlen
>             # TODO: 1/(eps + error difference)
>             # [1.0 for i in range(N2-1)]
>             penwidth = 30.0 / (1.0 + 29.0*edgelen/edgelen.max())
>             tree = igraph.Graph(edges, directed=True)
>             tree.es.set_attribute_values("minlen", minlen)
>             tree.es.set_attribute_values("len", edgelen)
>             tree.es.set_attribute_values("penwidth", penwidth)
>             tree["rankdir"] = "BT"
>             tree.write_dot(tree_dotfile)
>             del tree
> 
>         return {
>             "min_clusters": min_clusters, "node2cluster": node2cluster,
>             "cluster2rank": cluster2rank, "cluster_weight": cluster_weight,
>             "node_in_cluster": node_in_cluster, "error": error,
>             "children": children, "sibling": sibling, "parent": parent
>         }
> 
>     def do_nsi_hamming_clustering(self, admissible_joins=None, alpha=0.01,
>                                   tree_dotfile=None):
>         """
>         Perform agglomerative clustering based on Hamming distances.
> 
>         This minimizes in each step the Hamming distance between the original
>         and the "clustered" network.
> 
>         .. note::
>            This is still EXPERIMENTAL!
> 
>         See the code for arguments and return value.
> 
>         Clusters 0...n-1 are the singletons (cluster i containing just node i).
>         Clusters n...2n-2 are numbered in the order in which clusters are
>         joined (a cluster with id c is a union of two earlier clusters with
>         ids c1,c2 < c). In particular, cluster 2n-2 is the full set of nodes.
> 
>         :rtype:  dictionary
>         :return: A dictionary containing the following keys:
> 
>            - "error": array(n+1). Entry [k] is the representation error for the
>              solution with k clusters.
>            - "node2cluster": array(n,n+1). Entry [i,k] is the id of the cluster
>              that contains node i in the solution with k clusters.
>            - "cluster_weight": array(2n-1). Entry [c] is the total weight of
>              cluster c.
>            - "cluster2rank": array(2n-1,n+1). Entry [c,k] is the descending
>              order rank of cluster c in the k-cluster solution, i.e., the
>              number of larger clusters in that solution. Use this to convert
>              cluster ids in 0...2n-1 to cluster ids in 0...k-1.
>            - "node_in_cluster": array(n,2n-1). Entry [i,c] indicates whether
>              node i is in the cluster with id c.
>            - "children": array(2n-1,2). Entries [c,0] and [c,1] are the ids of
>              the two clusters that were joined to give cluster c.
>            - "sibling": array(2n-2). Entry [c] is the id of the cluster with
>              which cluster c is joined.
>            - "parent": array(2n-2). Entry [c] is the id of the cluster that
>              results from joining cluster c with its sibling.
>         """
>         # took about 15h on Zuse for HadCM3 globe
>         # ?takes about 90*(N/800)^4 seconds on a 1.67 GHz i686,
>         # which makes about 10 days for N=8000 (e.g. a HadCM3 globe)
> 
>         t0 = time.time()
> 
>         # initialize data structures:
> 
>         n = self.N
>         n2 = 2*n-1
>         w = self.node_weights
>         WW = self.total_node_weight**2
> 
>         # join admissibility matrix:
>         if admissible_joins is None:
>             print("all joins admissible")
>             mayJoin = np.zeros((n2, n2), dtype=MASK) + 1
>         else:
>             print("only some joins admissible")
>             mayJoin = np.zeros((n2, n2), dtype=MASK)
>             mayJoin[0:n, 0:n] = admissible_joins
>         # cluster membership indicators:
>         clusterMembers = np.zeros((n2, n), dtype=int)
>         clusterMembers[0:n, 0:n] = np.identity(n)
>         # cluster weights:
>         clusterWeights = np.zeros(n2)
>         clusterWeights[0:n] = w
>         # weight products:
>         weightProducts = np.zeros((n2, n2), dtype=DFIELD)
>         weightProducts[0:n, 0:n] = np.dot(w.reshape((n, 1)), w.reshape((1, n)))
>         # linked weights:
>         A, Aplus = self.adjacency, self.sp_Aplus().A
>         linkedWeights = np.zeros((n2, n2), dtype=DFIELD)
>         linkedWeights[0:n, 0:n] = \
>             self.node_weights.reshape((n, 1)) * Aplus * \
>             self.node_weights.reshape((1, n))
>         # error contributions of cluster pairs
>         # (sum up to total error = 2*Hamming distance):
>         errors = np.zeros((n2, n2), dtype=DFIELD)
>         # distance = increase of Hamming distance:
>         # and find first pair to join:
>         distances = np.zeros((n2, n2), dtype=DFIELD)
> 
>         # list of active cluster indices:
>         activeIndices = range(0, n)
> 
>         # final Hamming distances:
>         hamming = np.zeros(n2)
> 
>         # list of parents and siblings:
>         sibling = np.zeros(n2-1, dtype=int)
>         parent = np.zeros(n2-1, dtype=int)
> 
>         # list of parts:
>         parts = np.zeros((n2, 2), dtype=int)
>         parts[:n] = -1
> 
>         node2cluster = np.zeros((n, n+1), dtype=int)
>         node2cluster[:, 0] = -1
>         node2cluster[:, n] = range(n)
>         cluster2rank = np.zeros((n2, n+1), dtype=int) - 1
>         cluster2rank[0, 1] = 0
> 
>         lastunited = part1 = part2 = -1
> 
>         # iteratively join those two clusters which increase Hamming distance
>         # the least:
>         for united in range(n, n2):
> 
>             # find next optimal pair:
> 
>             nActiveIndices = len(activeIndices)
>             theActiveIndices = np.sort(activeIndices)
>             mind0 = float(np.power(1.0*self.total_node_weight, 3.0))
>             minwp0 = float(2.0*weightProducts.max())
> 
>             result = np.zeros(3, dtype=DFIELD)
>             _do_nsi_hamming_clustering(
>                 n2, nActiveIndices, mind0, minwp0, lastunited, part1, part2,
>                 to_cy(distances, DFIELD),
>                 to_cy(theActiveIndices, NODE),
>                 to_cy(linkedWeights, DFIELD),
>                 to_cy(weightProducts, DFIELD),
>                 to_cy(errors, DFIELD),
>                 result,
>                 to_cy(mayJoin, NODE))
> 
>             mind = result[0]
>             part1 = int(result[1])
>             part2 = int(result[2])
>             if mind < 0:
>                 print(united, mind, part1, part2)
>                 raise Exception
> 
>             cluster2rank[np.array(activeIndices)[
>                 (-clusterWeights[activeIndices]).argsort()], n2+1-united] = \
>                 range(n2+1-united)
> 
>             hamming[united] = hamming[united-1] + 2.0 * mind
> 
>             if united < n + 100 or united % (1 + n2/100) == 0 or \
>                     united >= n2 - 100:
>                 print(f"for {n2-united} clusters with error "
>                       f"{hamming[united]/WW} we join clusters "
>                       f"{part1} and {part2} to get cluster {united}")
>                 sys.stdout.flush()
> 
>             # unite parts:
> 
>             parent[part1] = parent[part2] = united
>             parts[united, 0] = sibling[part2] = part1
>             parts[united, 1] = sibling[part1] = part2
>             clusterMembers[united, :] = \
>                 clusterMembers[part1, :] + clusterMembers[part2, :]
>             node2cluster[:, n2-united] = \
>                 node2cluster[:, 1+n2-united] * (1-clusterMembers[united, :]) +\
>                 united*clusterMembers[united, :]
>             activeIndices.remove(part1)
>             activeIndices.remove(part2)
>             activeIndices.append(united)
> 
>             # compute new entries in clusterWeights, weightProducts,
>             # linkedWeights, errors, mayJoin:
>             clusterWeights[united] = \
>                 clusterWeights[part1] + clusterWeights[part2]
>             weightProducts[united, 0:united] = \
>                 weightProducts[part1, 0:united] + \
>                 weightProducts[part2, 0:united]
>             weightProducts[0:united, united] = \
>                 weightProducts[united, 0:united].flatten()
>             weightProducts[united, united] = \
>                 np.power(clusterWeights[united], 2.0)
>             linkedWeights[united, 0:united] = \
>                 linkedWeights[part1, 0:united] + linkedWeights[part2, 0:united]
>             linkedWeights[0:united, united] = \
>                 linkedWeights[united, 0:united].flatten()
>             linkedWeights[united, united] = \
>                 linkedWeights[part1, part1] + linkedWeights[part2, part2] + \
>                 2.0 * linkedWeights[part1, part2]
>             mayJoin[united, 0:united] = \
>                 mayJoin[part1, 0:united] + mayJoin[part2, 0:united]
>             mayJoin[0:united, united] = mayJoin[united, 0:united].flatten()
>             for c in range(0, united):
>                 lw = linkedWeights[united, c]
>                 errors[united, c] = errors[c, united] = \
>                     min(lw, weightProducts[united, c] - lw)
>             errors[united, united] = \
>                 weightProducts[united, united] - linkedWeights[united, united]
>             if errors.min() < -1e-10:
>                 print(errors)
>                 raise Exception
>             lastunited = united
> 
>         print(time.time()-t0, "seconds")
> 
>         # node2cluster = np.array(range(0, n2)).reshape((n2, 1))*clusterMembers
> 
>         node_in_cluster = clusterMembers.T.astype(int)
>         error = np.zeros(n+1)
>         error[0] = np.inf
>         error[-1-np.arange(n)] = hamming[-n:] / WW
> 
>         if tree_dotfile is not None:
>             edges = [(int(i), int(parent[i])) for i in range(n2-1)]
>             minlen = [int(parent[i]-max(i, n-1)) for i in range(n2-1)]
>             tree = igraph.Graph(edges, directed=True)
>             tree.es.set_attribute_values("minlen", minlen)
>             tree["rankdir"] = "LR"
>             tree.write_dot(tree_dotfile)
>             del tree
> 
>         return {
>             "node2cluster": node2cluster, "cluster2rank": cluster2rank,
>             "cluster_weight": clusterWeights,
>             "node_in_cluster": node_in_cluster, "error": error,
>             "children": parts, "sibling": sibling, "parent": parent
>         }
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/core/resistive_network.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/core/resistive_network.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
149,150c152,153
<         return (f"ResNetwork:\n{GeoNetwork.__str__(self)}\n"
<                 f"Average resistance: {self.resistances.mean()}")
---
>         return(f"ResNetwork:\n{GeoNetwork.__str__(self)}\n"
>         f"Average resistance: {self.resistances.mean()}")
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/core/spatial_network.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/core/spatial_network.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
19,21c22
< from typing import Tuple
< from collections.abc import Hashable
< 
---
> # array object and fast numerics
22a24
> # random number generation
23a26
> # high performance graph theory tools written in pure ANSI-C
29a33
> from .network import cached_const
33a38,41
> #
> #  Define class SpatialNetwork
> #
> 
46,47c54,55
<     def __init__(self, grid: Grid, adjacency=None, edge_list=None,
<                  directed=False, silence_level=0):
---
>     def __init__(self, grid, adjacency=None, edge_list=None, directed=False,
>                  silence_level=0):
62,63c70
<         assert isinstance(grid, Grid)
<         self.grid: Grid = grid
---
>         self.grid = grid
70,72d76
<     def __cache_state__(self) -> Tuple[Hashable, ...]:
<         return Network.__cache_state__(self) + (self.grid,)
< 
78a83,92
>     def clear_cache(self):
>         """
>         Clean up cache.
> 
>         Is reversible, since all cached information can be recalculated from
>         basic data.
>         """
>         Network.clear_cache(self)
>         self.grid.clear_cache()
> 
83,84c97,98
<     # pylint: disable=keyword-arg-before-vararg
<     def save(self, filename, fileformat=None, *args, **kwds):
---
>     def save(self, filename_network, filename_grid=None, fileformat=None,
>              *args, **kwds):
105,107c119,122
<         :arg tuple/list filename: Tuple or list of two strings, namely
<             the paths to the files where the Network object and the
<             GeoGrid object are to be stored (filename_network, filename_grid)
---
>         :arg str filename_network:  The name of the file where the Network
>             object is to be stored.
>         :arg str filename_grid:  The name of the file where the GeoGrid object
>             is to be stored (including ending).
119,123d133
<         try:
<             (filename_network, filename_grid) = filename
<         except ValueError as e:
<             raise ValueError("'filename' must be a tuple or list of two "
<                              "items: filename_network, filename_grid") from e
132d141
<     # pylint: disable=keyword-arg-before-vararg
134c143,144
<     def Load(filename, fileformat=None, silence_level=0, *args, **kwds):
---
>     def Load(filename_network, filename_grid, fileformat=None,
>              silence_level=0, *args, **kwds):
150c160
<         any changes.
---
>         any changes.Read
152,154c162,165
<         :arg tuple/list filename: Tuple or list of two strings, namely
<             the paths to the files containing the Network object and the
<             Grid object (filename_network, filename_grid)
---
>         :arg str filename_network:  The name of the file where the Network
>             object is to be stored.
>         :arg str filename_grid:  The name of the file where the Grid object
>             is to be stored (including ending).
167,172d177
<         try:
<             (filename_network, filename_grid) = filename
<         except ValueError as e:
<             raise ValueError("'filename' must be a tuple or list of two "
<                              "items: filename_network, filename_grid") from e
< 
185,186c190
<                              directed=graph.is_directed(),
<                              silence_level=silence_level)
---
>                              directed=graph.is_directed())
197,198c201,203
<         #  invalidate cache
<         net._mut_la += 1
---
>         #  Restore link attributes/weights
>         net.clear_paths_cache()
> 
509c514
<     def _calculate_general_average_link_distance(self, adjacency, degree,
---
>     def _calculate_general_average_link_distance(self, adjacency, degrees,
525,526c530,531
<         :type degree: 1D array [index]
<         :arg degree: The degree sequence.
---
>         :type degrees: 1D array [index]
>         :arg degrees: The degree sequence.
531a537
>         k = self.degree()
535,537c541,543
<         #  Normalize by degree, not by number of nodes
<         average_link_distance[degree != 0] = \
<             (D * adjacency).sum(axis=1)[degree != 0] / degree[degree != 0]
---
>         #  Normalize by degree, not by number of nodes!!!
>         average_link_distance[k != 0] = \
>             (D * adjacency).sum(axis=1)[k != 0] / k[k != 0]
574c580
<         A = self.undirected_adjacency().toarray()
---
>         A = self.undirected_adjacency().A
651c657
<         A = self.undirected_adjacency().toarray()
---
>         A = self.undirected_adjacency().A
660a667
>     @cached_const('base', 'distance')
666,667c673
<         if not self.find_link_attribute('distance'):
<             self.set_link_attribute('distance', dist)
---
>         self.set_link_attribute('distance', dist)
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/eventseries/event_series.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/eventseries/event_series.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
28,30c31,33
< from typing import Tuple
< from collections.abc import Hashable
< 
---
> #
> # Imports
> #
33a37
> 
36c40
< from ..core.cache import Cached
---
> from .. import cached_const
39c43
< class EventSeries(Cached):
---
> class EventSeries:
57c61
<                    [1, 0, 1],
---
>                    [0, 0, 0],
62,63c66,67
<                    [1, 0, 0],
<                    [0, 0, 1],
---
>                    [0, 0, 0],
>                    [0, 0, 0],
146a151,154
>         # Dictionary for chached constants
>         self.cache = {'base': {}}
>         """(dict) cache of re-usable computation results"""
> 
157,161d164
<     def __cache_state__(self) -> Tuple[Hashable, ...]:
<         # The following attributes are assumed immutable:
<         #   (__eventmatrix, __timestamps, __taumax, __lag)
<         return ()
< 
166,168c169,171
<         return (f"EventSeries: {self.__N} variables, "
<                 f"{self.__T} timesteps, taumax: {self.__taumax:.1f}, "
<                 f"lag: {self.__lag:.1f}")
---
>         return(f"EventSeries: {self.__N} variables, "
>         f"{self.__T} timesteps, taumax: {self.__taumax:.1f}, "
>         f"lag: {self.__lag:.1f}")
293c296
<         elif not threshold_method.shape:
---
>         elif threshold_method.shape == ():
313c316
<             elif not threshold_values.shape:
---
>             elif threshold_values.shape == ():
339c342
<             elif not threshold_types.shape:
---
>             elif threshold_types.shape == ():
533,534c536,537
<          :rtype: list
<          :return: [Precursor coincidence rate XY, Trigger coincidence rate XY,
---
>          :rtype list
>          :return [Precursor coincidence rate XY, Trigger coincidence rate XY,
599,606c602,609
<          :type window_type: str {'retarded', 'advanced', 'symmetric'}
<          :arg window_type: Only for ECA. Determines if precursor coincidence
<                            rate ('advanced'), trigger coincidence rate
<                            ('retarded') or a general coincidence rate with the
<                            symmetric interval [-taumax, taumax] are computed
<                            ('symmetric'). Default: 'symmetric'
<          :rtype: list
<          :return: Precursor coincidence rates [XY, YX]
---
>         :type window_type: str {'retarded', 'advanced', 'symmetric'}
>         :arg window_type: Only for ECA. Determines if precursor coincidence
>                           rate ('advanced'), trigger coincidence rate
>                           ('retarded') or a general coincidence rate with the
>                           symmetric interval [-taumax, taumax] are computed
>                           ('symmetric'). Default: 'symmetric'
>          :rtype list
>          :return [Precursor coincidence rate XY, Precursor coincidence rate YX]
607a611
> 
711c715
<         1/N_X sum_{i=1}^{N_X} Theta[sum{j=1}^{N_Y}
---
>          1/N_X sum_{i=1}^{N_X} Theta[sum{j=1}^{N_Y}
721c725
<                               'mean', 'max', 'min'} for ES,
---
>                                    'mean', 'max', 'min'} for ES,
731,732c735,736
<         :rtype: 2D numpy array
<         :return: pairwise event synchronization or pairwise coincidence rates
---
>         :rtype 2D numpy array
>         :return pairwise event synchronization or pairwise coincidence rates
771c775
<     @Cached.method()
---
>     @cached_const('base', 'directedES')
841c845
<                               'mean', 'max', 'min'} for ES,
---
>                                    'mean', 'max', 'min'} for ES,
929c933
<                               'mean', 'max', 'min'} for ES,
---
>                                    'mean', 'max', 'min'} for ES,
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/eventseries/__init__.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/eventseries/__init__.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
29a33,37
> 
> Known Bugs
> ~~~~~~~~~~
>   - ...
> 
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/funcnet/coupling_analysis_pure_python.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/funcnet/coupling_analysis_pure_python.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
106,107c109,110
<         return (f'CouplingAnalysisPurePython: {shape[0]} variables, '
<                 f'{shape[1]} timesteps.')
---
>         return 'CouplingAnalysisPurePython: %i variables, %i timesteps.' % (
>             shape[0], shape[1])
165,166c168,169
<         return self._calculate_cc(normalized_array, tau_max=tau_max,
<                                   lag_mode=lag_mode)
---
>         return self._calculate_cc(normalized_array, corr_range=corr_range,
>                                   tau_max=tau_max, lag_mode=lag_mode)
180,184d182
<         # pylint: disable=possibly-used-before-assignment
< 
<         if lag_mode not in self.lag_modi:
<             raise ValueError('lag_mode must be "all", "sum" or "max".')
< 
202c200,201
<         res = self._calculate_cc(sample_array, tau_max=0, lag_mode='all')
---
>         res = self._calculate_cc(sample_array, corr_range=corr_range,
>                                  tau_max=0, lag_mode='all')
245,246c244,245
<         return self._calculate_cc(sample_array, tau_max=tau_max,
<                                   lag_mode=lag_mode)
---
>         return self._calculate_cc(sample_array, corr_range=sample_range,
>                                   tau_max=tau_max, lag_mode=lag_mode)
248c247
<     def _calculate_cc(self, array, tau_max, lag_mode):
---
>     def _calculate_cc(self, array, corr_range, tau_max, lag_mode):
260d258
<         # pylint: disable=used-before-assignment
371c369
<         bin_edge = numpy.ceil(corr_range/float(bins)).astype(int)
---
>         bin_edge = numpy.ceil(corr_range/float(bins))
393c391
<     def mutual_information_edges(self, bins=16, tau=0):
---
>     def mutual_information_edges(self, bins=16, tau=0, lag_mode='all'):
411a410,420
>         Possible choices for lag_mode:
> 
>         - "all" will return the full function for all lags, possible large
>           memory need if only_tri is True, only the upper triangle contains the
>           values, the lower one is zeros
>         - "sum" will return the sum over positive and negative lags seperatly,
>           each inclunding tau=0 corrmat[0] is the positive sum, corrmat[1] the
>           negative sum
>         - "max" will return only the maximum coupling (in corrmat[0]) and its
>           lag (in corrmat[1])
> 
413a423
>         :arg str lag_mode: output mode
442,446d451
<         # pylint: disable=possibly-used-before-assignment
< 
<         if lag_mode not in self.lag_modi:
<             raise ValueError('lag_mode must be "all", "sum" or "max".')
< 
465c470
<         bin_edge = numpy.ceil(corr_range/float(bins)).astype(int)
---
>         bin_edge = numpy.ceil(corr_range/float(bins))
554d558
<         # pylint: disable=used-before-assignment
597,600c601,604
<                     for m in range(bins):
<                         for n in range(bins):
<                             jointent -= gfunc[hist2D[m, n]]
<                             hist2D[m, n] = 0
---
>                     for l in range(bins):
>                         for m in range(bins):
>                             jointent -= gfunc[hist2D[l, m]]
>                             hist2D[l, m] = 0
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/funcnet/coupling_analysis.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/funcnet/coupling_analysis.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
26,27c29
< from ..core._ext.types import to_cy, LAG, FIELD, \
<     INT16TYPE, INT32TYPE, INT64TYPE
---
> from ..core._ext.types import to_cy, LAG, FIELD
73,74c75,76
<         return (f'CouplingAnalysis: {self.N} variables, '
<                 f'{self.n_time} timesteps.')
---
>         return 'CouplingAnalysis: %i variables, %i timesteps.' % (
>             self.N, self.n_time)
133,134c135,136
<         return _symmetrize_by_absmax(to_cy(similarity_matrix, FIELD),
<                                      to_cy(lag_matrix, LAG), self.N)
---
>         return _symmetrize_by_absmax(
>             to_cy(similarity_matrix, FIELD), to_cy(lag_matrix, LAG), self.N)
195,197c197,203
<         assert numpy.isnan(data).sum() == 0, "NaNs in the data"
<         assert tau_max >= 0, f"{tau_max =}"
<         assert lag_mode in ['max', 'all'], f"{lag_mode =}"
---
>         if numpy.isnan(data).sum() != 0:
>             raise ValueError("NaNs in the data")
>         if tau_max < 0:
>             raise ValueError("tau_max = %d, but 0 <= tau_max" % tau_max)
>         if lag_mode not in ['max', 'all']:
>             raise ValueError("lag_mode = %s, but must be one of 'max', 'all'"
>                              % lag_mode)
313,316c319,322
<         assert numpy.isnan(data).sum() == 0, "NaNs in the data"
<         assert tau_max >= 0, f"{tau_max =}"
<         if estimator not in ('knn', 'binning', 'gauss'):
<             raise ValueError('estimator must be "knn", "binning" or "gauss".')
---
>         if numpy.isnan(data).sum() != 0:
>             raise ValueError("NaNs in the data")
>         if tau_max < 0:
>             raise ValueError("tau_max = %d, but 0 <= tau_max" % tau_max)
318c324,326
<             assert 1 <= knn <= T/2., f"{knn =}"
---
>             if knn > T/2. or knn < 1:
>                 raise ValueError("knn = %s, should be between 1 and T/2"
>                                  % str(knn))
321,322c329,330
<             similarity_matrix = numpy.ones((N, N), dtype=FIELD)
<             lag_matrix = numpy.zeros((N, N), dtype=LAG)
---
>             similarity_matrix = numpy.ones((N, N), dtype='float32')
>             lag_matrix = numpy.zeros((N, N), dtype='int8')
324c332
<             lagfuncs = numpy.zeros((N, N, tau_max+1), dtype=FIELD)
---
>             lagfuncs = numpy.zeros((N, N, tau_max+1), dtype='float32')
393d400
<                         # pylint: disable=possibly-used-before-assignment
502,503c509,510
<             raise TypeError(f"data is of type {type(data)},"
<                             " must be numpy.ndarray")
---
>             raise TypeError("data is of type %s, must be numpy.ndarray"
>                             % type(data))
513,515c520
<             raise ValueError(f"tau_max = {tau_max}, but 0 <= tau_max")
<         if estimator not in ('knn', 'binning', 'gauss'):
<             raise ValueError('estimator must be "knn", "binning" or "gauss".')
---
>             raise ValueError("tau_max = %d, but 0 <= tau_max" % tau_max)
521,522c526,527
<             similarity_matrix = numpy.ones((N, N), dtype=FIELD)
<             lag_matrix = numpy.zeros((N, N), dtype=LAG)
---
>             similarity_matrix = numpy.ones((N, N), dtype='float32')
>             lag_matrix = numpy.zeros((N, N), dtype='int8')
524c529
<             lagfuncs = numpy.zeros((N, N, tau_max+1), dtype=FIELD)
---
>             lagfuncs = numpy.zeros((N, N, tau_max+1), dtype='float32')
588d592
<                         # pylint: disable=possibly-used-before-assignment
652a657
> 
657c662
<             array = array.astype(FIELD)
---
>             array = array.astype('float32')
667c672,675
<         array += 1E-10 * numpy.random.rand(dim, T)
---
>         array += 1E-10 * numpy.random.rand(array.shape[0], array.shape[1])
> 
>         # Flatten for fast cython access
>         array = array.flatten()
674c682
<             to_cy(array, FIELD), dim, T, dim_x, dim_y, k)
---
>             to_cy(array, FIELD), T, dim_x, dim_y, k, dim)
696c704
<         bin_edge = numpy.ceil(T/float(bins)).astype(int)
---
>         bin_edge = numpy.ceil(T/float(bins))
698c706
<         symb_array = numpy.zeros((dim, T), dtype=INT32TYPE)
---
>         symb_array = numpy.zeros((dim, T), dtype='int32')
732,733c740,741
<             f'base = {base}, D = {D}: Histogram failed:'
<             ' dimension D*base**D exceeds int64 data type')
---
>             'base = %d, D = %d: Histogram failed: '
>             'dimension D*base**D exceeds int64 data type') % (base, D)
735,736c743,744
<         flathist = numpy.zeros((base**D), dtype=INT16TYPE)
<         multisymb = numpy.zeros(T, dtype=INT64TYPE)
---
>         flathist = numpy.zeros((base**D), dtype='int16')
>         multisymb = numpy.zeros(T, dtype='int64')
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/funcnet/_ext/__init__.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/funcnet/_ext/__init__.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/funcnet/_ext/numerics.pyx lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/funcnet/_ext/numerics.pyx
0a1,2
> # -*- coding: utf-8 -*-
> #
2,3c4,5
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
14a17,18
> cimport cython
> 
15a20
> cimport numpy as cnp
18,19c23,24
< from ...core._ext.types import LAG, FIELD, DFIELD, NODE
< from ...core._ext.types cimport LAG_t, FIELD_t, DFIELD_t, NODE_t
---
> from ...core._ext.types import LAG, FIELD, INT32TYPE
> from ...core._ext.types cimport LAG_t, FIELD_t, INT32TYPE_t
21c26,34
< # coupling_analysis ===========================================================
---
> cdef extern from "src_numerics.c":
>     void _symmetrize_by_absmax_fast(float *similarity_matrix,
>             signed char *lag_matrix, int N)
>     void _cross_correlation_max_fast(float *array, float *similarity_matrix,
>             signed char *lag_matrix, int N, int tau_max, int corr_range)
>     void _cross_correlation_all_fast(float *array, float *lagfuncs, int N,
>             int tau_max, int corr_range)
>     void _get_nearest_neighbors_fast(float *array, int T, int dim_x, int dim_y,
>             int k, int dim, int *k_xy, int *k_yz, int *k_z)
24,26c37
< def _symmetrize_by_absmax(
<         ndarray[FIELD_t, ndim=2, mode='c'] similarity_matrix not None,
<         ndarray[LAG_t, ndim=2, mode='c'] lag_matrix not None, int N):
---
> # coupling_analysis ===========================================================
28,29c39,41
<     cdef:
<         int i, j, I, J
---
> def _symmetrize_by_absmax(
>     ndarray[FIELD_t, ndim=2, mode='c'] similarity_matrix not None,
>     ndarray[LAG_t, ndim=2, mode='c'] lag_matrix not None, int N):
31,41c43,45
<     # loop over all node pairs
<     for i in range(N):
<         for j in range(i+1, N):
<             # calculate max and argmax by comparing to
<             # previous value and storing max
<             if abs(similarity_matrix[i, j]) > abs(similarity_matrix[j, i]):
<                 I, J = i, j
<             else:
<                 I, J = j, i
<             similarity_matrix[J, I] = similarity_matrix[I, J]
<             lag_matrix[J, I] = -lag_matrix[I, J]
---
>     _symmetrize_by_absmax_fast(
>         <FIELD_t*> cnp.PyArray_DATA(similarity_matrix),
>         <LAG_t*> cnp.PyArray_DATA(lag_matrix), N)
47,48c51,52
<         ndarray[FIELD_t, ndim=3, mode='c'] array not None,
<         int N, int tau_max, int corr_range):
---
>     ndarray[FIELD_t, ndim=3, mode='c'] array not None,
>     int N, int tau_max, int corr_range):
56c60
<         int i, j, tau, k, argmax
---
>         int i,j,tau,k, argmax
70c74
<                         crossij += array[tau, i, k] * array[tau_max, j, k]
---
>                         crossij += array[tau,i,k] * array[tau_max,j,k]
76,77c80,81
<                 similarity_matrix[i, j] = <FIELD_t> (max / corr_range)
<                 lag_matrix[i, j] = <LAG_t> (tau_max - argmax)
---
>                 similarity_matrix[i,j] = <FIELD_t> (max / corr_range)
>                 lag_matrix[i,j] = <LAG_t> (tau_max - argmax)
83,84c87,88
<         ndarray[FIELD_t, ndim=3, mode='c'] array not None,
<         int N, int tau_max, int corr_range):
---
>     ndarray[FIELD_t, ndim=3, mode='c'] array not None,
>     int N, int tau_max, int corr_range):
85a90,92
>     """
>     lagfuncs = np.zeros((N, N, tau_max+1), dtype="float32")
>     """
96c103
<             for tau in range(tau_max + 1):
---
>             for tau in range(tau_max):
101,102c108,110
<                     crossij += array[tau, i, k] * array[tau_max, j, k]
<                 lagfuncs[i, j, tau_max-tau] = <FIELD_t> (crossij / corr_range)
---
>                     crossij += array[tau,i,k] * array[tau_max,j,k]
> 
>                 lagfuncs[i,j,tau_max-tau] = <FIELD_t> (crossij / corr_range)
108,109c116,117
<         ndarray[FIELD_t, ndim=2, mode='c'] array not None,
<         int dim, int T, int dim_x, int dim_y, int k):
---
>         ndarray[FIELD_t, ndim=1, mode='c'] array not None,
>         int T, int dim_x, int dim_y, int k, int dim):
113,219c121,131
<         int i, j, index, t, m, n, d, kxz, kyz, kz
<         double dz, dxyz, dx, dy, eps, epsmax
<         ndarray[NODE_t, ndim=1, mode='c'] indexfound = np.zeros(T, dtype=NODE)
<         ndarray[DFIELD_t, ndim=2, mode='c'] dist = np.zeros((dim, T), dtype=DFIELD)
<         ndarray[DFIELD_t, ndim=1, mode='c'] dxyzarray = np.zeros(k+1, dtype=DFIELD)
<         ndarray[NODE_t, ndim=1, mode='c'] k_xz = np.zeros(T, dtype=NODE)
<         ndarray[NODE_t, ndim=1, mode='c'] k_yz = np.zeros(T, dtype=NODE)
<         ndarray[NODE_t, ndim=1, mode='c'] k_z = np.zeros(T, dtype=NODE)
< 
<     # Loop over time
<     for i in range(T):
<         # Growing cube algorithm:
<         # Test if n = #(points in epsilon-environment of reference point i) > k
<         # Start with epsilon for which 95% of points are inside the cube
<         # for a multivariate Gaussian
<         # eps increased by 2 later, also the initial eps
<         eps = (k/T)**(1./dim)
< 
<         # n counts the number of neighbors
<         n = 0
<         while n <= k:
<             # Increase cube size
<             eps *= 2.
<             # Start with zero again
<             n = 0
<             # Loop through all points
<             for t in range(T):
<                 d = 0
<                 while d < dim and abs(array[d, i] - array[d, t]) < eps:
<                     d += 1
< 
<                 # If all distances are within eps, the point t lies
<                 # within eps and n is incremented
<                 if d == dim:
<                     indexfound[n] = t
<                     n += 1
< 
<         # Calculate distance to points only within epsilon environment
<         # according to maximum metric
<         for j in range(n):
<             index = indexfound[j]
< 
<             # calculate maximum metric distance to point
<             dxyz = 0.
<             for d in range(dim):
<                 dist[d, j] = abs(array[d, i] - array[d, index])
<                 dxyz = max(dist[d, j], dxyz)
< 
<             # insertion-sort current distance into 'dxyzarray'
<             # if it is among the currently smallest k+1 distances
<             if j == 0:
<                 dxyzarray[j] = dxyz
<             else:
<                 m = min(k, <int> (j-1))
<                 # go through previously sorted smallest distances and
<                 # if it is smaller than any, find slot for current distance
<                 while m >= 0 and dxyz < dxyzarray[m]:
<                     # if it's not in the last slot already,
<                     # move previously found distance to the right
<                     if not m == k:
<                         dxyzarray[m+1] = dxyzarray[m]
<                     m -= 1
< 
<                 # sort in, if a slot was found
<                 if not m == k:
<                     dxyzarray[m+1] = dxyz
< 
<         # Epsilon of k-th nearest neighbor in joint space
<         epsmax = dxyzarray[k]
< 
<         # Count neighbors within epsmax in subspaces, since the reference
<         # point is included, all neighbors are at least 1
<         kz = 0
<         kxz = 0
<         kyz = 0
<         for j in range(T):
< 
<             # X-subspace
<             dx = abs(array[0, i] - array[0, j])
<             for d in range(1, dim_x):
<                 dist[d, j] = abs(array[d, i] - array[d, j])
<                 dx = max(dist[d, j], dx)
< 
<             # Y-subspace
<             dy = abs(array[dim_x, i] - array[dim_x, j])
<             for d in range(dim_x, dim_x + dim_y):
<                 dist[d, j] = abs(array[d, i] - array[d, j])
<                 dy = max(dist[d, j], dy)
< 
<             # Z-subspace, if empty, dz stays 0
<             dz = 0.
<             for d in range(dim_x + dim_y, dim):
<                 dist[d, j] = abs(array[d, i] - array[d, j])
<                 dz = max(dist[d, j], dz)
< 
<             # For no conditions, kz is counted up to T
<             if dz < epsmax:
<                 kz += 1
<                 if dx < epsmax:
<                     kxz += 1
<                 if dy < epsmax:
<                     kyz += 1
< 
<         # Write to numpy arrays
<         k_xz[i] = kxz
<         k_yz[i] = kyz
<         k_z[i] = kz
---
>         ndarray[INT32TYPE_t, ndim=1, mode='c'] k_xz = np.zeros(
>             (T), dtype=INT32TYPE)
>         ndarray[INT32TYPE_t, ndim=1, mode='c'] k_yz = np.zeros(
>             (T), dtype=INT32TYPE)
>         ndarray[INT32TYPE_t, ndim=1, mode='c'] k_z = np.zeros(
>             (T), dtype=INT32TYPE)
> 
>     _get_nearest_neighbors_fast(
>         <FIELD_t*> cnp.PyArray_DATA(array), T, dim_x, dim_y, k, dim,
>         <int*> cnp.PyArray_DATA(k_xz), <int*> cnp.PyArray_DATA(k_yz),
>         <int*> cnp.PyArray_DATA(k_z))
Only in lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/funcnet/_ext: src_numerics.c
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/funcnet/__init__.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/funcnet/__init__.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
18a22
> 
20a25,32
> 
> To do
> ~~~~~
>   - ...
> 
> Known Bugs
> ~~~~~~~~~~
>   - ...
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/__init__.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/__init__.py
2,3c2,3
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
18a19
> 
28a30,35
> 
> To Do
> -----
>    - A lot - See current product backlog.
>    - Clean up MapPlots class -> Alex!?
> 
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/timeseries/cross_recurrence_plot.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/timeseries/cross_recurrence_plot.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
20a24
> # array object and fast numerics
23,24d26
< from ..core.cache import Cached
< from .recurrence_plot import RecurrencePlot
28a31,36
> from .recurrence_plot import RecurrencePlot
> 
> #
> #  Class definitions
> #
> 
30a39
> 
105,106c114,115
<             sparse_rqa=sparse_rqa, silence_level=silence_level,
<             skip_recurrence=True)
---
>             sparse_rqa=sparse_rqa, threshold=threshold,
>             recurrence_rate=recurrence_rate, silence_level=silence_level)
134,135c143
<         self._mut_embedding: int = 0
<         if (dim is not None) and (tau is not None):
---
>         if dim is not None and tau is not None:
137a146
>             """The embedded time series x."""
138a148
>             """The embedded time series y."""
160,187c170,173
<         return ("CrossRecurrencePlot: "
<                 f"time series shapes {self.x.shape}, {self.y.shape}.\n"
<                 f"Embedding dimension {self.dim if self.dim else 0}\n"
<                 f"Threshold {self.threshold}, {self.metric} metric")
< 
<     @property
<     def x_embedded(self) -> np.ndarray:
<         """
<         The embedded time series x.
<         """
<         return self._x_embedded
< 
<     @x_embedded.setter
<     def x_embedded(self, embedding: np.ndarray):
<         self._x_embedded = to_cy(embedding, DFIELD)
<         self._mut_embedding += 1
< 
<     @property
<     def y_embedded(self) -> np.ndarray:
<         """
<         The embedded time series y.
<         """
<         return self._y_embedded
< 
<     @y_embedded.setter
<     def y_embedded(self, embedding: np.ndarray):
<         self._y_embedded = to_cy(embedding, DFIELD)
<         self._mut_embedding += 1
---
>         return ('CrossRecurrencePlot: time series shapes %s, %s.\n'
>                 'Embedding dimension %i\nThreshold %s, %s metric') % (
>                     self.x.shape, self.y.shape, self.dim if self.dim else 0,
>                     self.threshold, self.metric)
202c188
<     def distance_matrix(self, metric):
---
>     def distance_matrix(self, x_embedded, y_embedded, metric):
206a193,196
>         :type x_embedded: 2D array (time, embedding dimension)
>         :arg x_embedded: The phase space trajectory x.
>         :type y_embedded: 2D array (time, embedding dimension)
>         :arg y_embedded: The phase space trajectory y.
212,213c202,210
<         assert metric in self._known_metrics, f"unknown metric: {metric}"
<         return getattr(self, f"{metric}_distance_matrix")()
---
>         #  Return distance matrix according to chosen metric:
>         if metric == "manhattan":
>             return self.manhattan_distance_matrix(x_embedded, y_embedded)
>         elif metric == "euclidean":
>             return self.euclidean_distance_matrix(x_embedded, y_embedded)
>         elif metric == "supremum":
>             return self.supremum_distance_matrix(x_embedded, y_embedded)
>         else:
>             return None
219,220c216
<     @Cached.method(name="the manhattan distance matrix")
<     def manhattan_distance_matrix(self):
---
>     def manhattan_distance_matrix(self, x_embedded, y_embedded):
231,233c227,235
<         ntime_x = self.x_embedded.shape[0]
<         ntime_y = self.y_embedded.shape[0]
<         dim = self.x_embedded.shape[1]
---
>         if self.silence_level <= 1:
>             print("Calculating the manhattan distance matrix...")
> 
>         x_embedded = to_cy(x_embedded, DFIELD)
>         y_embedded = to_cy(y_embedded, DFIELD)
>         ntime_x = x_embedded.shape[0]
>         ntime_y = y_embedded.shape[0]
>         dim = x_embedded.shape[1]
> 
235c237
<                                               self.x_embedded, self.y_embedded)
---
>                                               x_embedded, y_embedded)
237,238c239
<     @Cached.method(name="the euclidean distance matrix")
<     def euclidean_distance_matrix(self):
---
>     def euclidean_distance_matrix(self, x_embedded, y_embedded):
241a243,246
>         :type x_embedded: 2D Numpy array (time, embedding dimension)
>         :arg x_embedded: The phase space trajectory x.
>         :type y_embedded: 2D Numpy array (time, embedding dimension)
>         :arg y_embedded: The phase space trajectory y.
245,247c250,258
<         ntime_x = self.x_embedded.shape[0]
<         ntime_y = self.y_embedded.shape[0]
<         dim = self.x_embedded.shape[1]
---
>         if self.silence_level <= 1:
>             print("Calculating the euclidean distance matrix...")
> 
>         x_embedded = to_cy(x_embedded, DFIELD)
>         y_embedded = to_cy(y_embedded, DFIELD)
>         ntime_x = x_embedded.shape[0]
>         ntime_y = y_embedded.shape[0]
>         dim = x_embedded.shape[1]
> 
249c260
<                                               self.x_embedded, self.y_embedded)
---
>                                               x_embedded, y_embedded)
251,252c262
<     @Cached.method(name="the supremum distance matrix")
<     def supremum_distance_matrix(self):
---
>     def supremum_distance_matrix(self, x_embedded, y_embedded):
255a266,269
>         :type x_embedded: 2D Numpy array (time, embedding dimension)
>         :arg x_embedded: The phase space trajectory x.
>         :type y_embedded: 2D Numpy array (time, embedding dimension)
>         :arg y_embedded: The phase space trajectory y.
259,261c273,281
<         ntime_x = self.x_embedded.shape[0]
<         ntime_y = self.y_embedded.shape[0]
<         dim = self.x_embedded.shape[1]
---
>         if self.silence_level <= 1:
>             print("Calculating the supremum distance matrix...")
> 
>         x_embedded = to_cy(x_embedded, DFIELD)
>         y_embedded = to_cy(y_embedded, DFIELD)
>         ntime_x = x_embedded.shape[0]
>         ntime_y = y_embedded.shape[0]
>         dim = x_embedded.shape[1]
> 
263c283
<                                              self.x_embedded, self.y_embedded)
---
>                                              x_embedded, y_embedded)
277c297,300
<         distance = self.distance_matrix(self.metric)
---
>         #  Get distance matrix, according to self.metric
>         distance = self.distance_matrix(self.x_embedded, self.y_embedded,
>                                         self.metric)
>         #  Get length of time series x and y
278a302,303
> 
>         #  Initialize recurrence matrix
279a305,306
> 
>         #  Thresholding the distance matrix
280a308,311
> 
>         #  Clean up
>         del distance
> 
298c329,333
<         distance = self.distance_matrix(self.metric)
---
>         #  Get distance matrix, according to self.metric
>         distance = self.distance_matrix(self.x_embedded, self.y_embedded,
>                                         self.metric)
> 
>         #  Get length of time series x and y
299a335,336
> 
>         #  Get threshold to obtain fixed recurrence rate
301a339,340
> 
>         #  Initialize recurrence matrix
302a342,343
> 
>         #  Thresholding the distance matrix
303a345,348
> 
>         #  Clean up
>         del distance
> 
355,372d399
< 
<     def diagline_dist(self):
<         """Not implemented yet"""
<         raise NotImplementedError(
<             "Line distributions are not yet "
<             "available for cross-recurrence plots")
< 
<     def vertline_dist(self):
<         """Not implemented yet"""
<         raise NotImplementedError(
<             "Line distributions are not yet "
<             "available for cross-recurrence plots")
< 
<     def white_vertline_dist(self):
<         """Not implemented yet"""
<         raise NotImplementedError(
<             "Line distributions are not yet "
<             "available for cross-recurrence plots")
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/timeseries/_ext/__init__.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/timeseries/_ext/__init__.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/timeseries/_ext/numerics.pyx lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/timeseries/_ext/numerics.pyx
0a1,2
> # -*- coding: utf-8 -*-
> #
2,3c4,5
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
15a18
> from libc.stdlib cimport rand, RAND_MAX
23a27
> randint = rd.randint
25c29
< from ...core._ext.types import MASK, NODE, LAG, FIELD, DFIELD
---
> from ...core._ext.types import MASK, NODE, FIELD, DFIELD
96c100
<             diff = 0
---
>             temp_diff = diff = 0
341c345
< # recurrence plot =============================================================
---
> # recurrence plot==============================================================
512a517,647
> def _diagline_dist_norqa_missingvalues(
>     int n_time, ndarray[NODE_t, ndim=1] diagline,
>     ndarray[LAG_t, ndim=2] recmat,
>     ndarray[MASK_t, ndim=1, cast=True] mv_indices):
> 
>     cdef:
>         int i, j, k = 0
>         MASK_t missing_flag = False
> 
>     for i in range(n_time):
>         if k != 0 and not missing_flag:
>             diagline[k] += 1
>             k = 0
> 
>         missing_flag = False
> 
>         for j in range(i+1):
>             # Check if curren tpoint in RP belongs to a mising value
>             if mv_indices[n_time-1-i+j] or mv_indices[j]:
>                 missing_flag = True
>                 k = 0
>             elif recmat[n_time-1-i+j, j] == 0 and missing_flag:
>                 missing_flag = False
> 
>             if not missing_flag:
>                 # Only incease k if some previous point in diagonal was not a
>                 # missing value!
>                 if recmat[n_time-1-i+j, j] == 1:
>                     k += 1
>                 # Only count diagonal lines that are not followed by a missing
>                 # point in the recurrence plot
>                 elif k != 0:
>                     diagline[k] += 1
>                     k = 0
> 
> 
> def _diagline_dist_norqa(
>     int n_time, ndarray[NODE_t, ndim=1] diagline,
>     ndarray[LAG_t, ndim=2] recmat):
> 
>     cdef:
>         int i, j, k = 0
> 
>     for i in range(n_time):
>         if k != 0:
>             diagline[k] += 1
>             k = 0
>         for j in range(i+1):
>             if recmat[n_time-1-i+j, j] == 1:
>                 k += 1
>             elif k != 0:
>                 diagline[k] += 1
>                 k = 0
> 
> 
> def _diagline_dist_rqa_missingvalues(
>     int n_time, ndarray[NODE_t, ndim=1] diagline,
>     ndarray[MASK_t, ndim=1, cast=True] mv_indices,
>     ndarray[FIELD_t, ndim=2] embedding, float eps, int dim):
> 
>     cdef:
>         int i, j, k = 0, l
>         FIELD_t temp_diff, diff
>         MASK_t missing_flag = False
> 
>     for i in range(n_time):
>         if k != 0 and not missing_flag:
>             diagline[k] += 1
>             k = 0
> 
>         missing_flag = False
> 
>         for j in range(i+1):
>             # Compute supreumum distance between state vectors
>             diff = 0
>             for l in range(dim):
>                 # Use supremum norm
>                 temp_diff = abs(embedding[j, l] - embedding[n_time-1-i+j, l])
>                 if temp_diff > diff:
>                     diff = temp_diff
> 
>             # Check if curren tpoint in RP belongs to a missing value
>             if mv_indices[n_time-1-i+j] or mv_indices[j]:
>                 missing_flag = True
>                 k = 0
>             elif diff > eps and missing_flag:
>                 missing_flag = False
> 
>             if not missing_flag:
>                 # Only increase k if some previous point in diagonal was not a
>                 # missig value!
>                 if diff < eps:
>                     k += 1
> 
>                 # Only count diagonal lines that are not followed by a missing
>                 # value point in the recurrenc plot
>                 elif k != 0:
>                     diagline[k] += 1
>                     k = 0
> 
> 
> def _diagline_dist_rqa(
>     int n_time, ndarray[NODE_t, ndim=1] diagline,
>     ndarray[FIELD_t, ndim=2] embedding, float eps, int dim):
> 
>     cdef:
>         int i, j, k = 0, l
>         FIELD_t temp_diff, diff
> 
>     for i in range(n_time):
>         if k != 0:
>             diagline[k] += 1
>             k = 0
> 
>         for j in range(i+1):
>             # Compute supremum distance between state vectors
>             diff = 0
>             for l in range(dim):
>                 # Use supremum norm
>                 temp_diff = abs(embedding[j, l] - embedding[n_time-1-i+j, l])
>                 if temp_diff > diff:
>                     diff = temp_diff
> 
>             # check if R(j, n_time-q-i+j) == 1 -> recurrence
>             if diff < eps:
>                 k += 1
>             elif k != 0:
>                 diagline[k] += 1
>                 k = 0
> 
> 
515c650
<     ndarray[NODE_t, ndim=1] resampled_dist, int N, int M):
---
>     ndarray[DFIELD_t, ndim=1] resampled_dist, int N, int M):
524c659
<             resampled_dist[x] += 1
---
>             resampled_dist[x] += <FIELD_t> 1
527a663,806
> def _vertline_dist_norqa_missingvalues(
>     int n_time, ndarray[NODE_t, ndim=1] vertline,
>     ndarray[LAG_t, ndim=2] recmat,
>     ndarray[MASK_t, ndim=1, cast=True] mv_indices):
> 
>     cdef:
>         int i, j, k = 0
>         MASK_t missing_flag = False
> 
>     for i in range(n_time):
>         if (k != 0 and not missing_flag):
>             vertline[k] += 1
>             k = 0
> 
>         missing_flag = False
> 
>         for j in range(n_time):
>             # check if current point in RP belongs to a missing value
>             if mv_indices[i] or mv_indices[j]:
>                 missing_flag = True
>                 k = 0
>             elif recmat[i, j] == 0 and missing_flag:
>                 missing_flag = False
> 
>             if not missing_flag:
>                 if recmat[i, j] != 0:
>                     k += 1
>                 elif k != 0:
>                     vertline[k] += 1
>                     k = 0
> 
> 
> def _vertline_dist_norqa(
>     int n_time, ndarray[NODE_t, ndim=1] vertline,
>     ndarray[LAG_t, ndim=2] recmat):
> 
>     cdef int i, j, k = 0
> 
>     for i in range(n_time):
>         if k != 0:
>             vertline[k] += 1
>             k = 0
> 
>         for j in range(n_time):
>             if recmat[i, j] != 0:
>                 k += 1
>             elif k != 0:
>                 vertline[k] += 1
>                 k = 0
> 
> 
> def _vertline_dist_rqa_missingvalues(
>     int n_time, ndarray[NODE_t, ndim=1] vertline,
>     ndarray[MASK_t, ndim=1, cast=True] mv_indices,
>     ndarray[FIELD_t, ndim=2] embedding, float eps, int dim):
> 
>     cdef:
>         int i, j, k = 0, l
>         FIELD_t temp_diff, diff
>         MASK_t missing_flag = False
> 
>     for i in range(n_time):
>         if k != 0 and not missing_flag:
>             vertline[k] += 1
>             k = 0
> 
>         missing_flag = False
> 
>         for j in range(n_time):
>             # Compute supremum distance between state vectors
>             diff = 0
>             for l in range(dim):
>                 # Use supremum norm
>                 temp_diff = abs(embedding[i, l] - embedding[j, l])
> 
>                 if temp_diff > diff:
>                     diff = temp_diff
> 
>             # Check if current point in RP belongs to a missing values
>             if mv_indices[i] or mv_indices[j]:
>                 missing_flag = True
>                 k = 0
>             elif diff > eps and missing_flag:
>                 missing_flag = True
> 
>             if not missing_flag:
>                 # Check if recurrent point has been reached
>                 if diff < eps:
>                     k += 1
>                 elif k != 0:
>                     vertline[k] += 1
>                     k = 0
> 
> 
> def _vertline_dist_rqa(
>     int n_time, ndarray[NODE_t, ndim=1] vertline,
>     ndarray[FIELD_t, ndim=2] embedding, float eps, int dim):
> 
>     cdef:
>         int i, j, k = 0, l
>         FIELD_t temp_diff, diff
> 
>     for i in range(n_time):
>         if k != 0:
>             vertline[k] += 1
>             k = 0
> 
>         for j in range(n_time):
>             # Compute supremum distance between state vectors
>             diff = 0
>             for l in range(dim):
>                 # Use supremum norm
>                 temp_diff = abs(embedding[i, l] - embedding[j, l])
> 
>                 if temp_diff > diff:
>                     diff = temp_diff
> 
>             # Check if recurrent point has been reached
>             if diff < eps:
>                 k += 1
>             elif k != 0:
>                 vertline[k] += 1
>                 k = 0
> 
> 
> def _white_vertline_dist(
>     int n_time, ndarray[NODE_t, ndim=1] white_vertline,
>     ndarray[LAG_t, ndim=2] R):
> 
>     cdef int i, j, k = 0
> 
>     for i in range(n_time):
>         if k != 0:
>             white_vertline[k] += 1
>             k = 0
> 
>         for j in range(n_time):
>             if R[i, j] == 0:
>                 k += 1
>             elif k != 0:
>                 white_vertline[k] += 1
>                 k = 0
> 
> 
618,793d896
< 
< 
< 
< # recurrence line distributions ===============================================
< 
< 
< # parameters for `_line_dist()`
< ctypedef int     (*line_type_i2J) (int, int)
< ctypedef int     (*line_type_ij2I)(int, int, int)
< ctypedef DFIELD_t (*metric_type)(int, int, int, DFIELD_t[:,:])
< 
< cdef:
<     inline int i2J_vertline(int i, int N): return N
<     inline int i2J_diagline(int i, int N): return i+1
<     inline int ij2I_vertline(int i, int j, int N): return i
<     inline int ij2I_diagline(int i, int j, int N): return N - i + j
<     metric_type metric_null = NULL
<     inline DFIELD_t metric_supremum(int I, int j, int dim, DFIELD_t[:,:] E):
<         cdef:
<             int l
<             DFIELD_t diff = 0, tmp_diff
<         for l in range(dim):
<             tmp_diff = abs(E[I, l] - E[j, l])
<             if tmp_diff > diff:
<                 diff = tmp_diff
<         return diff
< 
< 
< cdef void _line_dist(
<     int n_time, ndarray[NODE_t, ndim=1] hist,
<     ndarray[LAG_t, ndim=2] R, ndarray[DFIELD_t, ndim=2] E, float eps, int dim,
<     metric_type metric, bint black,
<     ndarray[MASK_t, ndim=1, cast=True] M, bint missing_values,
<     line_type_i2J i2J, line_type_ij2I ij2I, bint skip_main):
<     """
<     Recurrence line distributions, parametrised by the following arguments:
< 
<       - `R` | `E (dim > 0)`: recurrence computation (cached vs. raw embedding)
<       - `metric`: embedding metric (currently only supremum)
<       - `black`: RQA colour (black vs. white)
<       - `M (missing_values == 1)`: missing input values (ignore vs. account)
<       - `line_type_*`, `skip_main`: line type (vertical vs. diagonal)
<     """
< 
<     cdef:
<         int i, I, j, k = 0, N = n_time
<         FIELD_t d
<         bint line, missing_flag = False
< 
<     if skip_main:
<         # exclude main diagonal by skipping last outer loop iteration
<         N -= 1
<     for i in range(N):
<         for j in range(i2J(i, N)):
<             I = ij2I(i, j, N)
< 
<             if dim == 0:
<                 line = R[I, j] == black
<             else:
<                 # compute distance between embedding vectors
<                 d = metric(I, j, dim, E)
<                 line = (d < eps) == black
< 
<             if missing_values:
<                 # check if current point in RP is a missing value
<                 if M[I] or M[j]:
<                     missing_flag = True
<                     k = 0
<                 # or if previous point was one, reset flag if not within line
<                 elif missing_flag and not line:
<                     missing_flag = False
< 
<                 # only count line if it does not contain,
<                 # directly follow or is followed by a missing value
<                 if missing_flag:
<                     continue
< 
<             if line:
<                 # if within line, increment length
<                 k += 1
<             elif k != 0:
<                 # if end of line, count line and reset length
<                 hist[k-1] += 1
<                 k = 0
< 
<         if k != 0 and not missing_flag:
<             # at end of subspace, count the last uncounted line and reset length
<             hist[k-1] += 1
<             k = 0
<         missing_flag = False
< 
< 
< def _vertline_dist(
<         int n_time, ndarray[NODE_t, ndim=1] hist, ndarray[LAG_t, ndim=2] R):
<     cdef:
<         ndarray[DFIELD_t, ndim=2] E_null = np.array([[]], dtype=DFIELD)
<         ndarray[MASK_t, ndim=1] M_null = np.array([], dtype=MASK)
<     _line_dist(
<         n_time, hist, R, E_null, 0, 0, metric_null, True, M_null, False,
<         i2J_vertline, ij2I_vertline, False)
< 
< def _diagline_dist(
<         int n_time, ndarray[NODE_t, ndim=1] hist, ndarray[LAG_t, ndim=2] R):
<     cdef:
<         ndarray[DFIELD_t, ndim=2] E_null = np.array([[]], dtype=DFIELD)
<         ndarray[MASK_t, ndim=1] M_null = np.array([], dtype=MASK)
<     _line_dist(
<         n_time, hist, R, E_null, 0, 0, metric_null, True, M_null, False,
<         i2J_diagline, ij2I_diagline, True)
< 
< def _white_vertline_dist(
<         int n_time, ndarray[NODE_t, ndim=1] hist, ndarray[LAG_t, ndim=2] R):
<     cdef:
<         ndarray[DFIELD_t, ndim=2] E_null = np.array([[]], dtype=DFIELD)
<         ndarray[MASK_t, ndim=1] M_null = np.array([], dtype=MASK)
<     _line_dist(
<         n_time, hist, R, E_null, 0, 0, metric_null, False, M_null, False,
<         i2J_vertline, ij2I_vertline, False)
< 
< def _vertline_dist_sequential(
<         int n_time, ndarray[NODE_t, ndim=1] hist,
<         ndarray[DFIELD_t, ndim=2] E, float eps, int dim):
<     cdef:
<         ndarray[LAG_t, ndim=2] null_R = np.array([[]], dtype=LAG)
<         ndarray[MASK_t, ndim=1] M_null = np.array([], dtype=MASK)
<     _line_dist(
<         n_time, hist, null_R, E, eps, dim, metric_supremum, True, M_null, False,
<         i2J_vertline, ij2I_vertline, False)
< 
< def _diagline_dist_sequential(
<         int n_time, ndarray[NODE_t, ndim=1] hist,
<         ndarray[DFIELD_t, ndim=2] E, float eps, int dim):
<     cdef:
<         ndarray[LAG_t, ndim=2] null_R = np.array([[]], dtype=LAG)
<         ndarray[MASK_t, ndim=1] M_null = np.array([], dtype=MASK)
<     _line_dist(
<         n_time, hist, null_R, E, eps, dim, metric_supremum, True, M_null, False,
<         i2J_diagline, ij2I_diagline, True)
< 
< def _vertline_dist_missingvalues(
<         int n_time, ndarray[NODE_t, ndim=1] hist, ndarray[LAG_t, ndim=2] R,
<         ndarray[MASK_t, ndim=1, cast=True] M):
<     cdef:
<         ndarray[DFIELD_t, ndim=2] E_null = np.array([[]], dtype=DFIELD)
<     _line_dist(
<         n_time, hist, R, E_null, 0, 0, metric_null, True, M, True,
<         i2J_vertline, ij2I_vertline, False)
< 
< def _diagline_dist_missingvalues(
<         int n_time, ndarray[NODE_t, ndim=1] hist, ndarray[LAG_t, ndim=2] R,
<         ndarray[MASK_t, ndim=1, cast=True] M):
<     cdef:
<         ndarray[DFIELD_t, ndim=2] E_null = np.array([[]], dtype=DFIELD)
<     _line_dist(
<         n_time, hist, R, E_null, 0, 0, metric_null, True, M, True,
<         i2J_diagline, ij2I_diagline, True)
< 
< def _vertline_dist_sequential_missingvalues(
<         int n_time, ndarray[NODE_t, ndim=1] hist,
<         ndarray[MASK_t, ndim=1, cast=True] M,
<         ndarray[DFIELD_t, ndim=2] E, float eps, int dim):
<     cdef:
<         ndarray[LAG_t, ndim=2] null_R = np.array([[]], dtype=LAG)
<     _line_dist(
<         n_time, hist, null_R, E, eps, dim, metric_supremum, True, M, True,
<         i2J_vertline, ij2I_vertline, False)
< 
< def _diagline_dist_sequential_missingvalues(
<         int n_time, ndarray[NODE_t, ndim=1] hist,
<         ndarray[MASK_t, ndim=1, cast=True] M,
<         ndarray[DFIELD_t, ndim=2] E, float eps, int dim):
<     cdef:
<         ndarray[LAG_t, ndim=2] null_R = np.array([[]], dtype=LAG)
<     _line_dist(
<         n_time, hist, null_R, E, eps, dim, metric_supremum, True, M, True,
<         i2J_diagline, ij2I_diagline, True)
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/timeseries/_ext/src_numerics.c lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/timeseries/_ext/src_numerics.c
6c6
< * Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
---
> * Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/timeseries/__init__.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/timeseries/__init__.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/timeseries/inter_system_recurrence_network.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/timeseries/inter_system_recurrence_network.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
21,23d23
< from typing import Tuple
< from collections.abc import Hashable
< 
207,211c207,211
<         return ("InterSystemRecurrenceNetwork: "
<                 f"time series shapes {self.x.shape}, {self.y.shape}.\n"
<                 f"Embedding dimension {self.dim if self.dim else 0}\n"
<                 f"Threshold {self.threshold}, {self.metric} metric.\n"
<                 f"{InteractingNetworks.__str__(self)}")
---
>         return ('InterSystemRecurrenceNetwork: time series shapes %s, %s.\n'
>                 'Embedding dimension %i\nThreshold %s, %s metric.\n%s') % (
>                     self.x.shape, self.y.shape, self.dim if self.dim else 0,
>                     self.threshold, self.metric,
>                     InteractingNetworks.__str__(self))
217,218c217,232
<     def __cache_state__(self) -> Tuple[Hashable, ...]:
<         return (self.rp_x, self.rp_x, self.crp_xy,)
---
>     def clear_cache(self):
>         """
>         Clean up memory by deleting information that can be recalculated from
>         basic data.
> 
>         Extends the clean up methods of the parent classes.
>         """
>         #  Call clean up of RecurrencePlot objects
>         self.rp_x.clear_cache()
>         self.rp_y.clear_cache()
> 
>         #  Call clean up of CrossRecurrencePlot object
>         self.crp_xy.clear_cache()
> 
>         #  Call clean up of InteractingNetworks
>         InteractingNetworks.clear_cache(self)
233a248
>         N_y = self.N_y
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/timeseries/joint_recurrence_network.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/timeseries/joint_recurrence_network.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
157,159c160,173
<         return ("JointRecurrenceNetwork:\n"
<                 f"{JointRecurrencePlot.__str__(self)}\n"
<                 f"{Network.__str__(self)}")
---
>         return 'JointRecurrenceNetwork:\n%s\n%s' % (
>             JointRecurrencePlot.__str__(self), Network.__str__(self))
> 
>     def clear_cache(self):
>         """
>         Clean up memory by deleting information that can be recalculated from
>         basic data.
> 
>         Extends the clean up methods of the parent classes.
>         """
>         #  Call clean up of RecurrencePlot
>         JointRecurrencePlot.clear_cache(self)
>         #  Call clean up of Network
>         Network.clear_cache(self)
208c222
<     def set_fixed_recurrence_rate(self, recurrence_rate):
---
>     def set_fixed_recurrence_rate(self, density):
213,215c227,229
<         :type recurrence_rate: tuple of number
<         :arg recurrence_rate: The link density / recurrence rate.
<             Give for each time series separately.
---
>         :type density: tuple of number
>         :arg density: The link density / recurrence rate. Give for each time
>             series separately.
218c232
<         JointRecurrencePlot.set_fixed_recurrence_rate(self, recurrence_rate)
---
>         JointRecurrencePlot.set_fixed_recurrence_rate(self, density)
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/timeseries/joint_recurrence_plot.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/timeseries/joint_recurrence_plot.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
20a24,28
> #
> #  Import essential packages
> #
> 
> # array object and fast numerics
25a34,37
> #
> #  Class definitions
> #
> 
121,124d132
<         assert len(metric) == 2
<         assert all(m in ("manhattan", "euclidean", "supremum") for m in metric)
< 
<         # initialise base class using dummy values (bad OO design)
130,132c138
<         #  Store type of metric
<         self.metric = metric
< 
---
>         self._distance_matrix_cached = False
186,187c192,193
<                 raise ValueError("Delay value (lag) must not exceed length of "
<                                  "time series!")
---
>                 raise ValueError("Delay value (lag) must not exceed length of \
>                                  time series!")
203,205c209,211
<                 raise NameError("Please give either threshold or "
<                                 "recurrence_rate to construct the joint "
<                                 "recurrence plot!")
---
>                 raise NameError("Please give either threshold or \
>                                 recurrence_rate to construct the joint \
>                                 recurrence plot!")
211,212c217,218
<             raise ValueError("Both time series x and y need to have the same "
<                              "length!")
---
>             raise ValueError("Both time series x and y need to have the same \
>                              length!")
218,221c224,227
<         return ("JointRecurrencePlot: "
<                 f"time series shapes {self.x.shape}.\n"
<                 f"Embedding dimension {self.dim if self.dim else 0}\n"
<                 f"Threshold {self.threshold}, {self.metric} metric")
---
>         return ('JointRecurrencePlot: time series shapes %s.\n'
>                 'Embedding dimension %i\nThreshold %s, %s metric') % (
>                     self.x.shape, self.dim if self.dim else 0,
>                     self.threshold, self.metric)
254,255c260,265
<         self.embedding = self.x_embedded
<         distance = self.distance_matrix(self.metric[0])
---
>         #  Disable caching of distances in RecurrencePlot class
>         self._distance_matrix_cached = False
>         #  Get distance matrix for the first time series
>         distance = self.distance_matrix(self.x_embedded, self.metric[0])
> 
>         #  Get length of time series
256a267,268
> 
>         #  Initialize first recurrence matrix
257a270,271
> 
>         #  Thresholding the first distance matrix
258a273,274
> 
>         #  Clean up
261,262c277,282
<         self.embedding = self.y_embedded
<         distance = self.distance_matrix(self.metric[1])
---
>         #  Disable caching of distances in RecurrencePlot class
>         self._distance_matrix_cached = False
>         #  Get distance matrix for the second time series
>         distance = self.distance_matrix(self.y_embedded, self.metric[1])
> 
>         #  Initialize second recurrence matrix
263a284,285
> 
>         #  Thresholding the second distance matrix
265d286
<         del distance
276a298,300
>         #  Clean up
>         del distance, recurrence_x, recurrence_y
> 
292a317
>         #  Get absolute threshold
294a320,321
> 
>         #  Call set fixed threshold method
313,314c340,345
<         self.embedding = self.x_embedded
<         distance = self.distance_matrix(self.metric[0])
---
>         #  Disable caching of distances in RecurrencePlot class
>         self._distance_matrix_cached = False
>         #  Get distance matrix for the first time series
>         distance = self.distance_matrix(self.x_embedded, self.metric[0])
> 
>         #  Get length of time series
315a347,348
> 
>         #  Get first threshold to obtain fixed recurrence rate
317a351,352
> 
>         #  Initialize recurrence matrix
318a354,355
> 
>         #  Thresholding the distance matrix
319a357,358
> 
>         #  Clean up
322,323c361,366
<         self.embedding = self.y_embedded
<         distance = self.distance_matrix(self.metric[1])
---
>         #  Disable caching of distances in RecurrencePlot class
>         self._distance_matrix_cached = False
>         #  Get distance matrix for the second time series
>         distance = self.distance_matrix(self.y_embedded, self.metric[1])
> 
>         #  Get first threshold to obtain fixed recurrence rate
325a369,370
> 
>         #  Initialize recurrence matrix
326a372,373
> 
>         #  Thresholding the distance matrix
328d374
<         del distance
338a385,387
> 
>         #  Clean up
>         del distance, recurrence_x, recurrence_y
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/timeseries/recurrence_network.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/timeseries/recurrence_network.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
151,153c154,167
<         return (f"RecurrenceNetwork:\n"
<                 f"{RecurrencePlot.__str__(self)}\n"
<                 f"{Network.__str__(self)}")
---
>         return 'RecurrenceNetwork:\n%s\n%s' % (
>             RecurrencePlot.__str__(self), Network.__str__(self))
> 
>     def clear_cache(self):
>         """
>         Clean up memory by deleting information that can be recalculated from
>         basic data.
> 
>         Extends the clean up methods of the parent classes.
>         """
>         #  Call clean up of RecurrencePlot
>         RecurrencePlot.clear_cache(self)
>         #  Call clean up of Network
>         Network.clear_cache(self)
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/timeseries/recurrence_plot.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/timeseries/recurrence_plot.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
22,23d24
< from typing import Tuple
< from collections.abc import Hashable
24a26
> # array object and fast numerics
26d27
< from numpy.typing import NDArray
28c29
< from ..core.cache import Cached
---
> # Cython inline code
35,38c36,39
<     _diagline_dist_missingvalues, _diagline_dist, \
<     _diagline_dist_sequential_missingvalues, _diagline_dist_sequential, \
<     _vertline_dist_missingvalues, _vertline_dist, \
<     _vertline_dist_sequential_missingvalues, _vertline_dist_sequential, \
---
>     _diagline_dist_norqa_missingvalues, _diagline_dist_norqa, \
>     _diagline_dist_rqa_missingvalues, _diagline_dist_rqa, \
>     _vertline_dist_norqa_missingvalues, _vertline_dist_norqa, \
>     _vertline_dist_rqa_missingvalues, _vertline_dist_rqa, \
42c43,49
< class RecurrencePlot(Cached):
---
> #
> #  Class definitions
> #
> 
> 
> class RecurrencePlot:
> 
81,84c88,90
<     def __init__(self, time_series: NDArray, metric: str = "supremum",
<                  normalize: bool = False, missing_values: bool = False,
<                  sparse_rqa: bool = False, silence_level: int = 0,
<                  **kwargs):
---
>     def __init__(self, time_series, metric="supremum", normalize=False,
>                  missing_values=False, sparse_rqa=False, silence_level=0,
>                  **kwds):
111,112d116
<         :arg bool skip_recurrence: Skip calculation of recurrence matrix within
<             RP class (e.g. when overloading respective methods in child class)
151,152d154
<         self._known_metrics = ("manhattan", "euclidean", "supremum")
<         assert metric in self._known_metrics
163,170c165,167
<         #  Get embedding dimension and delay from **kwargs
<         self.dim = kwargs.get("dim")
<         self.tau = kwargs.get("tau")
< 
<         self.N: int = 0
<         """The number of state vectors (number of lines and rows) of the RP."""
<         self.R = None
<         """The recurrence matrix."""
---
>         #  Get embedding dimension and delay from **kwds
>         self.dim = kwds.get("dim")
>         self.tau = kwds.get("tau")
172,173c169
<         self._mut_embedding: int = 0
<         if (self.dim is not None) and (self.tau is not None):
---
>         if self.dim is not None and self.tau is not None:
175,176c171,173
<             self.embedding = self.embed_time_series(
<                 self.time_series, self.dim, self.tau)
---
>             self.embedding = self.embed_time_series(self.time_series, self.dim,
>                                                     self.tau)
>             """The embedded time series."""
179a177,181
>         self.N = self.embedding.shape[0]
>         """The number of state vectors (number of lines and rows) of the RP."""
>         self.R = None
>         """The recurrence matrix."""
> 
185c187
<         #  Get threshold or recurrence rate from **kwargs, construct recurrence
---
>         #  Get threshold or recurrence rate from **kwds, construct recurrence
187,188c189,190
<         self.threshold = kwargs.get("threshold")
<         self.threshold_std = kwargs.get("threshold_std")
---
>         self.threshold = kwds.get("threshold")
>         self.threshold_std = kwds.get("threshold_std")
190,191c192,193
<         recurrence_rate = kwargs.get("recurrence_rate")
<         self.local_recurrence_rate = kwargs.get("local_recurrence_rate")
---
>         recurrence_rate = kwds.get("recurrence_rate")
>         self.local_recurrence_rate = kwds.get("local_recurrence_rate")
193c195
<             kwargs.get("adaptive_neighborhood_size")
---
>             kwds.get("adaptive_neighborhood_size")
195,197c197,203
<         #  Precompute recurrence matrix only if sequential RQA is switched off,
<         #  and not calling from child class with respective overriding methods.
<         skip_recurrence = kwargs.get("skip_recurrence")
---
>         #  Initialize cache
>         self._distance_matrix_cached = False
>         self._distance_matrix = None
>         self._diagline_dist_cached = False
>         self._diagline_dist = None
>         self._vertline_dist_cached = False
>         self._vertline_dist = None
199c205,206
<         if not sparse_rqa and not skip_recurrence:
---
>         #  Precompute recurrence matrix only if sequential RQA is switched off.
>         if not sparse_rqa:
229,231d235
<     def __cache_state__(self) -> Tuple[Hashable, ...]:
<         return (self._mut_embedding,)
< 
236,253c240,256
<         return ("RecurrencePlot: "
<                 f"time series shape {self.time_series.shape}.\n"
<                 f"Embedding dimension {self.dim if self.dim else 0}\n"
<                 f"Threshold {self.threshold}, {self.metric} metric")
< 
<     @property
<     def embedding(self) -> np.ndarray:
<         """
<         The embedded time series / phase space trajectory
<         (time, embedding dimension).
<         """
<         return self._embedding
< 
<     @embedding.setter
<     def embedding(self, embedding: np.ndarray):
<         self._embedding = to_cy(embedding, DFIELD)
<         self.N = self._embedding.shape[0]
<         self._mut_embedding += 1
---
>         return ('RecurrencePlot: time series shape %s.\n'
>                 'Embedding dimension %i\nThreshold %s, %s metric') % (
>                     self.time_series.shape, self.dim if self.dim else 0,
>                     self.threshold, self.metric)
> 
>     def clear_cache(self, irreversible=False):
>         """Clean up memory."""
>         if irreversible:
>             if self._distance_matrix_cached:
>                 del self._distance_matrix
>                 self._distance_matrix_cached = False
>             if self._diagline_dist_cached:
>                 del self._diagline_dist
>                 self._diagline_dist_cached = False
>             if self._vertline_dist_cached:
>                 del self._vertline_dist
>                 self._vertline_dist_cached = False
273c276
<     def distance_matrix(self, metric: str):
---
>     def distance_matrix(self, embedding, metric):
277a281,282
>         :type embedding: 2D array (time, embedding dimension)
>         :arg embedding: The phase space trajectory.
283,284c288,303
<         assert metric in self._known_metrics, f"unknown metric: {metric}"
<         return getattr(RecurrencePlot, f"{metric}_distance_matrix")(self)
---
> 
>         if not self._distance_matrix_cached:
>             #  Return distance matrix according to chosen metric:
>             if metric == "manhattan":
>                 self._distance_matrix = \
>                     RecurrencePlot.manhattan_distance_matrix(self, embedding)
>             elif metric == "euclidean":
>                 self._distance_matrix = \
>                     RecurrencePlot.euclidean_distance_matrix(self, embedding)
>             elif metric == "supremum":
>                 self._distance_matrix = \
>                     RecurrencePlot.supremum_distance_matrix(self, embedding)
> 
>             self._distance_matrix_cached = True
> 
>         return self._distance_matrix
513,514c532
<     @Cached.method(name="the manhattan distance matrix")
<     def manhattan_distance_matrix(self):
---
>     def manhattan_distance_matrix(self, embedding):
518a537,538
>         :type embedding: 2D array (time, embedding dimension)
>         :arg embedding: The phase space trajectory.
522,523c542,547
<         (n_time, dim) = self.embedding.shape
<         return _manhattan_distance_matrix_rp(n_time, dim, self.embedding)
---
>         if self.silence_level <= 1:
>             print("Calculating the manhattan distance matrix...")
> 
>         (n_time, dim) = embedding.shape
>         return _manhattan_distance_matrix_rp(n_time, dim,
>                                              to_cy(embedding, DFIELD))
525,526c549
<     @Cached.method(name="the euclidean distance matrix")
<     def euclidean_distance_matrix(self):
---
>     def euclidean_distance_matrix(self, embedding):
530a554,555
>         :type embedding: 2D array (time, embedding dimension)
>         :arg embedding: The phase space trajectory.
534,535c559,564
<         (n_time, dim) = self.embedding.shape
<         return _euclidean_distance_matrix_rp(n_time, dim, self.embedding)
---
>         if self.silence_level <= 1:
>             print("Calculating the euclidean distance matrix...")
> 
>         (n_time, dim) = embedding.shape
>         return _euclidean_distance_matrix_rp(n_time, dim,
>                                              to_cy(embedding, DFIELD))
537,538c566
<     @Cached.method(name="the supremum distance matrix")
<     def supremum_distance_matrix(self):
---
>     def supremum_distance_matrix(self, embedding):
541a570,572
>         :type embedding: 2D Numpy array (time, embedding dimension)
>         :arg embedding: The phase space trajectory.
> 
545,546c576,581
<         (n_time, dim) = self.embedding.shape
<         return _supremum_distance_matrix_rp(n_time, dim, self.embedding)
---
>         if self.silence_level <= 1:
>             print("Calculating the supremum distance matrix...")
> 
>         (n_time, dim) = embedding.shape
>         return _supremum_distance_matrix_rp(n_time, dim,
>                                             to_cy(embedding, DFIELD))
560c595,599
<         distance = RecurrencePlot.distance_matrix(self, self.metric)
---
>         #  Get distance matrix, according to self.metric
>         distance = RecurrencePlot.distance_matrix(
>             self, self.embedding, self.metric)
> 
>         #  Get number of nodes
561a601,602
> 
>         #  Initialize recurrence matrix
562a604,605
> 
>         #  Thresholding the distance matrix
563a607,608
> 
>         #  Handle missing values
586a632
>         #  Get absolute threshold
587a634,635
> 
>         #  Call set fixed threshold method
602c650,653
<         distance = RecurrencePlot.distance_matrix(self, self.metric)
---
>         #  Get distance matrix, according to self.metric
>         distance = self.distance_matrix(self.embedding, self.metric)
> 
>         #  Get number of nodes
603a655,656
> 
>         #  Get threshold to obtain fixed recurrence rate
605a659,660
> 
>         #  Initialize recurrence matrix
606a662,663
> 
>         #  Thresholding the distance matrix
607a665
> 
627c685,688
<         distance = RecurrencePlot.distance_matrix(self, self.metric)
---
>         #  Get distance matrix, according to self.metric
>         distance = self.distance_matrix(self.embedding, self.metric)
> 
>         #  Get number of nodes
628a690,691
> 
>         #  Initialize recurrence matrix
629a693
> 
634a699
> 
636a702
> 
663c729,730
<         distance = RecurrencePlot.distance_matrix(self, self.metric)
---
>         #  Get distance matrix, according to self.metric
>         distance = self.distance_matrix(self.embedding, self.metric)
669a737
>         #  Get number of nodes
670a739,740
> 
>         #  Initialize recurrence matrix
684c754
<     def threshold_from_recurrence_rate(distance, recurrence_rate: float):
---
>     def threshold_from_recurrence_rate(distance, recurrence_rate):
704,706c774
<         assert 0 <= recurrence_rate <= 1
<         N = len(flat_distance)
<         threshold = flat_distance[int(recurrence_rate * (N - 1))]
---
>         threshold = flat_distance[int(recurrence_rate * len(flat_distance))]
812a881
>         #  Prepare
813a883
> 
816,822c886,890
<             RR = R.sum() / N ** 2
<         elif self.metric == "supremum":
<             RR = (self.vertline_dist() * np.arange(1, N + 1)).sum() / N ** 2
<         else:
<             raise NotImplementedError(
<                 "Sequential RQA is currently only available for "
<                 "fixed threshold and the supremum metric.")
---
>             RR = R.sum() / float(N ** 2)
>         elif self.sparse_rqa and self.metric == "supremum":
>             RR = (self.vertline_dist() * np.arange(N)).sum() / \
>                 float(N ** 2)
> 
844,845d911
<     @Cached.method(attrs=(
<         "metric", "threshold", "missing_values", "sparse_rqa"))
849,850c915
<         <triple: frequency distribution; diagonal; line length>`
<         :math:`P(l-1)`.
---
>         <triple: frequency distribution; diagonal; line length>` :math:`P(l)`.
852c917
<         Note that entry :math:`P(l-1)` contains the number of
---
>         The :math:`l` th entry of :math:`P(l)` contains the number of
854,857d918
<         Thus, :math:`P(0)` counts lines of length :math:`1`,
<         :math:`P(1)` counts lines of length :math:`2`, asf.
<         The main diagonal is not counted,
<         hence :math:`P(N)` will always be :math:`0`.
866c927
<             :math:`P(l-1)`.
---
>             :math:`P(l)`.
868,899c929,930
<         #  Prepare
<         n_time = self.N
<         diagline = np.zeros(n_time, dtype=NODE)
< 
<         if not self.sparse_rqa:
<             #  Get recurrence matrix
<             recmat = self.recurrence_matrix()
< 
<             if self.missing_values:
<                 mv_indices = self.missing_value_indices
<                 _diagline_dist_missingvalues(
<                     n_time, diagline, recmat, mv_indices)
<             else:
<                 _diagline_dist(n_time, diagline, recmat)
< 
<         #  Calculations for sequential RQA
<         elif self.metric == "supremum" and self.threshold is not None:
<             #  Get embedding
<             embedding = self.embedding
<             #  Get time series dimension
<             dim = embedding.shape[1]
<             #  Get threshold
<             eps = float(self.threshold)
< 
<             if self.missing_values:
<                 mv_indices = self.missing_value_indices
<                 _diagline_dist_sequential_missingvalues(
<                     n_time, diagline, mv_indices, embedding, eps, dim)
<             else:
<                 _diagline_dist_sequential(
<                     n_time, diagline, embedding, eps, dim)
< 
---
>         if self._diagline_dist_cached:
>             return self._diagline_dist
901,903c932,966
<             raise NotImplementedError(
<                 "Sequential RQA is currently only available for "
<                 "fixed threshold and the supremum metric.")
---
>             #  Prepare
>             n_time = self.N
>             diagline = np.zeros(n_time, dtype=NODE)
> 
>             if not self.sparse_rqa:
>                 #  Get recurrence matrix
>                 recmat = self.recurrence_matrix()
> 
>                 if self.missing_values:
>                     mv_indices = self.missing_value_indices
>                     _diagline_dist_norqa_missingvalues(n_time, diagline,
>                                                        recmat, mv_indices)
>                 else:
>                     _diagline_dist_norqa(n_time, diagline, recmat)
> 
>             #  Calculations for sequential RQA
>             elif self.sparse_rqa and self.metric == "supremum":
>                 #  Get embedding
>                 embedding = self.embedding
>                 #  Get time series dimension
>                 dim = embedding.shape[1]
>                 #  Get threshold
>                 eps = float(self.threshold)
> 
>                 if self.missing_values:
>                     mv_indices = self.missing_value_indices
>                     _diagline_dist_rqa_missingvalues(n_time, diagline,
>                                                      mv_indices, embedding,
>                                                      eps, dim)
>                 else:
>                     _diagline_dist_rqa(n_time, diagline, embedding, eps, dim)
> 
>             #  Function just runs over the upper triangular matrix
>             self._diagline_dist = 2*diagline
>             self._diagline_dist_cached = True
905,906c968
<         #  Function just runs over the upper triangular matrix
<         return 2 * diagline
---
>             return self._diagline_dist
926c988
<         resampled_dist = np.zeros(N, dtype=NODE)
---
>         resampled_dist = np.zeros(N, dtype=FIELD)
929c991
<         dist = to_cy(dist, DFIELD)
---
>         dist = to_cy(dist, FIELD)
959,964c1021,1023
<         if L_max == 0:
<             resampled_dist = diagline
<         else:
<             resampled_dist = np.zeros(len(diagline), dtype=NODE)
<             resampled_dist[:L_max] = RecurrencePlot.\
<                 rejection_sampling(diagline[:L_max], M)
---
>         resampled_dist = np.zeros(len(diagline))
>         resampled_dist[:L_max + 1] = RecurrencePlot.\
>             rejection_sampling(diagline[:L_max + 1], M)
978c1037,1045
<         return 1 + np.nonzero(self.diagline_dist())[0].max(initial=-1)
---
>         diagline = self.diagline_dist()
>         n_time = self.N
>         lmax = 1
> 
>         for i in range(1, n_time):
>             if diagline[i] != 0:
>                 lmax = i
> 
>         return lmax
995,996c1062,1066
<         diagline = (self.diagline_dist() if resampled_dist is None
<                     else resampled_dist)
---
>         if resampled_dist is None:
>             diagline = self.diagline_dist()
>         else:
>             diagline = resampled_dist
> 
1001c1071
<         partial_sum = np.arange(l_min, n_time+1) @ diagline[l_min-1:]
---
>         partial_sum = (np.arange(l_min, n_time) * diagline[l_min:]).sum()
1005c1075
<         full_sum = np.arange(1, n_time+1) @ diagline
---
>         full_sum = (np.arange(n_time) * diagline).sum()
1023,1024c1093,1097
<         diagline = (self.diagline_dist() if resampled_dist is None
<                     else resampled_dist)
---
>         if resampled_dist is None:
>             diagline = self.diagline_dist()
>         else:
>             diagline = resampled_dist
> 
1029c1102
<         partial_sum = np.arange(l_min, n_time+1) @ diagline[l_min-1:]
---
>         partial_sum = (np.arange(l_min, n_time) * diagline[l_min:]).sum()
1032c1105
<         number_diagline = diagline[l_min-1:].sum()
---
>         number_diagline = diagline[l_min:].sum()
1051,1052c1124,1127
<         diagline = (self.diagline_dist() if resampled_dist is None
<                     else resampled_dist)
---
>         if resampled_dist is None:
>             diagline = self.diagline_dist()
>         else:
>             diagline = resampled_dist
1056c1131
<         diagline = diagline[l_min-1:]
---
>         diagline = diagline[l_min:]
1069,1070d1143
<     @Cached.method(attrs=(
<         "metric", "threshold", "missing_values", "sparse_rqa"))
1074,1075c1147
<         <triple: frequency distribution; vertical; line length>`
<         :math:`P(v-1)`.
---
>         <triple: frequency distribution; vertical; line length>` :math:`P(v)`.
1077c1149
<         Note that entry :math:`P(v-1)` contains the number of
---
>         The :math:`v` th entry of :math:`P(v)` contains the number of
1079,1080d1150
<         Thus, :math:`P(0)` counts lines of length :math:`1`,
<         :math:`P(1)` counts lines of length :math:`2`, asf.
1084c1154
<             :math:`P(v-1)`.
---
>             :math:`P(v)`.
1086,1118c1156,1157
<         #  Prepare
<         n_time = self.N
<         vertline = np.zeros(n_time, dtype=NODE)
< 
<         if not self.sparse_rqa:
<             #  Get recurrence matrix
<             recmat = self.recurrence_matrix()
< 
<             if self.missing_values:
<                 mv_indices = self.missing_value_indices
<                 _vertline_dist_missingvalues(
<                     n_time, vertline, recmat, mv_indices)
<             else:
<                 _vertline_dist(n_time, vertline, recmat)
< 
<         #  Calculations for sequential RQA
<         elif self.metric == "supremum" and self.threshold is not None:
<             #  Get embedding
<             embedding = self.embedding
<             #  Get time series dimension
<             dim = embedding.shape[1]
<             #  Get threshold
<             eps = float(self.threshold)
< 
<             if self.missing_values:
<                 mv_indices = self.missing_value_indices
<                 _vertline_dist_sequential_missingvalues(
<                     n_time, vertline, mv_indices, embedding, eps, dim)
< 
<             else:
<                 _vertline_dist_sequential(
<                     n_time, vertline, embedding, eps, dim)
< 
---
>         if self._vertline_dist_cached:
>             return self._vertline_dist
1120,1122c1159,1194
<             raise NotImplementedError(
<                 "Sequential RQA is currently only available for "
<                 "fixed threshold and the supremum metric.")
---
>             #  Prepare
>             n_time = self.N
>             vertline = np.zeros(n_time, dtype=NODE)
> 
>             if not self.sparse_rqa:
>                 #  Get recurrence matrix
>                 recmat = self.recurrence_matrix()
> 
>                 if self.missing_values:
>                     mv_indices = self.missing_value_indices
>                     _vertline_dist_norqa_missingvalues(n_time, vertline,
>                                                        recmat, mv_indices)
>                 else:
>                     _vertline_dist_norqa(n_time, vertline, recmat)
> 
>             #  Calculations for sequential RQA
>             elif self.sparse_rqa and self.metric == "supremum":
>                 #  Get embedding
>                 embedding = self.embedding
>                 #  Get time series dimension
>                 dim = embedding.shape[1]
>                 #  Get threshold
>                 eps = float(self.threshold)
> 
>                 if self.missing_values:
>                     mv_indices = self.missing_value_indices
>                     _vertline_dist_rqa_missingvalues(n_time, vertline,
>                                                      mv_indices, embedding,
>                                                      eps, dim)
> 
>                 else:
>                     _vertline_dist_rqa(n_time, vertline, embedding, eps, dim)
> 
>             #  Function covers the whole recurrence matrix
>             self._vertline_dist = vertline
>             self._vertline_dist_cached = True
1124,1125c1196
<         #  Function covers the whole recurrence matrix
<         return vertline
---
>             return self._vertline_dist
1150,1155c1221,1223
<         if L_max == 0:
<             resampled_dist = vertline
<         else:
<             resampled_dist = np.zeros(len(vertline), dtype=NODE)
<             resampled_dist[:L_max] = RecurrencePlot.\
<                 rejection_sampling(vertline[:L_max], M)
---
>         resampled_dist = np.zeros(len(vertline))
>         resampled_dist[:L_max + 1] = RecurrencePlot.\
>             rejection_sampling(vertline[:L_max + 1], M)
1169c1237,1245
<         return 1 + np.nonzero(self.vertline_dist())[0].max(initial=-1)
---
>         vertline = self.vertline_dist()
>         n_time = self.N
>         vmax = 1
> 
>         for i in range(1, n_time):
>             if vertline[i] != 0:
>                 vmax = i
> 
>         return vmax
1185,1186c1261,1265
<         vertline = (self.vertline_dist() if resampled_dist is None
<                     else resampled_dist)
---
>         if resampled_dist is None:
>             vertline = self.vertline_dist()
>         else:
>             vertline = resampled_dist
> 
1191c1270
<         partial_sum = np.arange(v_min, n_time+1) @ vertline[v_min-1:]
---
>         partial_sum = (np.arange(v_min, n_time) * vertline[v_min:]).sum()
1194c1273
<         full_sum = np.arange(1, n_time+1) @ vertline
---
>         full_sum = (np.arange(n_time) * vertline).sum()
1213,1214c1292,1296
<         vertline = (self.vertline_dist() if resampled_dist is None
<                     else resampled_dist)
---
>         if resampled_dist is None:
>             vertline = self.vertline_dist()
>         else:
>             vertline = resampled_dist
> 
1219c1301
<         partial_sum = np.arange(v_min, n_time+1) @ vertline[v_min-1:]
---
>         partial_sum = (np.arange(v_min, n_time) * vertline[v_min:]).sum()
1222c1304
<         number_vertline = vertline[v_min-1:].sum()
---
>         number_vertline = vertline[v_min:].sum()
1254c1336
<         vertline = vertline[v_min-1:]
---
>         vertline = vertline[v_min:]
1271c1353
<         :math:`P(w-1)`.
---
>         :math:`P(w)`.
1273c1355
<         Note that entry :math:`P(w-1)` contains the number of
---
>         The :math:`w` th entry of :math:`P(w)` contains the number of
1275,1276c1357
<         :math:`w`. Thus, :math:`P(0)` counts lines of length :math:`1`,
<         :math:`P(1)` counts lines of length :math:`2`, asf.
---
>         :math:`w`.
1283c1364
<             :math:`P(w-1)`.
---
>             :math:`P(w)`.
1287a1369
> 
1304c1386,1394
<         return 1 + np.nonzero(self.white_vertline_dist())[0].max(initial=-1)
---
>         white_vertline = self.white_vertline_dist()
>         N = self.N
>         vmax = 1
> 
>         for i in range(1, N):
>             if white_vertline[i] != 0:
>                 vmax = i
> 
>         return vmax
1324c1414
<         partial_sum = np.arange(w_min, n_time+1) @ white_vertline[w_min-1:]
---
>         partial_sum = (np.arange(w_min, n_time) * white_vertline[w_min:]).sum()
1327c1417
<         number_white_vertline = white_vertline[w_min-1:].sum()
---
>         number_white_vertline = white_vertline[w_min:].sum()
1352c1442
<         white_vertline = white_vertline[w_min-1:]
---
>         white_vertline = white_vertline[w_min:]
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/timeseries/surrogates.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/timeseries/surrogates.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
20,22d22
< from typing import Tuple
< from collections.abc import Hashable
< 
27,32c27
< # easy progress bar handling
< from tqdm import trange
< 
< from ..core.cache import Cached
< 
< from ..core._ext.types import to_cy, ADJ, DEGREE, DFIELD
---
> from ..core._ext.types import to_cy, ADJ, DEGREE, FIELD, DFIELD
36a32,35
> # easy progress bar handling
> from ..utils import progressbar
> 
> 
41,42c40
< 
< class Surrogates(Cached):
---
> class Surrogates:
80,85d77
<         (self.N, self.n_time) = self.original_data.shape
< 
<         self._mut_embedding: int = 0
<         self._embedding = None
<         """The embedded times series"""
< 
87a80,85
>         self._fft_cached = False
>         self._twins_cached = False
> 
>         #  Cache
>         self._twins = None
>         self._original_data_fft = None
93,104c91,92
<         return f"Surrogates: time series shape {self.original_data.shape}."
< 
<     def __cache_state__(self) -> Tuple[Hashable, ...]:
<         return (self._mut_embedding,)
< 
<     @property
<     def embedding(self) -> np.ndarray:
<         """
<         The embedded time series / phase space trajectory
<         (time, embedding dimension).
<         """
<         return self._embedding
---
>         return 'Surrogates: time series shape %s.' % (
>             self.original_data.shape,)
106,109c94,100
<     @embedding.setter
<     def embedding(self, embedding: np.ndarray):
<         self._embedding = to_cy(embedding, DFIELD)
<         self._mut_embedding += 1
---
>     def clear_cache(self):
>         """Clean up cache."""
>         try:
>             del self._original_data_fft
>             del self._twins
>         except AttributeError:
>             pass
138,139c129,159
<     def embed_time_series_array(time_series_array, dimension, delay,
<                                 silence_level=1):
---
>     def normalize_time_series_array(time_series_array):
>         """
>         :index:`Normalize <pair: normalize; time series array>` an array of
>         time series to zero mean and unit variance individually for each
>         individual time series.
> 
>         **Modifies the given array in place!**
> 
>         **Examples:**
> 
>         >>> ts = Surrogates.SmallTestData().original_data
>         >>> Surrogates.SmallTestData().normalize_time_series_array(ts)
>         >>> r(ts.mean(axis=1))
>         array([ 0., 0., 0., 0., 0., 0.])
>         >>> r(ts.std(axis=1))
>         array([ 1., 1., 1., 1., 1., 1.])
> 
>         :type time_series_array: 2D array [index, time]
>         :arg time_series_array: The time series array to be normalized.
>         """
>         mean = time_series_array.mean(axis=1)
>         std = time_series_array.std(axis=1)
> 
>         for i in range(time_series_array.shape[0]):
>             #  Remove mean value from time series at each node (grid point)
>             time_series_array[i, :] -= mean[i]
>             #  Normalize the standard deviation of anomalies to one
>             if std[i] != 0:
>                 time_series_array[i, :] /= std[i]
> 
>     def embed_time_series_array(self, time_series_array, dimension, delay):
142a163,165
>         .. note::
>            Only works for scalar time series!
> 
146c169
<         >>> Surrogates.embed_time_series_array(
---
>         >>> Surrogates.SmallTestData().embed_time_series_array(
162c185
<         if silence_level <= 1:
---
>         if self.silence_level <= 1:
175,203c198,201
<     def normalize_original_data(self):
<         """
<         :index:`Normalize <pair: normalize; time series array>` the original
<         data to zero mean and unit variance individually for each
<         individual time series.
< 
<         **Examples:**
< 
<         >>> ts = Surrogates.SmallTestData()
<         >>> ts.normalize_original_data()
<         >>> r(ts.original_data.mean(axis=1))
<         array([ 0., 0., 0., 0., 0., 0.])
<         >>> r(ts.original_data.std(axis=1))
<         array([ 1., 1., 1., 1., 1., 1.])
<         """
<         mean = self.original_data.mean(axis=1)
<         std = self.original_data.std(axis=1)
< 
<         for i in range(self.N):
<             #  Remove mean value from time series at each node (grid point)
<             self.original_data[i, :] -= mean[i]
<             #  Normalize the standard deviation of anomalies to one
<             if std[i] != 0:
<                 self.original_data[i, :] /= std[i]
< 
<         self._normalized = True
< 
<     @staticmethod
<     def recurrence_plot(embedding, threshold, silence_level=1):
---
>     # FIXME: I(wb) included the line
>     # dimension = embedding.shape[1]
>     # whose missing caused an error. I can't guarantee if it is correct.
>     def recurrence_plot(self, embedding, threshold):
210,221d207
<         **Example:**
< 
<         >>> ts = Surrogates.SmallTestData().original_data
<         >>> embedding = Surrogates. \
<         ...     embed_time_series_array(ts, dimension=3, delay=2)
<         >>> Surrogates.recurrence_plot(embedding[0], threshold=.8)[:5, :5]
<         array([[1, 1, 0, 0, 0],
<                [1, 1, 1, 0, 0],
<                [0, 1, 1, 1, 0],
<                [0, 0, 1, 1, 1],
<                [0, 0, 0, 1, 1]], dtype=int8)
< 
228c214
<         if silence_level <= 1:
---
>         if self.silence_level <= 1:
236c222
<                          to_cy(embedding, DFIELD), R)
---
>                          to_cy(embedding, FIELD), R)
239,240c225,228
<     @Cached.method(name="twins", attrs=("_mut_embedding", "_normalized"))
<     def twins(self, threshold, min_dist=7):
---
>     # FIXME: I(wb) included the line
>     # dimension = embedding_array.shape[2]
>     # whose missing caused an error. I can't guarantee if it is correct.
>     def twins(self, embedding_array, threshold, min_dist=7):
261,263c249,251
<         N = self.embedding.shape[0]
<         n_time = self.embedding.shape[1]
<         dimension = self.embedding.shape[2]
---
>         N = embedding_array.shape[0]
>         n_time = embedding_array.shape[1]
>         dimension = embedding_array.shape[2]
272c260
<                  to_cy(self.embedding, DFIELD), R, nR, twins)
---
>                  to_cy(embedding_array, DFIELD), R, nR, twins)
280,290c268
<     @Cached.method(name="original data fft", attrs=("_normalized",))
<     def original_data_fft(self):
<         """
<         Return one-dimensional discrete Fourier Transform via numpy.fft.rfft()
< 
<         :rtype: 2D array [index, frequency]
<         :return: The original time series' FFT.
<         """
<         return np.fft.rfft(self.original_data, axis=1)
< 
<     def white_noise_surrogates(self):
---
>     def white_noise_surrogates(self, original_data):
303c281
<         ...     SmallTestData().white_noise_surrogates()
---
>                 SmallTestData().white_noise_surrogates(ts)
307a286,287
>         :type original_data: 2D array [index, time]
>         :arg original_data: The original time series.
317c297
<         surrogates = self.original_data.copy()
---
>         surrogates = original_data.copy()
324c304
<     def correlated_noise_surrogates(self):
---
>     def correlated_noise_surrogates(self, original_data):
333a314,318
>         The Fast Fourier transforms of all time series are cached to facilitate
>         a faster generation of several surrogates for each time series. Hence,
>         :meth:`clear_cache` has to be called before generating surrogates from
>         a different set of time series!
> 
342,347c327,331
<         >>> ts = Surrogates.SmallTestData()
<         >>> surrogates = ts.correlated_noise_surrogates()
<         >>> all(np.abs(np.fft.fft(
<         ...         ts.original_data, axis=1))[0,1:10]).round(4) ==
<         ...     np.abs(np.fft.fft(
<         ...         surrogates,       axis=1))[0,1:10]).round(4))
---
>         >>> ts = Surrogates.SmallTestData().original_data
>         >>> surrogates = Surrogates.\
>                 SmallTestData().correlated_noise_surrogates(ts)
>         >>> all(r(np.abs(np.fft.fft(ts,         axis=1))[0,1:10]) == \
>                 r(np.abs(np.fft.fft(surrogates, axis=1))[0,1:10]))
364c348,355
<         surrogates = self.original_data_fft()
---
>         #  The FFT of the original_data data has to be calculated only once,
>         #  so it is stored in self._original_data_fft.
>         if self._fft_cached:
>             surrogates = self._original_data_fft
>         else:
>             surrogates = np.fft.rfft(original_data, axis=1)
>             self._original_data_fft = surrogates
>             self._fft_cached = True
366a358
>         (N, n_time) = original_data.shape
371,372c363
<         phases = random.uniform(
<             low=0, high=2 * np.pi, size=(self.N, len_phase))
---
>         phases = random.uniform(low=0, high=2 * np.pi, size=(N, len_phase))
379,380c370,371
<         return np.ascontiguousarray(
<             np.real(np.fft.irfft(surrogates, n=self.n_time, axis=1)))
---
>         return np.ascontiguousarray(np.real(np.fft.irfft(surrogates, n=n_time,
>                                                          axis=1)))
382c373
<     def AAFT_surrogates(self):
---
>     def AAFT_surrogates(self, original_data):
388a380,381
>         :type original_data: 2D array [index, time]
>         :arg original_data: The original time series.
393c386
<         gaussian = random.randn(self.N, self.n_time)
---
>         gaussian = random.randn(original_data.shape[0], original_data.shape[1])
397,398c390,391
<         ranks = self.original_data.argsort(axis=1).argsort(axis=1)
<         rescaled_data = np.zeros((self.N, self.n_time))
---
>         ranks = original_data.argsort(axis=1).argsort(axis=1)
>         rescaled_data = np.zeros(original_data.shape)
400c393
<         for i in range(self.N):
---
>         for i in range(original_data.shape[0]):
404,406c397,398
<         phase_randomized_data = Surrogates(
<             original_data=rescaled_data, silence_level=2
<             ).correlated_noise_surrogates()
---
>         phase_randomized_data = \
>             self.correlated_noise_surrogates(rescaled_data)
409c401
<         sorted_original = self.original_data.copy()
---
>         sorted_original = original_data.copy()
414c406
<         for i in range(self.N):
---
>         for i in range(original_data.shape[0]):
419c411,412
<     def refined_AAFT_surrogates(self, n_iterations, output="true_amplitudes"):
---
>     def refined_AAFT_surrogates(self, original_data, n_iterations,
>                                 output="true_amplitudes"):
429a423,424
>         :type original_data: 2D array [index, time]
>         :arg original_data: The original time series.
438,439c433,442
<         #  Get Fourier transform of original data
<         fourier_transform = self.original_data_fft()
---
>         #  Get size of dimensions
>         n_time = original_data.shape[1]
> 
>         #  Get Fourier transform of original data with caching
>         if self._fft_cached:
>             fourier_transform = self._original_data_fft
>         else:
>             fourier_transform = np.fft.rfft(original_data, axis=1)
>             self._original_data_fft = fourier_transform
>             self._fft_cached = True
445c448
<         sorted_original = self.original_data.copy()
---
>         sorted_original = original_data.copy()
450c453
<         R = self.AAFT_surrogates()
---
>         R = self.AAFT_surrogates(original_data)
453c456
<         for _ in range(n_iterations):
---
>         for i in range(n_iterations):
460,461c463,464
<             s = np.fft.irfft(original_fourier_amps * r_phases,
<                              n=self.n_time, axis=1)
---
>             s = np.fft.irfft(original_fourier_amps * r_phases, n=n_time,
>                              axis=1)
466c469
<             for j in range(self.N):
---
>             for j in range(original_data.shape[0]):
478c481,482
<     def twin_surrogates(self, dimension, delay, threshold, min_dist=7):
---
>     def twin_surrogates(self, original_data, dimension, delay, threshold,
>                         min_dist=7):
490a495,501
>         The twin lists of all time series are cached to facilitate a faster
>         generation of several surrogates for each time series. Hence,
>         :meth:`clear_cache` has to be called before generating twin surrogates
>         from a different set of time series!
> 
>         :type original_data: 2D array [index, time]
>         :arg original_data: The original time series.
507c518,519
<         n_time = self.n_time - (dimension-1)*delay
---
>         (N, n_time) = original_data.shape
>         n_time = n_time - (dimension-1)*delay
509,511c521,529
<         self.embedding = \
<             self.embed_time_series_array(self.original_data, dimension, delay)
<         twins = self.twins(threshold, min_dist)
---
>         #  Make sure that twins are calculated only once
>         if self._twins_cached:
>             twins = self._twins
>         else:
>             embedding = self.embed_time_series_array(original_data,
>                                                      dimension, delay)
>             twins = self.twins(embedding, threshold, min_dist)
>             self._twins = twins
>             self._twins_cached = True
513,514c531,532
<         return _twin_surrogates_s(self.N, n_time, twins,
<                                   to_cy(self.original_data, DFIELD))
---
>         return _twin_surrogates_s(N, n_time, twins,
>                                   to_cy(original_data, DFIELD))
610c628
<     def original_distribution(self, test_function, n_bins=100):
---
>     def original_distribution(self, test_function, original_data, n_bins=100):
618a637,638
>         :type original_data: 2D array [index, time]
>         :arg original_data: The original time series.
629c649,650
<             self.normalize_original_data()
---
>             self.normalize_time_series_array(original_data)
>             self._normalized = True
631,632c652,653
<         correlation_measure = np.abs(test_function(self.original_data,
<                                                    self.original_data))
---
>         correlation_measure = np.abs(test_function(original_data,
>                                                    original_data))
675,676c696,701
<         self.original_data_fft.cache_clear()
<         self.twins.cache_clear()
---
>         original_data = self.original_data
>         self._fft_cached = False
>         self._twins_cached = False
> 
>         #  Create reference to np.histogram function
>         numpy_hist = np.histogram
680c705,706
<             self.normalize_original_data()
---
>             self.normalize_time_series_array(original_data)
>             self._normalized = True
685c711,719
<         for _ in trange(realizations, disable=self.silence_level > 2):
---
>         #  Initialize progress bar
>         if self.silence_level <= 2:
>             progress = progressbar.ProgressBar(maxval=realizations).start()
> 
>         for i in range(realizations):
>             #  Update progress bar
>             if self.silence_level <= 2:
>                 progress.update(i)
> 
688c722
<             surrogates = surrogate_function(self)
---
>             surrogates = surrogate_function(original_data)
691c725
<             correlation_measure_test = np.abs(test_function(self.original_data,
---
>             correlation_measure_test = np.abs(test_function(original_data,
701,702c735,736
<             (hist, lbb) = np.histogram(correlation_measure_test, n_bins,
<                                        interval, density=True)
---
>             (hist, lbb) = numpy_hist(correlation_measure_test, n_bins,
>                                      interval, normed=True)
709a744,746
> 
>         if self.silence_level <= 2:
>             progress.finish()
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/timeseries/visibility_graph.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/timeseries/visibility_graph.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
58,59c61,63
<         :type time_series: 1D array
<         :arg time_series: The (scalar) time series to be analyzed.
---
>         :type time_series: 2D array (time, dimension)
>         :arg time_series: The time series to be analyzed, can be scalar or
>             multi-dimensional.
107,109c111,112
<         return ("VisibilityGraph: "
<                 f"time series shape {self.time_series.shape}.\n"
<                 f"{InteractingNetworks.__str__(self)}")
---
>         return 'VisibilityGraph: time series shape %s.\n%s' % (
>             self.time_series.shape, InteractingNetworks.__str__(self))
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/utils/__init__.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/utils/__init__.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
18a22,30
> 
> To do
> ~~~~~
>   - ...
> 
> Known Bugs
> ~~~~~~~~~~
>   - ...
> 
21c33
< __all__ = ['mpi']
---
> __all__ = ['mpi', 'navigator']
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/utils/mpi.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/utils/mpi.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
32,33c35
<    .. literalinclude::
<         ../../../../docs/source/examples/modules/mpi/network_large.py
---
>    .. literalinclude:: ../../../../examples/modules/mpi/network_large.py
37,38c39
<    .. literalinclude::
<         ../../../../docs/source/examples/modules/mpi/network_mc.py
---
>    .. literalinclude:: ../../../../examples/modules/mpi/network_mc.py
43,44c44
<    .. literalinclude::
<         ../../../../docs/source/examples/modules/mpi/network_scan_no_comm.py
---
>    .. literalinclude:: ../../../../examples/modules/mpi/network_scan_no_comm.py
Only in lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/utils: navigator.py
Only in lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/utils: progressbar
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/version.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/src/pyunicorn/version.py
2,3c2,3
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
Only in source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests: conftest.py
Only in source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_climate: __init__.py
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_climate/test_climate_data.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_climate/test_climate_data.py
0a1,3
> #!/usr/bin/env python3
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_climate/test_climate_network.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_climate/test_climate_network.py
0a1,3
> #!/usr/bin/env python3
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
24c27
<     out = capsys.readouterr()[0]
---
>     out, err = capsys.readouterr()
Only in source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_climate: test_coupled_climate_network.py
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_climate/test_eventseries_climatenetwork.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_climate/test_eventseries_climatenetwork.py
0a1,3
> #!/usr/bin/env python3
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
31c34
<     out = capsys.readouterr()[0]
---
>     out, err = capsys.readouterr()
Only in source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_climate: test_map_plot.py
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_climate/test_tsonis.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_climate/test_tsonis.py
0a1,3
> #!/usr/bin/env python3
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
25c28
<     out = capsys.readouterr()[0]
---
>     out, err = capsys.readouterr()
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_core/ResistiveNetwork_utils.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_core/ResistiveNetwork_utils.py
0a1,3
> #!/usr/bin/env python
> # -*- coding: utf-8 -*-
> #
5,7c8
< 
< """
< Utils needed for Unit Tests in resistive networks
---
> """ Utils needed for Unit Tests in resistive networks
Only in source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_core: test_cache.py
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_core/test_data.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_core/test_data.py
0a1,3
> #!/usr/bin/env python3
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_core/test_geo_grid.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_core/test_geo_grid.py
0a1,3
> #!/usr/bin/env python3
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
24,25c27,28
<                               space_grid=(np.array([0., 5.]),
<                                           np.array([1., 2.])),
---
>                               lat_grid=np.array([0., 5.]),
>                               lon_grid=np.array([1., 2.]),
31,32c34,35
<                               space_grid=(np.array([0., 5.]),
<                                           np.array([1., 2.])),
---
>                               lat_grid=np.array([0., 5.]),
>                               lon_grid=np.array([1., 2.]),
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_core/test_geo_network.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_core/test_geo_network.py
0a1,3
> #!/usr/bin/env python3
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
27c30
<     out = capsys.readouterr()[0]
---
>     out, err = capsys.readouterr()
43c46,47
<                                degree=GeoNetwork.SmallTestNetwork().degree())
---
>                                degrees=GeoNetwork.SmallTestNetwork().degree(),
>                                silence_level=2)
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_core/test_grid.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_core/test_grid.py
0a1,3
> #!/usr/bin/env python3
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_core/test_interacting_networks.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_core/test_interacting_networks.py
0a1,3
> #!/usr/bin/env python3
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_core/test_network.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_core/test_network.py
0a1,3
> #!/usr/bin/env python3
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
14d16
< 
18d19
< 
21c22
< from multiprocessing import get_context, cpu_count
---
> from multiprocessing import Pool, cpu_count
23d23
< import pytest
28d27
< from pyunicorn.core.network import r
34a34,36
> # turn off for weave compilation & error detection
> parallel = False
> 
43c45
<             desired, actual = desired.toarray(), actual.toarray()
---
>             desired, actual = desired.A, actual.A
63,64c65,68
<     cores = cpu_count()
<     with get_context("spawn").Pool() as pool:
---
>     if not parallel:
>         compare_measures(net, pnets, rev_perms, tasks)
>     else:
>         pool, cores = Pool(), cpu_count()
176,192d179
< 
< # -----------------------------------------------------------------------------
< # test doctest helpers
< # -----------------------------------------------------------------------------
< 
< 
< def test_r():
<     arr = np.random.rand(3, 3)
<     assert r(arr).dtype == np.float64
< 
< 
< def test_r_type_error():
<     arr = np.array(['one', 'two', 'three'])
<     with pytest.raises(TypeError, match='obj is of unsupported dtype kind.'):
<         r(arr)
< 
< 
207c194
<     out = capsys.readouterr()[0]
---
>     out, err = capsys.readouterr()
220c207
<     out1 = capsys.readouterr()[0]
---
>     out1, err = capsys.readouterr()
225c212
<     out2 = capsys.readouterr()[0]
---
>     out2, err = capsys.readouterr()
234c221
<     out = capsys.readouterr()[0]
---
>     out, err = capsys.readouterr()
256c243
<     out = capsys.readouterr()[0]
---
>     out, err = capsys.readouterr()
261c248
< def test_set_node_weights():
---
> def test_set_node_weights(capsys):
273c260
<     out = capsys.readouterr()[0]
---
>     out, err = capsys.readouterr()
286c273
<     net = Network.Model("Configuration", degree=[3 for _ in range(0, 1000)])
---
>     net = Network.Model("Configuration", degrees=[3 for _ in range(0, 1000)])
295,300d281
< def test_GrowWeights():
<     n_nodes = 10
<     weights = Network.GrowWeights(n_nodes=n_nodes)
<     assert len(weights) == n_nodes
< 
< 
304c285
<     out = capsys.readouterr()[0]
---
>     out, err = capsys.readouterr()
309c290
<     out = capsys.readouterr()[0]
---
>     out, err = capsys.readouterr()
324c305
<     assert np.array_equal(net.undirected_adjacency().toarray(), adj_ref)
---
>     assert np.array_equal(net.undirected_adjacency().A, adj_ref)
334,338d314
< def test_laplacian_value_error():
<     with pytest.raises(ValueError, match='direction must be "in" or "out".'):
<         Network.SmallDirectedTestNetwork().laplacian(direction='some_other')
< 
< 
388a365
> 
393,401c370
< @pytest.mark.parametrize("tw, exp, exp_split", [
<     (None,
<      np.array([6.3, 5.3, 5.9, 3.6, 4., 2.5]),
<      np.array([6.3, 5.3, 5.9, 3.6, 4., 2.5, 2.5])),
<     (2.,
<      np.array([2.15, 1.65, 1.95, 0.8, 1., 0.25]),
<      np.array([2.15, 1.65, 1.95, 0.8, 1., 0.25, 0.25]))
<     ])
< def test_nsi_indegree(tw, exp, exp_split):
---
> def test_nsi_indegree():
403,405d371
<     assert np.allclose(net.nsi_indegree(typical_weight=tw), exp)
<     assert np.allclose(
<         net.splitted_copy().nsi_indegree(typical_weight=tw), exp_split)
406a373,374
>     deg_ref = np.array([6.3, 5.3, 5.9, 3.6, 4., 2.5])
>     assert np.allclose(net.nsi_indegree(), deg_ref)
408,420c376,377
< @pytest.mark.parametrize("tw, exp, exp_split", [
<     (None,
<      np.array([5.3, 5.9, 1.9, 3.8, 5.7, 4.]),
<      np.array([5.3, 5.9, 1.9, 3.8, 5.7, 4., 4.])),
<     (2.,
<      np.array([1.65, 1.95, -0.05, 0.9, 1.85, 1.]),
<      np.array([1.65, 1.95, -0.05, 0.9, 1.85, 1., 1.]))
<     ])
< def test_nsi_outdegree(tw, exp, exp_split):
<     net = Network.SmallDirectedTestNetwork()
<     assert np.allclose(net.nsi_outdegree(typical_weight=tw), exp)
<     assert np.allclose(net.splitted_copy().nsi_outdegree(typical_weight=tw),
<                        exp_split)
---
>     deg_ref = np.array([6.3, 5.3, 5.9, 3.6, 4., 2.5, 2.5])
>     assert np.allclose(net.splitted_copy().nsi_indegree(), deg_ref)
423,431c380
< @pytest.mark.parametrize("tw, exp, exp_split", [
<     (None,
<      np.array([1.5, 1.7, 1.9, 2.1, 2.3, 2.5]),
<      np.array([1.5, 1.7, 1.9, 2.1, 2.3, 2.5, 2.5])),
<     (2.,
<      np.array([-0.25, -0.15, -0.05,  0.05,  0.15,  0.25]),
<      np.array([-0.25, -0.15, -0.05,  0.05,  0.15,  0.25,  0.25]))
<     ])
< def test_nsi_bildegree(tw, exp, exp_split):
---
> def test_nsi_outdegree():
433,435c382,387
<     assert np.allclose(net.nsi_bildegree(typical_weight=tw), exp)
<     assert np.allclose(net.splitted_copy().nsi_bildegree(typical_weight=tw),
<                        exp_split)
---
> 
>     deg_ref = np.array([5.3, 5.9, 1.9, 3.8, 5.7, 4.])
>     assert np.allclose(net.nsi_outdegree(), deg_ref)
> 
>     deg_ref = np.array([5.3, 5.9, 1.9, 3.8, 5.7, 4., 4.])
>     assert np.allclose(net.splitted_copy().nsi_outdegree(), deg_ref)
552,564c504
< @pytest.mark.parametrize("tw, exp, exp_split", [
<     (None,
<      np.array([0.18448637, 0.20275024, 0.3220339,
<                0.32236842, 0.34385965, 0.625]),
<      np.array([0.18448637, 0.20275024, 0.3220339,
<                0.32236842, 0.34385965, 0.625, 0.20275024])),
<     (2.,
<      np.array([0.3309814,  0.29011913,  0.05236908,
<                -0.0260989,  0.22417582, -0.13636364]),
<      np.array([0.3309814,  0.29011913,  0.05236908,
<                -0.0260989,  0.22417582, -0.13636364,  0.29011913]))
<     ])
< def test_nsi_local_cyclemotif_clustering(tw, exp, exp_split):
---
> def test_nsi_local_cyclemotif_clustering():
566,571c506,515
<     assert np.allclose(net.nsi_local_cyclemotif_clustering(typical_weight=tw),
<                        exp)
<     assert np.allclose(
<         net.splitted_copy(node=1).nsi_local_cyclemotif_clustering(
<             typical_weight=tw),
<         exp_split)
---
> 
>     res = net.nsi_local_cyclemotif_clustering()
>     exp = np.array([0.18448637, 0.20275024, 0.3220339,
>                     0.32236842, 0.34385965, 0.625])
>     assert np.allclose(res, exp)
> 
>     res = net.splitted_copy(node=1).nsi_local_cyclemotif_clustering()
>     exp = np.array([0.18448637, 0.20275024, 0.3220339,
>                     0.32236842, 0.34385965, 0.625, 0.20275024])
>     assert np.allclose(res, exp)
661,669c605
< @pytest.mark.parametrize("tw, exp, exp_split", [
<     (None,
<      np.array([0.55130385, 0.724375, 1., 0.81844073, 0.80277575, 1.]),
<      np.array([0.55130385, 0.724375, 1., 0.81844073, 0.80277575, 1., 1.])),
<     (3.,
<      np.array([-1.44290123, -0.764, 1., 4.16770186, -0.75324675, 1.]),
<      np.array([-1.44290123, -0.764, 1., 4.16770186, -0.75324675, 1., 1.]))
<     ])
< def test_nsi_local_clustering(tw, exp, exp_split):
---
> def test_nsi_local_clustering():
672,675c608,614
<     assert np.allclose(net.nsi_local_clustering(typical_weight=tw), exp)
<     assert np.allclose(
<         net.splitted_copy().nsi_local_clustering(typical_weight=tw),
<         exp_split)
---
>     res = net.nsi_local_clustering()
>     exp = np.array([0.55130385, 0.724375, 1., 0.81844073, 0.80277575, 1.])
>     assert np.allclose(res, exp)
> 
>     res = net.splitted_copy().nsi_local_clustering()
>     exp = np.array([0.55130385, 0.724375, 1., 0.81844073, 0.80277575, 1., 1.])
>     assert np.allclose(res, exp)
790,791c729
< @pytest.mark.parametrize("parallelize", [False, True])
< def test_nsi_betweenness(parallelize):
---
> def test_nsi_betweenness():
794c732
<     res = net.nsi_betweenness(parallelize=parallelize)
---
>     res = net.nsi_betweenness()
798,799c736,738
<     res = net.splitted_copy().nsi_betweenness(parallelize=parallelize)
<     exp = np.append(exp, [0.])
---
>     res = net.splitted_copy().nsi_betweenness()
>     exp = np.array([29.68541738, 7.7128677, 0., 3.09090906, 9.69960462, 0.,
>                     0.])
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_core/TestResistiveNetwork-circuits.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_core/TestResistiveNetwork-circuits.py
0a1,3
> #!/usr/bin/env python
> # -*- coding: utf-8 -*-
> #
182c185
<         val = np.round(np.random.rand(len(i))*100)/10
---
>         val = np.round(np.random.ranf(len(i))*100)/10
235c238
<     print(f"NW1 {ER1:.3f}\tNW2 {ER2:.3f}\t 2*NW1 = {(2*ER1):.3f}")
---
>     print("NW1 %.3f\tNW2 %.3f\t 2*NW1 = %.3f" % (ER1, ER2, 2*ER1))
247c250
<     for _ in range(0, runs):
---
>     for run in range(0, runs):
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_core/TestResistiveNetwork-complexInput.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_core/TestResistiveNetwork-complexInput.py
0a1,3
> #!/usr/bin/env python
> # -*- coding: utf-8 -*-
> #
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_core/TestResistiveNetwork-cython.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_core/TestResistiveNetwork-cython.py
0a1,3
> #!/usr/bin/env python
> # -*- coding: utf-8 -*-
> #
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_core/test_resistive_networks.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_core/test_resistive_networks.py
0a1,3
> #!/usr/bin/env python3
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
28c31
<     out = capsys.readouterr()[0]
---
>     out, err = capsys.readouterr()
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_core/TestResistiveNetwork-types.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_core/TestResistiveNetwork-types.py
0a1,3
> #!/usr/bin/env python
> # -*- coding: utf-8 -*-
> #
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_core/test_spatial_network.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_core/test_spatial_network.py
0a1,3
> #!/usr/bin/env python3
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_eventseries/test_event_series.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_eventseries/test_event_series.py
0a1,3
> #!/usr/bin/env python3
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
40c43
<     data = create_test_data()[0]
---
>     data, tstamps = create_test_data()
167,168c170,171
<     return (precursorxy/(l1-s1p), triggerxy/(l2-s2t),
<             precursoryx/(l2-s2p), triggeryx/(l1-s1t))
---
>     return precursorxy/(l1-s1p), triggerxy/(l2-s2t),\
>         precursoryx/(l2-s2p), triggeryx/(l1-s1t)
Only in source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_funcnet: test_coupling_analysis_pure_python.py
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_funcnet/test_coupling_analysis.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_funcnet/test_coupling_analysis.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
19d21
< import pytest
24,25d25
< from pyunicorn.core._ext.types import LAG, FIELD
< 
29a30
>     n_index, n_times = tdata.shape
64c65
<     similarity_matrix = np.random.rand(n_index, n_times).astype(FIELD)
---
>     similarity_matrix = np.random.rand(n_index, n_times).astype('float32')
72c73
< def test_cross_correlation_max():
---
> def test_cross_correlation():
77c78
<     exp = (np.array([[1., 0.7570, 0.7790, 0.7536],
---
>     exp = (np.array([[1., 0.757, 0.779, 0.7536],
85,96c86
< def test_cross_correlation_all():
<     coup_ana = CouplingAnalysis(CouplingAnalysis.test_data())
<     res = coup_ana.cross_correlation(tau_max=1, lag_mode='all')
<     exp = np.array(
<         [[[1., 0.8173], [0.4849, 0.5804], [0.6214, 0.7786], [0.4831, 0.6042]],
<          [[0.4849, 0.4101], [1., 0.9362], [0.4503, 0.3780], [0.5199, 0.4286]],
<          [[0.6214, 0.5178], [0.4503, 0.5376], [1., 0.4962], [0.5004, 0.5996]],
<          [[0.4831, 0.3762], [0.5199, 0.5404], [0.5004, 0.4092], [1., 0.4380]]])
<     assert np.allclose(res, exp, atol=1e-04)
< 
< 
< def test_mutual_information_knn():
---
> def test_mutual_information():
104c94
<                      [0.1209, 0.199, 0.1453, 4.6505]], dtype=FIELD),
---
>                      [0.1209, 0.199, 0.1453, 4.6505]]),
108c98
<                      [0, 2, 0, 0]], dtype=LAG))
---
>                      [0, 2, 0, 0]], dtype=np.int8))
112,152c102
< def test_mutual_information_binning():
<     coup_ana = CouplingAnalysis(CouplingAnalysis.test_data())
<     similarity_matrix, lag_matrix = coup_ana.mutual_information(
<         tau_max=5, bins=6, estimator='binning')
<     res = (similarity_matrix, lag_matrix)
<     exp = (np.array([[1.7828, 0.3765, 0.3551, 0.3288],
<                      [0.1326, 1.7828, 0.1140, 0.1498],
<                      [0.1951, 0.1784, 1.7828, 0.1918],
<                      [0.1139, 0.1681, 0.1266, 1.7828]], dtype=FIELD),
<            np.array([[0, 4, 1, 2],
<                      [0, 0, 0, 0],
<                      [0, 3, 0, 1],
<                      [0, 2, 0, 0]], dtype=LAG))
<     assert np.allclose(res, exp, atol=1e-04)
< 
< 
< @pytest.mark.filterwarnings("ignore:divide by zero encountered in log")
< def test_mutual_information_gauss():
<     coup_ana = CouplingAnalysis(CouplingAnalysis.test_data())
<     similarity_matrix, lag_matrix = coup_ana.mutual_information(
<         tau_max=5, estimator='gauss')
<     res = (similarity_matrix, lag_matrix)
<     exp = (np.array([[np.inf, 0.4255, 0.4668, 0.4196],
<                      [0.1339, np.inf, 0.1133, 0.1574],
<                      [0.2445, 0.2089, np.inf, 0.2224],
<                      [0.1326, 0.1808, 0.1436, np.inf]], dtype=FIELD),
<            np.array([[0, 4, 1, 2],
<                      [0, 0, 0, 0],
<                      [0, 3, 0, 1],
<                      [0, 2, 0, 0]], dtype=LAG))
<     assert np.allclose(res, exp, atol=1e-04)
< 
< 
< def test_mutual_information_value_error():
<     with pytest.raises(ValueError,
<                        match='estimator must be "knn", "binning" or "gauss".'):
<         CouplingAnalysis(CouplingAnalysis.test_data()) \
<             .mutual_information(estimator='some_other')
< 
< 
< def test_information_transfer_knn():
---
> def test_information_transfer():
163,186d112
< 
< 
< @pytest.mark.filterwarnings("ignore:divide by zero encountered in log")
< def test_information_transfer_gauss():
<     coup_ana = CouplingAnalysis(CouplingAnalysis.test_data())
<     similarity_matrix, lag_matrix = coup_ana.information_transfer(
<         tau_max=5, estimator='gauss')
<     res = (similarity_matrix, lag_matrix)
<     exp = (np.array([[0., 0.1732, 0.3256, 0.3148],
<                      [0.0006, 0., 0.0324, 0.0755],
<                      [0.0012, 0.0754, 0., 0.1365],
<                      [0.0008, 0.0753, 0.0442, 0.]], dtype=FIELD),
<            np.array([[0, 2, 1, 2],
<                      [5, 0, 0, 0],
<                      [3, 1, 0, 1],
<                      [1, 0, 0, 0]], dtype=LAG))
<     assert np.allclose(res, exp, atol=1e-04)
< 
< 
< def test_information_transfer_value_error():
<     with pytest.raises(ValueError,
<                        match='estimator must be "knn", "binning" or "gauss".'):
<         CouplingAnalysis(CouplingAnalysis.test_data()) \
<             .information_transfer(estimator='some_other')
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_generic.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_generic.py
0a1,2
> #! /usr/bin/env python2
> 
2,3c4,5
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
18a21,22
> import numpy as np
> 
66,67c70,71
<         # climate.EventSynchronizationClimateNetwork(
<         #     ec, 0.8, 16, eventsynctype="directedES"),
---
>         # climate.EventSynchronizationClimateNetwork(ec, 0.8, 16,
>         #                                           eventsynctype="directedES"),
Only in source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_timeseries: test_joint_recurrence_plot.py
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_timeseries/test_recurrence_plot.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_timeseries/test_recurrence_plot.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
14d16
< 
18,21d19
< 
< from itertools import chain, product
< 
< import pytest
24,148c22
< from pyunicorn.core import Data
< from pyunicorn.core._ext.types import NODE, ADJ, DFIELD
< from pyunicorn.timeseries import RecurrencePlot
< 
< 
< # -----------------------------------------------------------------------------
< # test RecurrencePlot instantiation
< # -----------------------------------------------------------------------------
< 
< # test non-default metrics
< 
< def test_RP_euclidean():
<     x = Data.SmallTestData().observable()
<     RP = RecurrencePlot(x[:, :-3], threshold=1.2, metric='euclidean')
<     res = RP.recurrence_matrix()
<     assert res.dtype == ADJ
<     exp = np.array([
<         [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
<         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],
<         [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],
<         [0, 1, 1, 1, 1, 1, 1, 1, 0, 0],
<         [0, 1, 1, 1, 1, 1, 1, 1, 0, 0],
<         [0, 0, 1, 1, 1, 1, 1, 1, 1, 0],
<         [0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
<         [0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
<         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],
<         [0, 0, 0, 0, 0, 0, 1, 1, 1, 1]
<     ])
<     assert np.array_equal(res, exp)
< 
< 
< def test_RP_manhattan():
<     x = Data.SmallTestData().observable()
<     RP = RecurrencePlot(x, threshold=3.5, metric='manhattan')
<     res = RP.recurrence_matrix()
<     assert res.dtype == ADJ
<     exp = np.array([
<         [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
<         [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],
<         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],
<         [0, 1, 1, 1, 1, 1, 1, 0, 0, 0],
<         [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],
<         [0, 0, 0, 1, 1, 1, 1, 1, 0, 0],
<         [0, 0, 0, 1, 1, 1, 1, 1, 1, 0],
<         [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],
<         [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],
<         [0, 0, 0, 0, 0, 0, 0, 1, 1, 1]
<     ])
<     assert np.array_equal(res, exp)
< 
< 
< # test thresholding variations
< 
< def test_RP_threshold_std():
<     x = Data.SmallTestData().observable()
<     RP = RecurrencePlot(x, threshold_std=.8)
<     res = RP.recurrence_matrix()
<     assert res.dtype == ADJ
<     exp = np.array([
<         [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],
<         [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],
<         [0, 1, 1, 1, 1, 0, 0, 0, 0, 0],
<         [0, 1, 1, 1, 1, 0, 0, 0, 0, 0],
<         [0, 0, 1, 1, 1, 1, 0, 0, 0, 0],
<         [0, 0, 0, 0, 1, 1, 1, 0, 0, 0],
<         [0, 0, 0, 0, 0, 1, 1, 1, 1, 0],
<         [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],
<         [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],
<         [0, 0, 0, 0, 0, 0, 0, 1, 1, 1]
<     ])
<     assert np.array_equal(res, exp)
< 
< 
< def test_RP_recurrence_rate():
<     x = Data.SmallTestData().observable()
<     RP = RecurrencePlot(x, recurrence_rate=.4)
<     res = RP.recurrence_matrix()
<     assert res.dtype == ADJ
<     exp = np.array([
<         [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],
<         [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],
<         [0, 1, 1, 1, 1, 0, 0, 0, 0, 0],
<         [0, 1, 1, 1, 1, 1, 0, 0, 0, 0],
<         [0, 0, 1, 1, 1, 1, 0, 0, 0, 0],
<         [0, 0, 0, 1, 1, 1, 1, 0, 0, 0],
<         [0, 0, 0, 0, 0, 1, 1, 1, 1, 0],
<         [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],
<         [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],
<         [0, 0, 0, 0, 0, 0, 0, 1, 1, 1]
<     ])
<     assert np.array_equal(res, exp)
< 
< 
< def test_RP_local_recurrence_rate():
<     x = Data.SmallTestData().observable()
<     RP = RecurrencePlot(x, local_recurrence_rate=.6)
<     res = RP.recurrence_matrix()
<     assert res.dtype == ADJ
<     exp = np.array([
<         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],
<         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],
<         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],
<         [0, 1, 1, 1, 1, 1, 0, 0, 0, 0],
<         [0, 0, 1, 1, 1, 1, 1, 0, 0, 0],
<         [0, 0, 0, 1, 1, 1, 1, 1, 0, 0],
<         [0, 0, 0, 0, 1, 1, 1, 1, 1, 0],
<         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],
<         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],
<         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
<     ])
<     assert np.array_equal(res, exp)
< 
< 
< # -----------------------------------------------------------------------------
< # prepare fixtures
< # -----------------------------------------------------------------------------
< 
< 
< @pytest.fixture(scope="module", name="recurrence_crit", ids=str,
<                 params=list(chain(product(np.arange(0, 1.7, .8), [None]),
<                                   product([None], np.arange(0, 1.1, .4)))))
< def recurrence_crit_fixture(request):
<     threshold, rate = request.param
<     assert np.sum([threshold is None, rate is None]) == 1
<     return request.param
---
> from pyunicorn.timeseries.recurrence_plot import RecurrencePlot
151,159c25,27
< @pytest.fixture(scope="module", name="small_RP")
< def small_RP_fixture(metric, recurrence_crit):
<     """
<     RP fixture, parametrized to cover various settings.
<     """
<     x = Data.SmallTestData().observable()
<     threshold, rate = recurrence_crit
<     return RecurrencePlot(
<         x, threshold=threshold, recurrence_rate=rate, metric=metric)
---
> def test_permutation_entropy():
>     ts = np.array([[4], [7], [9], [10], [6], [11], [3]])
>     rp = RecurrencePlot(ts, threshold=1, dim=3, tau=1)
161,260c29,30
< 
< @pytest.fixture(scope="module", name="small_RP_basic", params=[False, True])
< def small_RP_basic_fixture(request):
<     """
<     RP fixture with single basic setting to test numerical results.
<     """
<     sparse = request.param
<     x = Data.SmallTestData().observable()
<     RP = RecurrencePlot(x, threshold=.8, metric='supremum', sparse_rqa=sparse)
<     if not sparse:
<         res = RP.recurrence_matrix()
<         assert res.dtype == ADJ
<         exp = np.array([
<             [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
<             [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],
<             [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],
<             [0, 1, 1, 1, 1, 1, 0, 0, 0, 0],
<             [0, 1, 1, 1, 1, 1, 1, 0, 0, 0],
<             [0, 0, 0, 1, 1, 1, 1, 1, 0, 0],
<             [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],
<             [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],
<             [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],
<             [0, 0, 0, 0, 0, 0, 1, 1, 1, 1]
<         ])
<         assert np.array_equal(res, exp)
<     return RP
< 
< 
< # -----------------------------------------------------------------------------
< # test RecurrencePlot RQA
< # -----------------------------------------------------------------------------
< 
< 
< @pytest.mark.parametrize("measure", ["diag", "vert", "white_vert"])
< def test_line_dist(measure: str, small_RP):
<     res = getattr(small_RP, f"{measure}line_dist")()
<     assert res.dtype == NODE
<     assert res.shape[0] == small_RP.N
<     assert (0 <= res).all() and (res <= small_RP.N).all()
< 
< 
< @pytest.mark.parametrize(
<     "measure, exp",
<     [("diag", [4, 0, 0, 0, 0, 0, 0, 2, 2, 0]),
<      ("vert", [0, 0, 1, 2, 5, 2, 0, 0, 0, 0]),
<      ("white_vert", [2, 1, 2, 2, 3, 2, 1, 0, 0, 0])])
< def test_line_dist_numeric(measure: str, small_RP_basic, exp):
<     if small_RP_basic.sparse_rqa and measure == "white_vert":
<         return
<     res = getattr(small_RP_basic, f"{measure}line_dist")()
<     assert res.dtype == NODE
<     assert res.shape[0] == small_RP_basic.N
<     assert np.array_equal(res, exp)
< 
< 
< @pytest.mark.parametrize("sparse", [False, True])
< def test_line_dist_edgecases(sparse):
<     x = Data.SmallTestData().observable()
< 
<     RP = RecurrencePlot(x, metric="supremum", threshold=0., sparse_rqa=sparse)
<     assert RP.max_diaglength() == 0
<     assert RP.max_vertlength() == 0
<     if not sparse:
<         assert RP.max_white_vertlength() == RP.N
< 
<     RP = RecurrencePlot(x, metric="supremum", threshold=2., sparse_rqa=sparse)
<     assert RP.max_diaglength() == (RP.N - 1)
<     assert RP.max_vertlength() == RP.N
<     if not sparse:
<         assert RP.max_white_vertlength() == 0
< 
< 
< def test_rqa_summary(small_RP):
<     res = small_RP.rqa_summary()
<     measures = ['RR', 'DET', 'L', 'LAM']
<     assert all(res[m].dtype == DFIELD for m in measures)
< 
< 
< def test_rqa_summary_numeric(small_RP_basic):
<     res = small_RP_basic.rqa_summary()
<     exp = {'RR': 0.48, 'DET': 0.8947, 'L': 8.4999, 'LAM': 0.9999}
<     assert all(np.isclose(res[m], val, atol=1e-04)
<                for m, val in exp.items())
< 
< 
< @pytest.mark.parametrize('measure', ['diagline', 'vertline'])
< @pytest.mark.parametrize('M', np.arange(5, 90, 40).tolist())
< def test_resample_line_dist(measure: str, M: int, small_RP):
<     res = getattr(small_RP, f"resample_{measure}_dist")(M)
<     assert res.dtype == NODE
<     assert res.shape[0] == small_RP.N
<     assert (0 <= res).all()
< 
< 
< @pytest.mark.parametrize(
<     "var, exp", [("trapping", 4.7999), ("mean_recurrence", 3.9999)])
< def test_time(small_RP_basic, var, exp):
<     if small_RP_basic.sparse_rqa and var == "mean_recurrence":
<         return
<     res = getattr(small_RP_basic, f"{var}_time")()
---
>     res = rp.permutation_entropy()
>     exp = 0.5888
264c34,36
< # test entropy
---
> def test_complexity_entropy():
>     ts = np.array([[4], [7], [9], [10], [6], [11], [3]])
>     rp = RecurrencePlot(ts, threshold=1, dim=3, tau=1)
266,273c38,40
< @pytest.mark.parametrize(
<     "ts, measure, value",
<     [(np.array([4, 7, 9, 10, 6, 11, 3]), m, v)
<      for m, v in [("permutation", 0.5888), ("complexity", 0.29)]]
<     + [(np.arange(20), "complexity", 0.0)])
< def test_entropy(ts: np.ndarray, measure: str, value: float):
<     rp = RecurrencePlot(ts[:, np.newaxis], threshold=1, dim=3, tau=1)
<     assert np.isclose(getattr(rp, f"{measure}_entropy")(), value, atol=1e-04)
---
>     res = rp.complexity_entropy()
>     exp = 0.29
>     assert np.isclose(res, exp, atol=1e-04)
274a42,43
>     ts = np.array([[1], [2], [3], [4], [5], [6], [7]])
>     rp = RecurrencePlot(ts, threshold=1, dim=3, tau=1)
276,282c45,46
< @pytest.mark.parametrize(
<     'measure, exp',
<     [('diag', 0.6931), ('vert', 1.2206), ('white_vert', 1.8848)])
< def test_line_dist_entropy(measure: str, exp: float, small_RP_basic):
<     if small_RP_basic.sparse_rqa and measure == "white_vert":
<         return
<     res = getattr(small_RP_basic, f"{measure}_entropy")()
---
>     res = rp.complexity_entropy()
>     exp = 0.0
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_timeseries/test_timeseries.py lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tests/test_timeseries/test_timeseries.py
0a1,3
> #!/usr/bin/python
> # -*- coding: utf-8 -*-
> #
2,3c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
19d21
< import pytest
23,25c25,26
< from pyunicorn.timeseries import RecurrencePlot, CrossRecurrencePlot, \
<     RecurrenceNetwork, JointRecurrenceNetwork, InterSystemRecurrenceNetwork, \
<     Surrogates, VisibilityGraph
---
> from pyunicorn.timeseries import CrossRecurrencePlot, VisibilityGraph, \
>     Surrogates
29a31,34
> # turn off for weave compilation & error detection
> parallel = False
> 
> 
32a38
>     n_index, n_times = tdata.shape
45,99c51
< @pytest.mark.parametrize("thresh, rr", [(.2, None), (None, .2)])
< def testCrossRecurrencePlot(thresh, rr, metric: str):
<     # create two instances of the same test dataset
<     tdata1 = create_test_data()
<     x1 = tdata1[:, 0]
<     y1 = tdata1[:, 1]
<     tdata2 = create_test_data()
<     x2 = tdata2[:, 0]
<     y2 = tdata2[:, 1]
<     # create CrossRecurrencePlot for both
<     crp1 = CrossRecurrencePlot(
<             x1, y1, threshold=thresh, recurrence_rate=rr, metric=metric)
<     crp2 = CrossRecurrencePlot(
<             x2, y2, threshold=thresh, recurrence_rate=rr, metric=metric)
<     # get respective distance matrices
<     dist_1 = crp1.distance_matrix(metric)
<     dist_2 = crp2.distance_matrix(metric)
<     # get respective recurrence matrices
<     CR1 = crp1.recurrence_matrix()
<     CR2 = crp2.recurrence_matrix()
< 
<     assert np.allclose(dist_1, dist_2, atol=1e-04)
<     assert CR1.shape == CR2.shape
<     assert CR1.shape == (len(x1), len(y1))
<     assert CR1.dtype == CR2.dtype
<     assert CR1.dtype == np.int8
< 
< 
< # -----------------------------------------------------------------------------
< # recurrence_network
< # -----------------------------------------------------------------------------
< 
< @pytest.mark.parametrize("thresh, rr",
<                          [(.2, None), (None, .2)], ids=str)
< def testRecurrenceNetwork(thresh, rr, metric: str):
<     # create two instances of the same test dataset
<     tdata1 = create_test_data()
<     tdata2 = create_test_data()
<     # create RecurrenceNetwork for both
<     rn1 = RecurrenceNetwork(
<             tdata1, threshold=thresh, recurrence_rate=rr, metric=metric)
<     rn2 = RecurrenceNetwork(
<             tdata2, threshold=thresh, recurrence_rate=rr, metric=metric)
<     # get respective adjacency matrices
<     A1 = rn1.adjacency
<     A2 = rn2.adjacency
< 
<     assert np.array_equal(A1, A2)
<     assert A1.shape == A2.shape
<     assert A1.shape == (len(tdata1), len(tdata1))
<     assert A1.dtype == A2.dtype
<     assert A1.dtype == np.int16
< 
< 
< def testRecurrenceNetwork_setters():
---
> def testCrossRecurrencePlot():
101,113c53
<     rn = RecurrenceNetwork(tdata, threshold=.2)
<     # recalculate with different fixed threshold
<     rn.set_fixed_threshold(.3)
<     assert rn.adjacency.shape == (len(tdata), len(tdata))
<     # recalculate with fixed threshold in units of the ts' std
<     rn.set_fixed_threshold_std(.3)
<     assert rn.adjacency.shape == (len(tdata), len(tdata))
<     # recalculate with fixed recurrence rate
<     rn.set_fixed_recurrence_rate(.2)
<     assert rn.adjacency.shape == (len(tdata), len(tdata))
<     # recalculate with fixed local recurrence rate
<     rn.set_fixed_local_recurrence_rate(.2)
<     assert rn.adjacency.shape == (len(tdata), len(tdata))
---
>     CrossRecurrencePlot(x=tdata, y=tdata, threshold=0.2)
116,118c56,59
< # -----------------------------------------------------------------------------
< # joint_recurrence_network
< # -----------------------------------------------------------------------------
---
> def testDistanceMatrix():
>     tdata = create_test_data()
>     crp = CrossRecurrencePlot(x=tdata, y=tdata, threshold=1.0)
>     crp.distance_matrix(tdata.T, tdata.T, metric='manhattan')
121c62
< def testJointRecurrenceNetwork(metric: str):
---
> def testManhattanDistanceMatrix():
123,133c64,68
<     x = tdata[:, 0]
<     y = tdata[:, 1]
<     n = len(tdata)
<     jrp = JointRecurrenceNetwork(x, y, threshold=(.1, .1),
<                                  metric=(metric, metric))
<     dist = {}
<     for i in "xy":
<         jrp.embedding = getattr(jrp, f"{i}_embedded")
<         dist[i] = jrp.distance_matrix(metric=metric)
<     assert all(d.shape == (n, n) for d in dist.values())
<     assert jrp.recurrence_matrix().shape == (n, n)
---
>     n_index, n_times = tdata.shape
>     crp = CrossRecurrencePlot(x=tdata, y=tdata, threshold=1.0)
>     manh_dist_1 = crp.manhattan_distance_matrix(tdata.T, tdata.T)
>     manh_dist_2 = crp.manhattan_distance_matrix(tdata.T, tdata.T)
>     assert np.allclose(manh_dist_1, manh_dist_2, atol=1e-04)
136,138c71,77
< # -----------------------------------------------------------------------------
< # inter_system_recurrence_network
< # -----------------------------------------------------------------------------
---
> def testEuclideanDistanceMatrix():
>     tdata = create_test_data()
>     n_index, n_times = tdata.shape
>     crp = CrossRecurrencePlot(x=tdata, y=tdata, threshold=1.0)
>     eucl_dist_1 = crp.euclidean_distance_matrix(tdata.T, tdata.T)
>     eucl_dist_2 = crp.euclidean_distance_matrix(tdata.T, tdata.T)
>     assert np.allclose(eucl_dist_1, eucl_dist_2, atol=1e-04)
141,164c80,86
< @pytest.mark.parametrize("thresh, rr",
<                          [((.2, .3, .2), None), (None, (.2, .3, .2))], ids=str)
< def testInterSystemRecurrenceNetwork(thresh, rr, metric: str):
<     # create two instances of the same test dataset
<     tdata1 = create_test_data()
<     x1 = tdata1[:, 0]
<     y1 = tdata1[:, 1]
<     tdata2 = create_test_data()
<     x2 = tdata2[:, 0]
<     y2 = tdata2[:, 1]
<     # create InterSystemRecurrenceNetwork for both
<     isrn1 = InterSystemRecurrenceNetwork(
<             x1, y1, threshold=thresh, recurrence_rate=rr, metric=metric)
<     isrn2 = InterSystemRecurrenceNetwork(
<             x2, y2, threshold=thresh, recurrence_rate=rr, metric=metric)
<     # get respective adjacency matrices
<     A1 = isrn1.adjacency
<     A2 = isrn2.adjacency
< 
<     assert np.array_equal(A1, A2)
<     assert A1.shape == A2.shape
<     assert A1.shape == (len(x1)*2, len(y1)*2)
<     assert A1.dtype == A2.dtype
<     assert A1.dtype == np.int16
---
> def testSupremumDistanceMatrix():
>     tdata = create_test_data()
>     n_index, n_times = tdata.shape
>     crp = CrossRecurrencePlot(x=tdata, y=tdata, threshold=1.0)
>     supr_dist_1 = crp.supremum_distance_matrix(tdata.T, tdata.T)
>     supr_dist_2 = crp.supremum_distance_matrix(tdata.T, tdata.T)
>     assert np.allclose(supr_dist_1, supr_dist_2, atol=1e-04)
172,176c94,97
< def testNormalizeOriginalData():
<     ts = Surrogates.SmallTestData()
<     ts.normalize_original_data()
< 
<     res = ts.original_data.mean(axis=1)
---
> def testNormalizeTimeSeriesArray():
>     ts = Surrogates.SmallTestData().original_data
>     Surrogates.SmallTestData().normalize_time_series_array(ts)
>     res = ts.mean(axis=1)
180c101
<     res = ts.original_data.std(axis=1)
---
>     res = ts.std(axis=1)
186,189c107,109
<     ts = Surrogates.SmallTestData()
<     res = Surrogates.embed_time_series_array(
<         time_series_array=ts.original_data,
<         dimension=3, delay=2)[0, :6, :]
---
>     ts = Surrogates.SmallTestData().original_data
>     res = Surrogates.SmallTestData().embed_time_series_array(
>         time_series_array=ts, dimension=3, delay=2)[0, :6, :]
199,213d118
< def testSurrogatesRecurrencePlot():
<     thresh = .2
<     dim = 3
<     tau = 2
<     # calculate Surrogates.recurrence_plot()
<     ts = Surrogates.SmallTestData()
<     embedding = Surrogates.\
<         embed_time_series_array(ts.original_data, dimension=dim, delay=tau)
<     rp1 = Surrogates.recurrence_plot(embedding[0], threshold=thresh)
<     # compare to timeseries.RecurrencePlot
<     rp2 = RecurrencePlot(
<         ts.original_data[0], threshold=thresh, dim=dim, tau=tau).R
<     assert np.array_equal(rp1, rp2)
< 
< 
215,216c120,121
<     ts = Surrogates.SmallTestData()
<     surrogates = ts.white_noise_surrogates()
---
>     ts = Surrogates.SmallTestData().original_data
>     surrogates = Surrogates.SmallTestData().white_noise_surrogates(ts)
218,219c123,124
<     assert np.allclose(np.histogram(ts.original_data[0, :])[0],
<                        np.histogram(surrogates[0, :])[0])
---
>     assert(np.allclose(np.histogram(ts[0, :])[0],
>                        np.histogram(surrogates[0, :])[0]))
223,225c128,130
<     ts = Surrogates.SmallTestData()
<     surrogates = ts.correlated_noise_surrogates()
<     assert np.allclose(np.abs(np.fft.fft(ts.original_data, axis=1))[0, 1:10],
---
>     ts = Surrogates.SmallTestData().original_data
>     surrogates = Surrogates.SmallTestData().correlated_noise_surrogates(ts)
>     assert np.allclose(np.abs(np.fft.fft(ts, axis=1))[0, 1:10],
231,234c136,140
<     ts = Surrogates(tdata)
<     tsurro = ts.twin_surrogates(1, 0, 0.2)
<     corrcoef = np.corrcoef(tdata, tsurro)[ts.N:, :ts.N]
<     for i in range(ts.N):
---
>     n_index, n_times = tdata.shape
>     s = Surrogates(tdata)
>     tsurro = s.twin_surrogates(tdata, 1, 0, 0.2)
>     corrcoef = np.corrcoef(tdata, tsurro)[n_index:, :n_index]
>     for i in range(n_index):
239,250d144
< def testAAFTSurrogates():
<     ts = Surrogates.SmallTestData()
<     # also covers Surrogates.AAFT_surrogates(), which is used as starting point
<     surr_R, surr_s = ts.refined_AAFT_surrogates(n_iterations=3, output="both")
<     # assert conserved amplitude distribution
<     assert all(np.histogram(ts.original_data[0, :])[0] ==
<                np.histogram(surr_R[0, :])[0])
<     # assert conserved power spectrum
<     assert np.allclose(np.abs(np.fft.fft(ts.original_data, axis=1))[0, 1:10],
<                        np.abs(np.fft.fft(surr_s, axis=1))[0, 1:10])
< 
< 
270,292d163
< 
< def testOriginalDistribution():
<     nbins = 10
<     ts = Surrogates.SmallTestData()
<     hist, lbb = ts.original_distribution(
<         Surrogates.test_mutual_information, n_bins=nbins)
< 
<     assert np.isclose(hist.sum(), 1) and len(lbb) == nbins
< 
< 
< def testThresholdSignificance():
<     nbins = 10
<     ts = Surrogates.SmallTestData()
<     density_estimate, lbb = ts.test_threshold_significance(
<         Surrogates.white_noise_surrogates,
<         Surrogates.test_mutual_information,
<         realizations=5,
<         interval=[0, 2],
<         n_bins=nbins)
< 
<     assert np.isclose(density_estimate.sum(), 1) and len(lbb) == nbins
< 
< 
300c171
<     Return test time series including random values and timings
---
>     Return test time series including random values and timings 
333c204
< 
---
>     
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tools/update-copyright lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/tools/update-copyright
1a2
> # -*- coding: utf-8 -*-
4,5c5,6
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
---
> # Copyright (C) 2008--2023 Jonathan F. Donges and pyunicorn authors
> # URL: <http://www.pik-potsdam.de/members/donges/software>
diff -r source-pyunicorn-master.tar.gz-extracted/pyunicorn-master/.travis.yml lookaside-pyunicorn-master.tar.gz-extracted/pyunicorn-master/.travis.yml
1,11c1,2
< # This file is part of pyunicorn.
< # Copyright (C) 2008--2024 Jonathan F. Donges and pyunicorn authors
< # URL: <https://www.pik-potsdam.de/members/donges/software-2/software>
< # License: BSD (3-clause)
< 
< 
< # documentation & validation ==================================================
< 
< # - https://docs.travis-ci.com/user/reference/overview
< # - https://docs.travis-ci.com/user/build-matrix/
< # - https://docs.travis-ci.com/user/multi-os/
---
> # require the branch name to be master
> if: branch = master
13,14c4,24
< # - https://docs.travis-ci.com/user/build-config-validation/
< # - https://config.travis-ci.com/explore
---
> language: python
> python:
>     - "3.8"
>     - "3.9"
>     - "3.10"
>     - "3.11"
> 
> sudo: false
> dist: jammy
> matrix:
>     fast_finish: true
> 
> before_install:
>     # Python package manager
>     - travis_retry wget http://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh
>     - bash miniconda.sh -b -p $HOME/miniconda
>     - export PATH="$HOME/miniconda/bin:$PATH"; hash -r
>     - conda config --set quiet yes --set always_yes yes --set changeps1 no
>     - travis_retry conda update -n base -c defaults conda
>     - travis_retry conda update --all
>     - conda info -a
15a26,41
> install:
>     # runtime dependencies
>     - travis_retry conda create -n test-env
>     - eval "$(conda shell.bash hook)"
>     - conda activate test-env
>     - travis_retry conda install -c conda-forge python=$TRAVIS_PYTHON_VERSION
>     - travis_retry conda install -c conda-forge numpy scipy python-igraph h5netcdf
>     - travis_retry conda update  -c conda-forge --all
> 
>     # testing dependencies
>     - travis_retry conda install -c conda-forge tox flake8 pylint pytest-xdist pytest-cov
>     - travis_retry conda install -c conda-forge networkx matplotlib cartopy sphinx
> 
> before_script:
>     # limit pytest processes within container environment
>     - sed -i 's/-n auto/-n 2/' setup.cfg
17c43,45
< # meta ========================================================================
---
> script:
>     # package
>     - pip install -vvv -e .
19,20c47,48
< # enable build config validation
< version: ~> 1.0
---
>     # test suite
>     - tox -v
22,23c50,51
< # save Travis budget
< if: branch = master
---
> after_success:
>     - codecov
25d52
< # report outcomes
27,115c54,56
<   email:
<     on_success: change
<     on_failure: always
< 
< 
< # default jobs: Linux, all Python versions ====================================
< 
< os: linux
< dist: focal
< arch: arm64
< virt: lxd
< language: generic
< env:
<   jobs:
<     - PYTHON=3.12
<     - PYTHON=3.11
<     - PYTHON=3.10
<     - PYTHON=3.9
<     - PYTHON=3.8
< 
< before_install: export ARCH=Linux-aarch64 SED=sed
< 
< install:
<   - | # install Miniconda
<     travis_retry wget https://repo.anaconda.com/miniconda/Miniconda3-latest-${ARCH}.sh -O miniconda.sh
<     bash miniconda.sh -b -p ${HOME}/miniconda
<     export PATH="${HOME}/miniconda/bin:${PATH}"; hash -r
<     conda config --set quiet yes --set always_yes yes --set changeps1 no
<     travis_retry conda update -n base -c defaults conda
<     travis_retry conda update --all
<     conda config --set solver libmamba
<     conda info -a
<     conda list
<   - | # install executables via Miniconda: Python, Pandoc, Codecov
<     travis_retry conda create -n test-env
<     eval "$(conda shell.bash hook)"
<     conda activate test-env
<     travis_retry conda install -c conda-forge python=${PYTHON} pandoc codecov
<     travis_retry conda update  -c conda-forge --all
<     conda info -a
<     conda list
<   - | # install Python libs via Miniconda
<     travis_retry conda install -c conda-forge \
<         numpy scipy python-igraph h5netcdf tqdm \
<         ipython networkx matplotlib cartopy sphinx nbsphinx \
<         tox flake8 pylint pytest-xdist pytest-cov
<     conda info -a
<     conda list
< 
< script:
<   - | # limit procs to available cores (use GNU `sed`, fail if pattern not found)
<     ${SED} -i '/nthreads=./{s//nthreads=2/;h}; ${x;/./{x;q0};x;q1}' setup.py
<     ${SED} -i '/-j ./      {s//-j 2/;      h}; ${x;/./{x;q0};x;q1}' setup.cfg
<     ${SED} -i '/-n auto/   {s//-n 2/;      h}; ${x;/./{x;q0};x;q1}' pyproject.toml
<     ${SED} -i '/jobs = ./  {s//jobs = 2/;  h}; ${x;/./{x;q0};x;q1}' pyproject.toml
<   - | # install Python libs via Pip: self (+ dependencies, if on Windows)
<     travis_retry python -m pip install -v -e .[tests,docs]
<   - | # run test suite
<     tox -v
< 
< # report statistics
< after_success: codecov
< 
< 
< # modified jobs: OSX + Windows, newest Python version =========================
< # (inherit only 1st `env.jobs` entry)
< 
< jobs:
<   fast_finish: true
<   include:
<     - os: osx
<       osx_image: xcode14
<       language: shell
<       before_install:
<         - export ARCH=MacOSX-x86_64 SED=gsed
<         - | # install executables via Homebrew: GNU `sed`
<           export HOMEBREW_NO_AUTO_UPDATE=1 HOMEBREW_NO_INSTALL_CLEANUP=1
<           travis_retry brew install gnu-sed
< 
<     - os: windows
<       language: shell
<       before_install:
<         - export ARCH=Windows-x86_64 SED=sed
<         - export PATH=/c/Python${PYTHON/.}:/c/Python${PYTHON/.}/Scripts:${PATH}
<       install:
<         - | # install executables via Chocolatey: Python, Pandoc, Codecov
<           travis_retry choco install python --version ${PYTHON}
<           travis_retry python -m pip install --upgrade pip
<           travis_retry choco install pandoc codecov
---
>     email:
>         on_success: change
>         on_failure: always
